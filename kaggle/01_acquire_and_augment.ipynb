{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3012d8",
   "metadata": {},
   "source": [
    "# ğŸ§¬ Genesis IR Head â€” Data Curation Pipeline\n",
    "\n",
    "**Notebook 01**: Acquire â†’ Sterilize â†’ Augment â†’ Target\n",
    "\n",
    "This notebook produces training triples for Genesis's IR Synthesis Head:\n",
    "\n",
    "| Tensor | Format | Description |\n",
    "|--------|--------|-------------|\n",
    "| `source_wet` | int16 waveform | Sterile vocal convolved with a **bad** IR + random degradations |\n",
    "| `target_wet` | int16 waveform | Same sterile vocal convolved with a **target** MIT IR (ground truth) |\n",
    "| `target_clap` | float32 vector | CLAP embedding of the target IR (pre-computed) |\n",
    "\n",
    "**Checkpoint chaining**: Output is capped at ~19 GB per run. A `checkpoint.json`\n",
    "tracks progress so the next run (using this output as input) continues where\n",
    "we left off.\n",
    "\n",
    "> **Pool definitions**\n",
    "> * **Bad pool** â€” User-uploaded unlabeled IRs (uncontrolled environments)\n",
    "> * **Target pool** â€” MIT Acoustical Reverberation Scene Statistics IRs (labeled, real-world rooms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q noisereduce pyloudnorm soundfile librosa transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b748e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 0 â€” Environment & Path Configuration\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os, json, hashlib, time, random, warnings, shutil, pickle\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.signal import fftconvolve\n",
    "import noisereduce as nr\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# KAGGLE INPUT PATHS  (datasets attached to this notebook)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PATHS = {\n",
    "    'irs':       Path('/kaggle/input/datasets/itorousa/impulse-responses'),\n",
    "    'mit_raw':   Path('/kaggle/input/datasets/kynthesis/mit-reverb-dataset'\n",
    "                      '/MIT_Reverb_Dataset/MIT_Reverb_Dataset'),\n",
    "    'ljspeech':  Path('/kaggle/input/datasets/dromosys/ljspeech'),\n",
    "    'vctk':      Path('/kaggle/input/datasets/kynthesis/vctk-corpus'\n",
    "                      '/VCTK-Corpus/wav48'),\n",
    "    'langid_en': Path('/kaggle/input/datasets/shrivatssudhir'\n",
    "                      '/language-identifier/english/clips'),\n",
    "}\n",
    "\n",
    "# Chaining â€” previous run's output (attached as input to this notebook).\n",
    "# Adjust the path for each subsequent run.\n",
    "PREV_RUN_PATH = Path('/kaggle/input/notebooks/itorousa/genesis-data-run1')\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OUTPUT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "OUTPUT          = Path('/kaggle/working')\n",
    "BATCH_DIR       = OUTPUT / 'batches'\n",
    "CLAP_DIR        = OUTPUT / 'clap_model'\n",
    "STERILIZED_DIR  = OUTPUT / 'sterilized_batches'\n",
    "MIT_IR_DIR      = OUTPUT / 'irs' / 'mit_irs'\n",
    "\n",
    "for d in [BATCH_DIR, CLAP_DIR, STERILIZED_DIR, MIT_IR_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# AUDIO PARAMETERS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SR            = 48_000\n",
    "CLIP_SEC      = 5.0\n",
    "CLIP_SAMPLES  = int(SR * CLIP_SEC)    # 240,000 samples\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# BUDGET & BATCHING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "TRIPLES_PER_BATCH  = 500\n",
    "MAX_OUTPUT_GB      = 19.0             # safety margin under 20 GB cap\n",
    "AUGMENTATIONS_MIN  = 3\n",
    "AUGMENTATIONS_MAX  = 6\n",
    "\n",
    "print(f\"SR={SR}, CLIP_SEC={CLIP_SEC}, CLIP_SAMPLES={CLIP_SAMPLES}\")\n",
    "print(f\"Output budget: {MAX_OUTPUT_GB} GB, batch size: {TRIPLES_PER_BATCH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6336cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 1 â€” Aggressive Checkpoint Initialization & Hardware Protection\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CHECKPOINT_PATH = OUTPUT / 'checkpoint.json'\n",
    "\n",
    "def load_checkpoint() -> dict:\n",
    "    \\\"\\\"\\\"Load checkpoint: previous run â†’ current working dir â†’ fresh start.\\\"\\\"\\\"\n",
    "    # 1) Check previous run first (chaining)\n",
    "    prev_ckpt = PREV_RUN_PATH / 'checkpoint.json'\n",
    "    if prev_ckpt.exists():\n",
    "        with open(prev_ckpt) as f:\n",
    "            ckpt = json.load(f)\n",
    "        ckpt['run_number'] += 1\n",
    "        print(f\"[Checkpoint] â™» Resuming from previous run: \"\n",
    "              f\"{ckpt['triples_completed']} triples done\")\n",
    "        return ckpt\n",
    "\n",
    "    # 2) Check current working dir (kernel restart mid-session)\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        with open(CHECKPOINT_PATH) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # 3) Fresh start\n",
    "    return {\n",
    "        'batch_id': 0,\n",
    "        'triples_completed': 0,\n",
    "        'vocal_cursor': 0,\n",
    "        'run_number': 1,\n",
    "    }\n",
    "\n",
    "def save_checkpoint(ckpt: dict):\n",
    "    with open(CHECKPOINT_PATH, 'w') as f:\n",
    "        json.dump(ckpt, f, indent=2)\n",
    "\n",
    "def get_output_size_gb() -> float:\n",
    "    \\\"\\\"\\\"Calculate the exact size of /kaggle/working in GB.\\\"\\\"\\\"\n",
    "    total = sum(f.stat().st_size for f in OUTPUT.rglob('*') if f.is_file())\n",
    "    return total / (1024 ** 3)\n",
    "\n",
    "ckpt = load_checkpoint()\n",
    "print(f\"[Checkpoint] Run #{ckpt['run_number']}, \"\n",
    "      f\"{ckpt['triples_completed']} triples completed so far, \"\n",
    "      f\"starting at vocal cursor {ckpt['vocal_cursor']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 2 â€” Impulse Response Acquisition & Pooling\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CATALOGUE_PATH = OUTPUT / 'ir_catalogue.json'\n",
    "\n",
    "# â”€â”€â”€ Fast path: reload from existing catalogue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if CATALOGUE_PATH.exists():\n",
    "    print(\"ğŸŸ¢ Found existing ir_catalogue.json â€” skipping audio-loading block.\")\n",
    "    with open(CATALOGUE_PATH, 'r') as f:\n",
    "        ir_catalogue = json.load(f)\n",
    "\n",
    "    all_irs: Dict[str, dict] = {}   # will NOT have audio yet; lazy-loaded later\n",
    "    bad_pool:    List[str] = []\n",
    "    target_pool: List[str] = []\n",
    "\n",
    "    for ir_id, feats in ir_catalogue.items():\n",
    "        if feats['source'] == 'mit':\n",
    "            target_pool.append(ir_id)\n",
    "        elif feats['rt60'] > 0.25 or feats['c50'] < 8:\n",
    "            bad_pool.append(ir_id)\n",
    "        else:\n",
    "            bad_pool.append(ir_id)\n",
    "\n",
    "    # Safety: ensure target pool is adequate\n",
    "    if len(target_pool) < 20:\n",
    "        mit_ids = [k for k, v in ir_catalogue.items() if v['source'] == 'mit']\n",
    "        target_pool = mit_ids if mit_ids else list(ir_catalogue.keys())[:50]\n",
    "        bad_pool = [k for k in ir_catalogue if k not in target_pool]\n",
    "\n",
    "    print(f\"ğŸŸ¢ Loaded {len(ir_catalogue)} IRs from catalogue\")\n",
    "    print(f\"ğŸŸ¢ Bad pool: {len(bad_pool)} | Target pool: {len(target_pool)}\")\n",
    "    _catalogue_loaded_from_disk = True\n",
    "\n",
    "else:\n",
    "    _catalogue_loaded_from_disk = False\n",
    "\n",
    "    # â”€â”€â”€ Stage 1: Copy / acquire MIT IRs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    prev_mit = PREV_RUN_PATH / 'irs' / 'mit_irs'\n",
    "    if prev_mit.exists() and not any(MIT_IR_DIR.iterdir()):\n",
    "        print(\"ğŸŸ¢ [Acquire] Copying MIT IRs from previous run...\")\n",
    "        shutil.copytree(prev_mit, MIT_IR_DIR, dirs_exist_ok=True)\n",
    "        print(\"ğŸŸ¢ [Acquire] MIT IRs staged from previous run âœ“\")\n",
    "    elif PATHS['mit_raw'].exists() and not any(MIT_IR_DIR.iterdir()):\n",
    "        print(\"ğŸŸ¢ [Acquire] Copying MIT IRs from raw dataset...\")\n",
    "        shutil.copytree(PATHS['mit_raw'], MIT_IR_DIR, dirs_exist_ok=True)\n",
    "        print(\"ğŸŸ¢ [Acquire] MIT IRs staged from dataset âœ“\")\n",
    "    else:\n",
    "        print(\"ğŸŸ¢ [Acquire] MIT IRs already staged or unavailable\")\n",
    "\n",
    "    # â”€â”€â”€ Stage 2: Load & normalize every IR to 48 kHz mono â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def load_and_normalize_ir(filepath: Path, target_sr: int = SR) -> Optional[np.ndarray]:\n",
    "        \\\"\\\"\\\"Load an IR file, force mono / target SR, peak-normalize.\\\"\\\"\\\"\n",
    "        try:\n",
    "            audio, _ = librosa.load(str(filepath), sr=target_sr, mono=True)\n",
    "            if len(audio) < 64:\n",
    "                return None\n",
    "            peak = np.max(np.abs(audio))\n",
    "            if peak > 1e-6:\n",
    "                audio = audio / peak\n",
    "            return audio.astype(np.float32)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    print(\"\\nğŸŸ¢ [Acquire] Loading all impulse responses...\")\n",
    "    all_irs: Dict[str, dict] = {}\n",
    "\n",
    "    # Bad pool â€” user's unlabeled IRs\n",
    "    for ext in ('*.irs', '*.wav'):\n",
    "        for f in sorted(PATHS['irs'].rglob(ext)):\n",
    "            ir = load_and_normalize_ir(f)\n",
    "            if ir is not None:\n",
    "                all_irs[f'user_{f.stem}'] = {'audio': ir, 'source': 'user'}\n",
    "\n",
    "    # Target pool â€” MIT labeled IRs\n",
    "    if MIT_IR_DIR.exists():\n",
    "        for f in sorted(MIT_IR_DIR.rglob('*.wav')):\n",
    "            ir = load_and_normalize_ir(f)\n",
    "            if ir is not None:\n",
    "                all_irs[f'mit_{f.stem}'] = {'audio': ir, 'source': 'mit'}\n",
    "\n",
    "    print(f\"ğŸŸ¢ [Acquire] Loaded {len(all_irs)} IRs \"\n",
    "          f\"(user/bad: {sum(1 for v in all_irs.values() if v['source']=='user')}, \"\n",
    "          f\"MIT/target: {sum(1 for v in all_irs.values() if v['source']=='mit')})\")\n",
    "\n",
    "    # â”€â”€â”€ Stage 3: Classify & build pools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def compute_ir_features(ir_audio: np.ndarray, sr: int = SR) -> dict:\n",
    "        \\\"\\\"\\\"Compute RT60 estimate, spectral centroid, and clarity (C50).\\\"\\\"\\\"\n",
    "        energy = ir_audio ** 2\n",
    "        cumsum = np.cumsum(energy[::-1])[::-1]\n",
    "\n",
    "        # RT60 â€” time for energy to decay 60 dB\n",
    "        rt60 = len(ir_audio) / sr\n",
    "        if cumsum[0] > 1e-10:\n",
    "            decay_db = 10 * np.log10(cumsum / cumsum[0] + 1e-12)\n",
    "            idx = np.where(decay_db < -60)[0]\n",
    "            if len(idx) > 0:\n",
    "                rt60 = idx[0] / sr\n",
    "\n",
    "        # Spectral centroid\n",
    "        S = np.abs(np.fft.rfft(ir_audio))\n",
    "        freqs = np.fft.rfftfreq(len(ir_audio), 1 / sr)\n",
    "        centroid = float(np.sum(freqs * S) / (np.sum(S) + 1e-10))\n",
    "\n",
    "        # Clarity C50 â€” early-to-late energy ratio at 50 ms\n",
    "        split = int(0.05 * sr)\n",
    "        early = np.sum(ir_audio[:split] ** 2) + 1e-12\n",
    "        late  = np.sum(ir_audio[split:] ** 2) + 1e-12\n",
    "        c50   = float(10 * np.log10(early / late))\n",
    "\n",
    "        return {\n",
    "            'rt60': round(rt60, 4),\n",
    "            'centroid': round(centroid, 1),\n",
    "            'c50': round(c50, 2),\n",
    "            'length_sec': round(len(ir_audio) / sr, 4),\n",
    "        }\n",
    "\n",
    "    ir_catalogue = {}\n",
    "    bad_pool:    List[str] = []\n",
    "    target_pool: List[str] = []\n",
    "\n",
    "    for ir_id, ir_data in all_irs.items():\n",
    "        feats = compute_ir_features(ir_data['audio'])\n",
    "        ir_data['features'] = feats\n",
    "        ir_catalogue[ir_id] = {'source': ir_data['source'], **feats}\n",
    "\n",
    "        # MIT IRs â†’ target pool (labeled ground-truth rooms)\n",
    "        # User IRs â†’ bad pool (unlabeled/degraded)\n",
    "        if ir_data['source'] == 'mit':\n",
    "            target_pool.append(ir_id)\n",
    "        else:\n",
    "            bad_pool.append(ir_id)\n",
    "\n",
    "    # Safety: ensure target pool is adequate\n",
    "    if len(target_pool) < 20:\n",
    "        mit_ids = sorted(\n",
    "            [k for k, v in all_irs.items() if v['source'] == 'mit'],\n",
    "            key=lambda k: all_irs[k]['features']['c50'], reverse=True\n",
    "        )\n",
    "        target_pool = mit_ids if mit_ids else list(all_irs.keys())[:50]\n",
    "        bad_pool = [k for k in all_irs if k not in target_pool]\n",
    "\n",
    "    print(f\"ğŸŸ¢ [Classify] Bad pool:    {len(bad_pool)} IRs\")\n",
    "    print(f\"ğŸŸ¢ [Classify] Target pool: {len(target_pool)} IRs\")\n",
    "\n",
    "    # ğŸ’¾ Save catalogue\n",
    "    with open(CATALOGUE_PATH, 'w') as f:\n",
    "        json.dump(ir_catalogue, f, indent=2)\n",
    "    print(\"ğŸŸ¢ [Classify] ir_catalogue.json saved âœ“\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 3 â€” CLAP Target Embedding Cache\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CLAP_MODEL_ID = \"laion/larger_clap_music_and_speech\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸŸ¢ [CLAP] Device: {device}\")\n",
    "\n",
    "# â”€â”€â”€ Model loading chain: local â†’ previous run â†’ download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prev_clap = PREV_RUN_PATH / 'clap_model'\n",
    "\n",
    "if (CLAP_DIR / 'config.json').exists():\n",
    "    print(\"ğŸŸ¢ [CLAP] Loading frozen model from current working dir...\")\n",
    "    clap_processor = ClapProcessor.from_pretrained(CLAP_DIR)\n",
    "    clap_model = ClapModel.from_pretrained(CLAP_DIR).to(device).eval()\n",
    "\n",
    "elif prev_clap.exists() and (prev_clap / 'config.json').exists():\n",
    "    print(\"ğŸŸ¢ [CLAP] Copying frozen model from previous run...\")\n",
    "    shutil.copytree(prev_clap, CLAP_DIR, dirs_exist_ok=True)\n",
    "    clap_processor = ClapProcessor.from_pretrained(CLAP_DIR)\n",
    "    clap_model = ClapModel.from_pretrained(CLAP_DIR).to(device).eval()\n",
    "\n",
    "else:\n",
    "    print(\"ğŸŸ¢ [CLAP] Downloading model from Hugging Face...\")\n",
    "    clap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)\n",
    "    clap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(device).eval()\n",
    "    clap_model.save_pretrained(CLAP_DIR)\n",
    "    clap_processor.save_pretrained(CLAP_DIR)\n",
    "    print(f\"ğŸŸ¢ [CLAP] Frozen model saved to {CLAP_DIR}\")\n",
    "\n",
    "CLAP_DIM = clap_model.config.projection_dim\n",
    "print(f\"ğŸŸ¢ [CLAP] Loaded â€” embedding dim = {CLAP_DIM}\")\n",
    "\n",
    "\n",
    "def get_clap_audio_embedding(audio: np.ndarray, sr: int = SR) -> np.ndarray:\n",
    "    \\\"\\\"\\\"\n",
    "    Encode audio through CLAP's audio tower. Returns (CLAP_DIM,) float32.\n",
    "\n",
    "    âš  Critical: extract .pooler_output from the wrapper before calling .cpu().\n",
    "    \\\"\\\"\\\"\n",
    "    inputs = clap_processor(\n",
    "        audios=audio, sampling_rate=sr, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = clap_model.get_audio_features(**inputs)\n",
    "        emb = outputs.pooler_output\n",
    "    return emb.cpu().numpy().flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "# â”€â”€â”€ Pre-compute CLAP embeddings for every target-pool IR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Convolve each IR with 3s of white noise so CLAP has a rich scene.\n",
    "CLAP_CACHE_PATH = OUTPUT / 'clap_cache.npz'\n",
    "prev_cache = PREV_RUN_PATH / 'clap_cache.npz'\n",
    "\n",
    "if CLAP_CACHE_PATH.exists():\n",
    "    print(\"ğŸŸ¢ [CLAP] Found existing clap_cache.npz â€” skipping embedding compute.\")\n",
    "    clap_cache = dict(np.load(CLAP_CACHE_PATH))\n",
    "    print(f\"ğŸŸ¢ [CLAP] Loaded {len(clap_cache)} embeddings from cache âœ“  (dim={CLAP_DIM})\")\n",
    "\n",
    "elif prev_cache.exists():\n",
    "    print(\"ğŸŸ¢ [CLAP] Copying clap_cache.npz from previous run...\")\n",
    "    shutil.copy2(prev_cache, CLAP_CACHE_PATH)\n",
    "    clap_cache = dict(np.load(CLAP_CACHE_PATH))\n",
    "    print(f\"ğŸŸ¢ [CLAP] Loaded {len(clap_cache)} embeddings from previous run âœ“\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nğŸŸ¢ [CLAP] Pre-computing embeddings for {len(target_pool)} target IRs...\")\n",
    "\n",
    "    # Need IR audio loaded â€” if catalogue was loaded from disk, lazy-load now\n",
    "    if _catalogue_loaded_from_disk:\n",
    "        print(\"   (Lazy-loading IR audio for embedding computation...)\")\n",
    "        def _lazy_load_ir(ir_id: str) -> Optional[np.ndarray]:\n",
    "            if ir_id.startswith('mit_'):\n",
    "                for f in MIT_IR_DIR.rglob('*.wav'):\n",
    "                    if f.stem == ir_id[4:]:\n",
    "                        audio, _ = librosa.load(str(f), sr=SR, mono=True)\n",
    "                        peak = np.max(np.abs(audio))\n",
    "                        return (audio / peak).astype(np.float32) if peak > 1e-6 else audio\n",
    "            elif ir_id.startswith('user_'):\n",
    "                for ext in ('*.irs', '*.wav'):\n",
    "                    for f in PATHS['irs'].rglob(ext):\n",
    "                        if f.stem == ir_id[5:]:\n",
    "                            audio, _ = librosa.load(str(f), sr=SR, mono=True)\n",
    "                            peak = np.max(np.abs(audio))\n",
    "                            return (audio / peak).astype(np.float32) if peak > 1e-6 else audio\n",
    "            return None\n",
    "\n",
    "        all_irs = {}\n",
    "        for ir_id in target_pool + bad_pool:\n",
    "            a = _lazy_load_ir(ir_id)\n",
    "            if a is not None:\n",
    "                all_irs[ir_id] = {'audio': a, 'source': ir_catalogue[ir_id]['source']}\n",
    "\n",
    "    ref_noise = np.random.randn(SR * 3).astype(np.float32) * 0.1\n",
    "\n",
    "    clap_cache: Dict[str, np.ndarray] = {}\n",
    "    for i, ir_id in enumerate(target_pool):\n",
    "        if ir_id not in all_irs:\n",
    "            continue\n",
    "        ir_audio = all_irs[ir_id]['audio']\n",
    "        scene = fftconvolve(ref_noise, ir_audio, mode='full')[:SR * 3]\n",
    "        scene = scene / (np.max(np.abs(scene)) + 1e-8)\n",
    "        clap_cache[ir_id] = get_clap_audio_embedding(scene)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  {i+1}/{len(target_pool)}\")\n",
    "\n",
    "    np.savez(CLAP_CACHE_PATH, **clap_cache)\n",
    "    print(f\"ğŸŸ¢ [CLAP] Cached {len(clap_cache)} embeddings âœ“  (dim={CLAP_DIM})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyloudnorm as pyln\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 4 â€” Vocal Sterilization & \"Dead\" Audio Guarantee\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "meter = pyln.Meter(SR)\n",
    "STATE_FILE = STERILIZED_DIR / 'sterilize_state.json'\n",
    "\n",
    "# â”€â”€â”€ Check if a previous run already completed sterilization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prev_sterilized = PREV_RUN_PATH / 'sterilized_batches'\n",
    "prev_state_file = prev_sterilized / 'sterilize_state.json' if prev_sterilized.exists() else None\n",
    "\n",
    "_skip_sterilization = False\n",
    "if prev_state_file and prev_state_file.exists():\n",
    "    with open(prev_state_file) as f:\n",
    "        prev_state = json.load(f)\n",
    "    if prev_state.get('completed', False):\n",
    "        print(\"ğŸŸ¢ [Sterilize] Previous run completed sterilization. Copying batches...\")\n",
    "        shutil.copytree(prev_sterilized, STERILIZED_DIR, dirs_exist_ok=True)\n",
    "        _skip_sterilization = True\n",
    "        print(\"ğŸŸ¢ [Sterilize] Sterilized batches copied from previous run âœ“\")\n",
    "\n",
    "if STATE_FILE.exists() and not _skip_sterilization:\n",
    "    with open(STATE_FILE) as f:\n",
    "        _st = json.load(f)\n",
    "    if _st.get('completed', False):\n",
    "        print(\"ğŸŸ¢ [Sterilize] Already completed in this working dir. Skipping.\")\n",
    "        _skip_sterilization = True\n",
    "\n",
    "if not _skip_sterilization:\n",
    "    def discover_audio_files() -> List[Tuple[Path, str]]:\n",
    "        \\\"\\\"\\\"Collect all vocal files from all datasets.\\\"\\\"\\\"\n",
    "        files = []\n",
    "        # LJSpeech\n",
    "        for f in sorted(PATHS['ljspeech'].rglob('*.wav')):\n",
    "            files.append((f, 'ljspeech'))\n",
    "        # VCTK â€” subfolder per speaker\n",
    "        for f in sorted(PATHS['vctk'].rglob('*.wav')):\n",
    "            files.append((f, f'vctk_{f.parent.name}'))\n",
    "        # Language Identifier â€” english clips\n",
    "        for ext in ('*.wav', '*.mp3', '*.ogg'):\n",
    "            for f in sorted(PATHS['langid_en'].rglob(ext)):\n",
    "                files.append((f, 'langid_en'))\n",
    "        return files\n",
    "\n",
    "    def sterilize_and_segment(filepath: Path, tag: str) -> List[dict]:\n",
    "        \\\"\\\"\\\"Load â†’ noise-reduce â†’ trim â†’ LUFS normalize â†’ segment into 5s windows.\\\"\\\"\\\"\n",
    "        try:\n",
    "            audio, _ = librosa.load(str(filepath), sr=SR, mono=True)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "        if len(audio) < SR * 1.0:\n",
    "            return []\n",
    "\n",
    "        # Spectral noise reduction â€” strip residual room tone / hiss\n",
    "        audio = nr.reduce_noise(y=audio, sr=SR, stationary=True, prop_decrease=0.85)\n",
    "\n",
    "        # Trim absolute silence\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=40)\n",
    "        if len(audio) < SR * 1.5:\n",
    "            return []\n",
    "\n",
    "        # Normalize loudness to -23 LUFS\n",
    "        try:\n",
    "            loudness = meter.integrated_loudness(audio)\n",
    "            if loudness > -70:\n",
    "                audio = pyln.normalize.loudness(audio, loudness, -23.0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Segment into rigid CLIP_SAMPLES (5.0s) chunks\n",
    "        segments = []\n",
    "        for start in range(0, len(audio) - SR, CLIP_SAMPLES):\n",
    "            chunk = audio[start : start + CLIP_SAMPLES]\n",
    "            if len(chunk) < CLIP_SAMPLES:\n",
    "                chunk = np.pad(chunk, (0, CLIP_SAMPLES - len(chunk)))\n",
    "\n",
    "            rms = np.sqrt(np.mean(chunk ** 2))\n",
    "            if rms < 1e-4:\n",
    "                continue\n",
    "\n",
    "            segments.append({\n",
    "                'audio':   chunk.astype(np.float32),\n",
    "                'file':    filepath.name,\n",
    "                'dataset': tag,\n",
    "            })\n",
    "        return segments\n",
    "\n",
    "    # â”€â”€ Discover & deterministic shuffle â”€â”€\n",
    "    print(\"ğŸŸ¢ [Sterilize] Discovering audio files...\")\n",
    "    all_audio_files = discover_audio_files()\n",
    "\n",
    "    # Sort alphabetically â†’ shuffle with fixed seed for reproducibility\n",
    "    all_audio_files.sort(key=lambda x: str(x[0]))\n",
    "    random.Random(42).shuffle(all_audio_files)\n",
    "    print(f\"ğŸŸ¢ [Sterilize] Found {len(all_audio_files)} source files\")\n",
    "\n",
    "    # â”€â”€ Resume from sterilize checkpoint â”€â”€\n",
    "    cursor_start = 0\n",
    "    if STATE_FILE.exists():\n",
    "        with open(STATE_FILE) as f:\n",
    "            cursor_start = json.load(f).get('cursor', 0)\n",
    "\n",
    "    STERILIZE_CHUNK = 500\n",
    "    vocal_segments: List[dict] = []\n",
    "\n",
    "    print(f\"ğŸŸ¢ [Sterilize] Processing from file index {cursor_start}...\")\n",
    "    for i in range(cursor_start, len(all_audio_files)):\n",
    "        fpath, tag = all_audio_files[i]\n",
    "\n",
    "        # Budget check\n",
    "        if get_output_size_gb() > MAX_OUTPUT_GB:\n",
    "            print(f\"\\nâš  Output size limit reached. Saving sterilization progress.\")\n",
    "            break\n",
    "\n",
    "        if i % 200 == 0 and i > cursor_start:\n",
    "            print(f\"ğŸŸ¢ Processed {i}/{len(all_audio_files)} files â†’ \"\n",
    "                  f\"{len(vocal_segments)} segments in RAM buffer\")\n",
    "\n",
    "        segs = sterilize_and_segment(fpath, tag)\n",
    "        vocal_segments.extend(segs)\n",
    "\n",
    "        # ğŸ’¾ Granular disk flushing every STERILIZE_CHUNK files\n",
    "        if (i + 1) % STERILIZE_CHUNK == 0 or (i + 1) == len(all_audio_files):\n",
    "            batch_index = (i + 1) // STERILIZE_CHUNK\n",
    "            batch_path = STERILIZED_DIR / f\"sterilized_batch_{batch_index:04d}.pkl\"\n",
    "\n",
    "            with open(batch_path, 'wb') as f:\n",
    "                pickle.dump(vocal_segments, f)\n",
    "\n",
    "            completed = (i + 1) >= len(all_audio_files)\n",
    "            with open(STATE_FILE, 'w') as f:\n",
    "                json.dump({'cursor': i + 1, 'completed': completed}, f)\n",
    "\n",
    "            print(f\"ğŸ’¾ Saved {len(vocal_segments)} segments to {batch_path.name}. RAM cleared.\")\n",
    "            vocal_segments.clear()\n",
    "\n",
    "# â”€â”€ Aggregate: read all .pkl batches to compile the segment count â”€â”€\n",
    "print(\"\\nğŸŸ¢ [Sterilize] Compiling dataset breakdown from disk...\")\n",
    "ds_counts = defaultdict(int)\n",
    "total_segments = 0\n",
    "\n",
    "for batch_file in sorted(STERILIZED_DIR.glob('*.pkl')):\n",
    "    with open(batch_file, 'rb') as f:\n",
    "        batch_data = pickle.load(f)\n",
    "        total_segments += len(batch_data)\n",
    "        for s in batch_data:\n",
    "            ds_counts[s['dataset'].split('_')[0]] += 1\n",
    "\n",
    "print(f\"ğŸŸ¢ [Sterilize] Total sterile segments: {total_segments}\")\n",
    "for ds, cnt in sorted(ds_counts.items()):\n",
    "    print(f\"  {ds}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51828f09",
   "metadata": {},
   "source": [
    "---\n",
    "> âš ï¸ **Phase Skip**: Phases 2â€“4 above can be skipped entirely if a previous run\n",
    "> completed them successfully. The notebook detects existing `ir_catalogue.json`,\n",
    "> `clap_cache.npz`, and `sterilize_state.json` to bypass redundant computation.\n",
    "> Previous run files are located at `/kaggle/input/notebooks/itorousa/genesis-data-run#`.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 5 â€” The Messy DSP Toolkit\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Random, destructive audio effects applied ONLY to the source_wet input\n",
    "# to simulate terrible recording environments.\n",
    "\n",
    "def add_noise(audio: np.ndarray, noise_type: str, snr_db: float) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Mix coloured noise at the target SNR (5â€“40 dB).\\\"\\\"\\\"\n",
    "    n = len(audio)\n",
    "    if noise_type == 'white':\n",
    "        noise = np.random.randn(n)\n",
    "    elif noise_type == 'pink':\n",
    "        freqs = np.fft.rfftfreq(n, 1 / SR)\n",
    "        freqs[0] = 1\n",
    "        S = 1.0 / np.sqrt(freqs)\n",
    "        noise = np.fft.irfft(S * np.exp(2j * np.pi * np.random.rand(len(S))))[:n]\n",
    "    elif noise_type == 'brown':\n",
    "        noise = np.cumsum(np.random.randn(n))\n",
    "        noise -= np.mean(noise)\n",
    "    elif noise_type == 'hvac':\n",
    "        noise = np.random.randn(n)\n",
    "        from scipy.signal import butter, sosfilt\n",
    "        sos = butter(4, [100, 1000], btype='band', fs=SR, output='sos')\n",
    "        noise = sosfilt(sos, noise)\n",
    "    elif noise_type == 'hum':\n",
    "        t = np.arange(n) / SR\n",
    "        base_freq = random.choice([50, 60])\n",
    "        noise = np.zeros(n)\n",
    "        for h in range(1, 6):\n",
    "            amp = 1.0 / h\n",
    "            noise += amp * np.sin(2 * np.pi * base_freq * h * t\n",
    "                                  + random.uniform(0, 2 * np.pi))\n",
    "    else:\n",
    "        noise = np.random.randn(n)\n",
    "\n",
    "    sig_power = np.mean(audio ** 2) + 1e-12\n",
    "    noise_power = np.mean(noise ** 2) + 1e-12\n",
    "    target_noise_power = sig_power / (10 ** (snr_db / 10))\n",
    "    noise = noise * np.sqrt(target_noise_power / noise_power)\n",
    "    return audio + noise.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_eq(audio: np.ndarray) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Random 3-band parametric EQ (simulates mic coloration).\\\"\\\"\\\"\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    bands = [(80, 300), (300, 3000), (3000, 12000)]\n",
    "    for lo, hi in bands:\n",
    "        gain_db = random.uniform(-6, 6)\n",
    "        if abs(gain_db) < 1:\n",
    "            continue\n",
    "        try:\n",
    "            sos = butter(2, [lo, hi], btype='band', fs=SR, output='sos')\n",
    "            band_sig = sosfilt(sos, audio)\n",
    "            gain_lin = 10 ** (gain_db / 20)\n",
    "            audio = audio + band_sig * (gain_lin - 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_highpass(audio: np.ndarray) -> np.ndarray:\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    cutoff = random.uniform(60, 300)\n",
    "    sos = butter(4, cutoff, btype='high', fs=SR, output='sos')\n",
    "    return sosfilt(sos, audio).astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_lowpass(audio: np.ndarray) -> np.ndarray:\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    cutoff = random.uniform(3000, 16000)\n",
    "    sos = butter(4, cutoff, btype='low', fs=SR, output='sos')\n",
    "    return sosfilt(sos, audio).astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_gain_jitter(audio: np.ndarray) -> np.ndarray:\n",
    "    gain_db = random.uniform(-6, 6)\n",
    "    return audio * (10 ** (gain_db / 20))\n",
    "\n",
    "\n",
    "def apply_bitcrush(audio: np.ndarray) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Bitcrushing / hard clipping â€” quantize audio resolution.\\\"\\\"\\\"\n",
    "    bits = random.randint(8, 16)\n",
    "    levels = 2 ** bits\n",
    "    return (np.round(audio * levels) / levels).astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_hard_clip(audio: np.ndarray) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Harsh mathematical clipping at a random threshold.\\\"\\\"\\\"\n",
    "    threshold = random.uniform(0.3, 0.9)\n",
    "    return np.clip(audio, -threshold, threshold).astype(np.float32)\n",
    "\n",
    "\n",
    "# Registry of all degradation functions\n",
    "DEGRADATIONS = {\n",
    "    'noise':     lambda a: add_noise(a, random.choice(\n",
    "                     ['white', 'pink', 'brown', 'hvac', 'hum']),\n",
    "                     random.uniform(5, 40)),\n",
    "    'eq':        apply_eq,\n",
    "    'highpass':  apply_highpass,\n",
    "    'lowpass':   apply_lowpass,\n",
    "    'gain':      apply_gain_jitter,\n",
    "    'bitcrush':  apply_bitcrush,\n",
    "    'clip':      apply_hard_clip,\n",
    "}\n",
    "\n",
    "def apply_random_degradations(audio: np.ndarray) -> Tuple[np.ndarray, List[str]]:\n",
    "    \\\"\\\"\\\"Apply a random subset of 3â€“6 degradations. Returns (degraded, names).\\\"\\\"\\\"\n",
    "    n_augs = random.randint(AUGMENTATIONS_MIN, AUGMENTATIONS_MAX)\n",
    "    chosen = random.sample(list(DEGRADATIONS.keys()), min(n_augs, len(DEGRADATIONS)))\n",
    "    for name in chosen:\n",
    "        audio = DEGRADATIONS[name](audio)\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return audio.astype(np.float32), chosen\n",
    "\n",
    "print(\"ğŸŸ¢ [Degradation] Toolkit loaded âœ“\")\n",
    "print(f\"  Available: {list(DEGRADATIONS.keys())}\")\n",
    "print(f\"  Per triple: {AUGMENTATIONS_MIN}â€“{AUGMENTATIONS_MAX} random degradations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974378e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 6 â€” The Training Data Engine (Triple Generation)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# For each dead dry vocal V:\n",
    "#   1) Pick one random Bad IR (A) and one random Target MIT IR (C)\n",
    "#   2) target_wet  = V âŠ› C  (ground truth)\n",
    "#   3) source_wet  = (V âŠ› A) Ã— wet_ratio + V Ã— (1 - wet_ratio)\n",
    "#   4) Degrade source_wet through DSP toolkit\n",
    "#   5) Fetch CLAP embedding for C\n",
    "#   6) QA â†’ pack into batch\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def convolve_and_trim(vocal: np.ndarray, ir: np.ndarray) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Convolve vocal with IR, trim to exactly 5s, peak-normalize.\\\"\\\"\\\"\n",
    "    wet = fftconvolve(vocal, ir, mode='full')[:CLIP_SAMPLES]\n",
    "    peak = np.max(np.abs(wet))\n",
    "    if peak > 1e-6:\n",
    "        wet = wet / peak\n",
    "    return wet.astype(np.float32)\n",
    "\n",
    "\n",
    "def audio_to_int16(audio: np.ndarray) -> np.ndarray:\n",
    "    \\\"\\\"\\\"Convert float32 [-1,1] to int16 for compact storage.\\\"\\\"\\\"\n",
    "    return (np.clip(audio, -1, 1) * 32767).astype(np.int16)\n",
    "\n",
    "\n",
    "# â”€â”€ Load sterilized vocal segments from disk â”€â”€\n",
    "print(\"ğŸŸ¢ [Engine] Loading sterilized vocal segments from disk...\")\n",
    "vocal_segments: List[dict] = []\n",
    "for pkl_file in sorted(STERILIZED_DIR.glob('sterilized_batch_*.pkl')):\n",
    "    with open(pkl_file, 'rb') as f:\n",
    "        vocal_segments.extend(pickle.load(f))\n",
    "print(f\"ğŸŸ¢ [Engine] Loaded {len(vocal_segments)} vocal segments\")\n",
    "\n",
    "# â”€â”€ Ensure IR audio is loaded (lazy-load if catalogue was from disk) â”€â”€\n",
    "if not all_irs or not any('audio' in v for v in all_irs.values() if isinstance(v, dict)):\n",
    "    print(\"ğŸŸ¢ [Engine] Lazy-loading IR audio for convolution...\")\n",
    "    def _load_ir(ir_id: str) -> Optional[np.ndarray]:\n",
    "        if ir_id.startswith('mit_'):\n",
    "            for f in MIT_IR_DIR.rglob('*.wav'):\n",
    "                if f.stem == ir_id[4:]:\n",
    "                    audio, _ = librosa.load(str(f), sr=SR, mono=True)\n",
    "                    peak = np.max(np.abs(audio))\n",
    "                    return (audio / peak).astype(np.float32) if peak > 1e-6 else audio\n",
    "        elif ir_id.startswith('user_'):\n",
    "            for ext in ('*.irs', '*.wav'):\n",
    "                for f in PATHS['irs'].rglob(ext):\n",
    "                    if f.stem == ir_id[5:]:\n",
    "                        audio, _ = librosa.load(str(f), sr=SR, mono=True)\n",
    "                        peak = np.max(np.abs(audio))\n",
    "                        return (audio / peak).astype(np.float32) if peak > 1e-6 else audio\n",
    "        return None\n",
    "\n",
    "    all_irs = {}\n",
    "    for ir_id in target_pool + bad_pool:\n",
    "        a = _load_ir(ir_id)\n",
    "        if a is not None:\n",
    "            all_irs[ir_id] = {'audio': a, 'source': ir_catalogue.get(ir_id, {}).get('source', 'unknown')}\n",
    "    print(f\"ğŸŸ¢ [Engine] Loaded {len(all_irs)} IRs\")\n",
    "\n",
    "# â”€â”€ Shuffle segments for reproducibility â”€â”€\n",
    "random.shuffle(vocal_segments)\n",
    "\n",
    "# â”€â”€ Resume from checkpoint â”€â”€\n",
    "start_idx = ckpt.get('vocal_cursor', 0)\n",
    "batch_id  = ckpt.get('batch_id', 0)\n",
    "total     = ckpt.get('triples_completed', 0)\n",
    "\n",
    "# â”€â”€ Batch accumulators â”€â”€\n",
    "batch_sources:  List[np.ndarray] = []\n",
    "batch_targets:  List[np.ndarray] = []\n",
    "batch_claps:    List[np.ndarray] = []\n",
    "batch_meta:     List[dict]       = []\n",
    "\n",
    "# â”€â”€ Load CLAP cache â”€â”€\n",
    "clap_cache_data = dict(np.load(CLAP_CACHE_PATH))\n",
    "\n",
    "print(f\"\\nğŸŸ¢ [Engine] Starting triple generation...\")\n",
    "print(f\"  Vocal segments: {len(vocal_segments)}\")\n",
    "print(f\"  Bad pool: {len(bad_pool)}, Target pool: {len(target_pool)}\")\n",
    "print(f\"  Resuming from index {start_idx}, batch {batch_id}\\n\")\n",
    "\n",
    "t_start = time.time()\n",
    "triples_this_run = 0\n",
    "skipped = 0\n",
    "\n",
    "for seg_idx in range(start_idx, len(vocal_segments)):\n",
    "    # â”€â”€ Budget check â”€â”€\n",
    "    if seg_idx % 100 == 0 and get_output_size_gb() > MAX_OUTPUT_GB:\n",
    "        print(f\"\\nâš  Output size limit reached ({MAX_OUTPUT_GB} GB). Stopping.\")\n",
    "        break\n",
    "\n",
    "    seg = vocal_segments[seg_idx]\n",
    "    V = seg['audio']\n",
    "\n",
    "    # â”€â”€ Pick random bad IR & target IR â”€â”€\n",
    "    bad_ir_id    = random.choice(bad_pool)\n",
    "    target_ir_id = random.choice(target_pool)\n",
    "\n",
    "    if bad_ir_id not in all_irs or target_ir_id not in all_irs:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    ir_A = all_irs[bad_ir_id]['audio']\n",
    "    ir_C = all_irs[target_ir_id]['audio']\n",
    "\n",
    "    # â”€â”€ Create target audio (ground truth): V âŠ› target_IR â”€â”€\n",
    "    target_wet = convolve_and_trim(V, ir_C)\n",
    "\n",
    "    # â”€â”€ Create messy source: (V âŠ› bad_IR) mixed with raw V â”€â”€\n",
    "    source_wet = convolve_and_trim(V, ir_A)\n",
    "    wet_dry = random.uniform(0.3, 1.0)\n",
    "    source_wet = wet_dry * source_wet + (1 - wet_dry) * V[:len(source_wet)]\n",
    "\n",
    "    # â”€â”€ Degrade source only â”€â”€\n",
    "    source_wet, aug_names = apply_random_degradations(source_wet)\n",
    "\n",
    "    # â”€â”€ QA: reject silent / NaN triples â”€â”€\n",
    "    src_rms = np.sqrt(np.mean(source_wet ** 2))\n",
    "    tgt_rms = np.sqrt(np.mean(target_wet ** 2))\n",
    "    if src_rms < 1e-4 or tgt_rms < 1e-4:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    if np.any(np.isnan(source_wet)) or np.any(np.isnan(target_wet)):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # â”€â”€ CLAP embedding for target IR â”€â”€\n",
    "    target_clap = clap_cache_data.get(target_ir_id)\n",
    "    if target_clap is None:\n",
    "        target_clap = get_clap_audio_embedding(target_wet)\n",
    "\n",
    "    # â”€â”€ Quantize & accumulate â”€â”€\n",
    "    batch_sources.append(audio_to_int16(source_wet))\n",
    "    batch_targets.append(audio_to_int16(target_wet))\n",
    "    batch_claps.append(target_clap)\n",
    "    batch_meta.append({\n",
    "        'vocal_file':   seg['file'],\n",
    "        'dataset':      seg['dataset'],\n",
    "        'bad_ir':       bad_ir_id,\n",
    "        'target_ir':    target_ir_id,\n",
    "        'wet_dry':      round(wet_dry, 3),\n",
    "        'degradations': aug_names,\n",
    "    })\n",
    "\n",
    "    # â”€â”€ ğŸ’¾ Flush batch when full â”€â”€\n",
    "    if len(batch_sources) >= TRIPLES_PER_BATCH:\n",
    "        batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n",
    "        np.savez(\n",
    "            batch_path,\n",
    "            source_audio  = np.stack(batch_sources),\n",
    "            target_audio  = np.stack(batch_targets),\n",
    "            target_clap   = np.stack(batch_claps),\n",
    "        )\n",
    "        meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n",
    "        with open(meta_path, 'w') as f:\n",
    "            json.dump(batch_meta, f)\n",
    "\n",
    "        triples_this_run += len(batch_sources)\n",
    "        total += len(batch_sources)\n",
    "        batch_id += 1\n",
    "\n",
    "        ckpt.update({\n",
    "            'batch_id': batch_id,\n",
    "            'triples_completed': total,\n",
    "            'vocal_cursor': seg_idx + 1,\n",
    "        })\n",
    "        save_checkpoint(ckpt)\n",
    "\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = triples_this_run / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Batch {batch_id-1:4d} saved | \"\n",
    "              f\"triples: {total:,} total, {triples_this_run:,} this run | \"\n",
    "              f\"{rate:.0f}/sec | \"\n",
    "              f\"{get_output_size_gb():.1f} GB used\")\n",
    "\n",
    "        batch_sources.clear()\n",
    "        batch_targets.clear()\n",
    "        batch_claps.clear()\n",
    "        batch_meta.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c46af",
   "metadata": {},
   "source": [
    "---\n",
    "> âš ï¸ **Phase Skip**: Phases 5â€“6 above can be skipped if a previous run already\n",
    "> generated sufficient training batches. Attach the previous output as input and\n",
    "> the checkpoint system will resume from where it left off.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PHASE 7 â€” Manifest & Pipeline Conclusion\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Flush any remaining triples (<500) â”€â”€\n",
    "if batch_sources:\n",
    "    batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n",
    "    np.savez(\n",
    "        batch_path,\n",
    "        source_audio = np.stack(batch_sources),\n",
    "        target_audio = np.stack(batch_targets),\n",
    "        target_clap  = np.stack(batch_claps),\n",
    "    )\n",
    "    meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(batch_meta, f)\n",
    "\n",
    "    triples_this_run += len(batch_sources)\n",
    "    total += len(batch_sources)\n",
    "    batch_id += 1\n",
    "\n",
    "    ckpt.update({\n",
    "        'batch_id': batch_id,\n",
    "        'triples_completed': total,\n",
    "        'vocal_cursor': len(vocal_segments),\n",
    "    })\n",
    "    save_checkpoint(ckpt)\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "\n",
    "# â”€â”€ SHA-256 checksums â”€â”€\n",
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "manifest = {\n",
    "    'run_number':        ckpt['run_number'],\n",
    "    'triples_total':     ckpt['triples_completed'],\n",
    "    'batches':           ckpt['batch_id'],\n",
    "    'sample_rate':       SR,\n",
    "    'clip_seconds':      CLIP_SEC,\n",
    "    'clip_samples':      CLIP_SAMPLES,\n",
    "    'clap_dim':          CLAP_DIM,\n",
    "    'bad_pool_size':     len(bad_pool),\n",
    "    'target_pool_size':  len(target_pool),\n",
    "    'output_size_gb':    round(get_output_size_gb(), 3),\n",
    "    'batch_checksums':   {},\n",
    "}\n",
    "\n",
    "print(\"[Manifest] Computing SHA-256 checksums...\")\n",
    "for f in sorted(BATCH_DIR.glob('batch_*.npz')):\n",
    "    manifest['batch_checksums'][f.name] = sha256_file(f)\n",
    "\n",
    "with open(OUTPUT / 'manifest.json', 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"[Manifest] Saved âœ“\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  GENESIS DATA CURATION â€” RUN {ckpt['run_number']} COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total triples:   {manifest['triples_total']:,}\")\n",
    "print(f\"  Batches:          {manifest['batches']}\")\n",
    "print(f\"  Output size:      {manifest['output_size_gb']:.2f} GB\")\n",
    "print(f\"  CLAP dim:         {manifest['clap_dim']}\")\n",
    "print(f\"  Elapsed:          {elapsed/60:.1f} min\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if ckpt['vocal_cursor'] < len(vocal_segments):\n",
    "    remaining = len(vocal_segments) - ckpt['vocal_cursor']\n",
    "    print(f\"\\nâš   {remaining} vocal segments remaining.\")\n",
    "    print(f\"  To continue:\")\n",
    "    print(f\"    1. Save this notebook's output as a Kaggle dataset\")\n",
    "    print(f\"    2. Update PREV_RUN_PATH in Cell 3 to point to it\")\n",
    "    print(f\"    3. Create a new notebook, attach the same input datasets\")\n",
    "    print(f\"    4. Run all cells â€” the checkpoint system will resume\")\n",
    "else:\n",
    "    print(f\"\\nğŸŸ¢ All vocal segments processed. Dataset complete!\")\n",
    "    print(f\"   Output is ready for Genesis's STFT Dataloader in the training phase.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148d746",
   "metadata": {},
   "source": [
    "## ğŸ”— Checkpoint Chaining (20 GB Limit)\n",
    "\n",
    "If the output hit the size limit before processing all vocals:\n",
    "\n",
    "1. **Save this notebook's output** as a Kaggle dataset (e.g. `genesis-data-run1`)\n",
    "2. **Create a new notebook** (or re-run this one) and attach:\n",
    "   - All the same input datasets (IRs, LJSpeech, VCTK, Language Identifier)\n",
    "   - The previous output as input (update `PREV_RUN_PATH` in Cell 3)\n",
    "3. **Run all cells** â€” the checkpoint system automatically skips completed work\n",
    "\n",
    "Each run produces ~19 GB of training triples. Chain as many times as needed.\n",
    "\n",
    "### Using the Data in Training\n",
    "\n",
    "```python\n",
    "# In the training notebook, load all batches from all runs:\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "run_dirs = [\n",
    "    Path('/kaggle/input/genesis-data-run1/batches'),\n",
    "    Path('/kaggle/input/genesis-data-run2/batches'),\n",
    "    # ... add more runs\n",
    "]\n",
    "\n",
    "for run_dir in run_dirs:\n",
    "    for batch_file in sorted(run_dir.glob('batch_*.npz')):\n",
    "        data = np.load(batch_file)\n",
    "        source_audio = data['source_audio']   # (N, 240000) int16\n",
    "        target_audio = data['target_audio']   # (N, 240000) int16\n",
    "        target_clap  = data['target_clap']    # (N, CLAP_DIM) float32\n",
    "        # Convert int16 back to float32: audio = source_audio.astype(np.float32) / 32767\n",
    "        # Compute STFT on-the-fly during training for memory efficiency\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
