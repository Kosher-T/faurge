{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14927063,"datasetId":9551535,"databundleVersionId":15794187},{"sourceType":"datasetVersion","sourceId":7085845,"datasetId":4060468,"databundleVersionId":7173592},{"sourceType":"datasetVersion","sourceId":101413,"datasetId":53291,"databundleVersionId":103953},{"sourceType":"datasetVersion","sourceId":4588404,"datasetId":2675000,"databundleVersionId":4649793}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9595e3ad","cell_type":"code","source":"!pip install -q noisereduce pyloudnorm soundfile librosa transformers torch","metadata":{"execution":{"iopub.status.busy":"2026-02-24T05:39:02.276080Z","iopub.execute_input":"2026-02-24T05:39:02.276771Z","iopub.status.idle":"2026-02-24T05:39:06.409252Z","shell.execute_reply.started":"2026-02-24T05:39:02.276741Z","shell.execute_reply":"2026-02-24T05:39:06.407729Z"},"trusted":true},"outputs":[],"execution_count":10},{"id":"f06df653-775d-405b-82e9-996732efb8af","cell_type":"markdown","source":"### Wipe Kaggle's Output Storage","metadata":{}},{"id":"2184223c-ebcf-4d96-a569-e0a08aa1ea13","cell_type":"code","source":"import os\nimport shutil\n\nworking_dir = '/kaggle/working'\n\nprint(\"ðŸŸ¡ Wiping Kaggle working directory...\")\n\nfor item in os.listdir(working_dir):\n    item_path = os.path.join(working_dir, item)\n    try:\n        if os.path.isfile(item_path) or os.path.islink(item_path):\n            os.unlink(item_path)\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)\n    except Exception as e:\n        print(f\"ðŸ”´ Failed to delete {item_path}. Reason: {e}\")\n\nprint(\"ðŸŸ¢ Kaggle working directory is completely clean.\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T05:39:06.410986Z","iopub.execute_input":"2026-02-24T05:39:06.411246Z","iopub.status.idle":"2026-02-24T05:39:07.646024Z","shell.execute_reply.started":"2026-02-24T05:39:06.411222Z","shell.execute_reply":"2026-02-24T05:39:07.644550Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ðŸŸ¡ Wiping Kaggle working directory...\nðŸŸ¢ Kaggle working directory is completely clean.\n","output_type":"stream"}],"execution_count":11},{"id":"3a4d9a93-0729-4381-92e6-21f1101d510b","cell_type":"markdown","source":"### Wipe a Specific Folder/File","metadata":{}},{"id":"79622b28-b88f-47e0-a74a-d5e635002071","cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\ndef delete_target(target_path):\n    target = Path(target_path)\n    \n    if not target.exists():\n        print(f\"ðŸŸ¡ Target does not exist: {target}\")\n        return\n        \n    try:\n        if target.is_file() or target.is_symlink():\n            target.unlink()\n            print(f\"ðŸŸ¢ Successfully deleted file: {target}\")\n        elif target.is_dir():\n            shutil.rmtree(target)\n            print(f\"ðŸŸ¢ Successfully deleted directory: {target}\")\n    except Exception as e:\n        print(f\"ðŸ”´ Failed to delete {target}. Reason: {e}\")\n\n# Just drop your path here\ndelete_target('/kaggle/working/your_folder_or_file_here')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T05:39:07.647267Z","iopub.execute_input":"2026-02-24T05:39:07.647512Z","iopub.status.idle":"2026-02-24T05:39:07.655967Z","shell.execute_reply.started":"2026-02-24T05:39:07.647489Z","shell.execute_reply":"2026-02-24T05:39:07.654776Z"}},"outputs":[{"name":"stdout","text":"ðŸŸ¡ Target does not exist: /kaggle/working/your_folder_or_file_here\n","output_type":"stream"}],"execution_count":12},{"id":"34b748e3","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 0 â€” Environment & Path Configuration\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport os, json, hashlib, time, random, warnings, shutil, pickle\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple\nfrom collections import defaultdict\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom scipy.signal import fftconvolve\nimport noisereduce as nr\n\nwarnings.filterwarnings('ignore')\nrandom.seed(42)\nnp.random.seed(42)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# KAGGLE INPUT PATHS  (datasets attached to this notebook)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPATHS = {\n    'irs':       Path('/kaggle/input/datasets/itorousa/impulse-responses'),\n    'mit_raw':   Path('/kaggle/input/notebooks/itorousa/irs/mit_irs/Audio'\n                      'https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip'),\n    'ljspeech':  Path('/kaggle/input/datasets/dromosys/ljspeech'),\n    'vctk':      Path('/kaggle/input/datasets/kynthesis/vctk-corpus'\n                      '/VCTK-Corpus/wav48'),\n    'langid_en': Path('/kaggle/input/datasets/shrivatssudhir'\n                      '/language-identifier/english/clips'),\n}\n\n# Chaining â€” previous run's output (attached as input to this notebook).\n# Adjust the path for each subsequent run.\nPREV_RUN_PATH = Path('/kaggle/input/notebooks/itorousa/genesis-data-run1')\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# OUTPUT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nOUTPUT          = Path('/kaggle/working')\nBATCH_DIR       = OUTPUT / 'batches'\nCLAP_DIR        = OUTPUT / 'clap_model'\nSTERILIZED_DIR  = OUTPUT / 'sterilized_batches'\nMIT_IR_DIR      = OUTPUT / 'irs' / 'mit_irs'\n\nfor d in [BATCH_DIR, CLAP_DIR, STERILIZED_DIR, MIT_IR_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# AUDIO PARAMETERS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSR            = 48_000\nCLIP_SEC      = 5.0\nCLIP_SAMPLES  = int(SR * CLIP_SEC)    # 240,000 samples\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# BUDGET & BATCHING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTRIPLES_PER_BATCH  = 500\nMAX_OUTPUT_GB      = 19.0             # safety margin under 20 GB cap\nAUGMENTATIONS_MIN  = 3\nAUGMENTATIONS_MAX  = 6\n\nprint(f\"SR={SR}, CLIP_SEC={CLIP_SEC}, CLIP_SAMPLES={CLIP_SAMPLES}\")\nprint(f\"Output budget: {MAX_OUTPUT_GB} GB, batch size: {TRIPLES_PER_BATCH}\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-24T05:39:07.658259Z","iopub.execute_input":"2026-02-24T05:39:07.658583Z","iopub.status.idle":"2026-02-24T05:39:07.683460Z","shell.execute_reply.started":"2026-02-24T05:39:07.658549Z","shell.execute_reply":"2026-02-24T05:39:07.682141Z"},"trusted":true},"outputs":[{"name":"stdout","text":"SR=48000, CLIP_SEC=5.0, CLIP_SAMPLES=240000\nOutput budget: 19.0 GB, batch size: 500\n","output_type":"stream"}],"execution_count":13},{"id":"ab6336cd","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 1 â€” Aggressive Checkpoint Initialization & Hardware Protection\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nCHECKPOINT_PATH = OUTPUT / 'checkpoint.json'\n\ndef load_checkpoint() -> dict:\n    '''Load checkpoint: previous run â†’ current working dir â†’ fresh start.'''\n    # 1) Check previous run first (chaining)\n    prev_ckpt = PREV_RUN_PATH / 'checkpoint.json'\n    if prev_ckpt.exists():\n        with open(prev_ckpt) as f:\n            ckpt = json.load(f)\n        ckpt['run_number'] += 1\n        print(f\"[Checkpoint] â™» Resuming from previous run: \"\n              f\"{ckpt['triples_completed']} triples done\")\n        return ckpt\n\n    # 2) Check current working dir (kernel restart mid-session)\n    if CHECKPOINT_PATH.exists():\n        with open(CHECKPOINT_PATH) as f:\n            return json.load(f)\n\n    # 3) Fresh start\n    return {\n        'batch_id': 0,\n        'triples_completed': 0,\n        'vocal_cursor': 0,\n        'run_number': 1,\n    }\n\ndef save_checkpoint(ckpt: dict):\n    with open(CHECKPOINT_PATH, 'w') as f:\n        json.dump(ckpt, f, indent=2)\n\ndef get_output_size_gb() -> float:\n    '''Calculate the exact size of /kaggle/working in GB.'''\n    total = sum(f.stat().st_size for f in OUTPUT.rglob('*') if f.is_file())\n    return total / (1024 ** 3)\n\nckpt = load_checkpoint()\nprint(f\"[Checkpoint] Run #{ckpt['run_number']}, \"\n      f\"{ckpt['triples_completed']} triples completed so far, \"\n      f\"starting at vocal cursor {ckpt['vocal_cursor']}\")\n","metadata":{"execution":{"iopub.status.busy":"2026-02-24T05:39:07.685161Z","iopub.execute_input":"2026-02-24T05:39:07.685559Z","iopub.status.idle":"2026-02-24T05:39:07.710684Z","shell.execute_reply.started":"2026-02-24T05:39:07.685523Z","shell.execute_reply":"2026-02-24T05:39:07.709133Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[Checkpoint] Run #1, 0 triples completed so far, starting at vocal cursor 0\n","output_type":"stream"}],"execution_count":14},{"id":"85eb9eb0","cell_type":"code","source":"import urllib.request\nimport zipfile\nimport shutil\nimport json\nimport librosa\nimport numpy as np\nfrom typing import Dict, List, Optional\nfrom pathlib import Path\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 2 â€” Impulse Response Acquisition & Pooling\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nCATALOGUE_PATH = OUTPUT / 'ir_catalogue.json'\nMIT_RAW_URL = 'https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip'\n\n# â”€â”€â”€ Fast path: reload from existing catalogue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif CATALOGUE_PATH.exists():\n    print(\"ðŸŸ¢ Found existing ir_catalogue.json â€” skipping audio-loading block.\")\n    with open(CATALOGUE_PATH, 'r') as f:\n        ir_catalogue = json.load(f)\n\n    all_irs: Dict[str, dict] = {}   # will NOT have audio yet; lazy-loaded later\n    bad_pool:    List[str] = []\n    target_pool: List[str] = []\n\n    for ir_id, feats in ir_catalogue.items():\n        if feats['source'] == 'mit':\n            target_pool.append(ir_id)\n        elif feats['rt60'] > 0.25 or feats['c50'] < 8:\n            bad_pool.append(ir_id)\n        else:\n            bad_pool.append(ir_id)\n\n    # Safety: ensure target pool is adequate\n    if len(target_pool) < 20:\n        mit_ids = [k for k, v in ir_catalogue.items() if v['source'] == 'mit']\n        target_pool = mit_ids if mit_ids else list(ir_catalogue.keys())[:50]\n        bad_pool = [k for k in ir_catalogue if k not in target_pool]\n\n    print(f\"ðŸŸ¢ Loaded {len(ir_catalogue)} IRs from catalogue\")\n    print(f\"ðŸŸ¢ Bad pool: {len(bad_pool)} | Target pool: {len(target_pool)}\")\n    _catalogue_loaded_from_disk = True\n\nelse:\n    _catalogue_loaded_from_disk = False\n\n    # â”€â”€â”€ Stage 1: Copy / acquire MIT IRs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    prev_mit = PREV_RUN_PATH / 'irs' / 'mit_irs'\n    MIT_IR_DIR.mkdir(parents=True, exist_ok=True)\n    \n    if prev_mit.exists() and not any(MIT_IR_DIR.iterdir()):\n        print(\"ðŸŸ¢ [Acquire] Copying MIT IRs from previous run...\")\n        shutil.copytree(prev_mit, MIT_IR_DIR, dirs_exist_ok=True)\n        print(\"ðŸŸ¢ [Acquire] MIT IRs staged from previous run âœ“\")\n    elif not any(MIT_IR_DIR.iterdir()):\n        print(\"ðŸŸ¢ [Acquire] Downloading MIT IRs...\")\n        zip_path = MIT_IR_DIR / 'mit_irs.zip'\n        urllib.request.urlretrieve(MIT_RAW_URL, str(zip_path))\n        with zipfile.ZipFile(zip_path, 'r') as zf:\n            zf.extractall(MIT_IR_DIR)\n        zip_path.unlink()\n        print(\"ðŸŸ¢ [Acquire] MIT IRs downloaded and extracted âœ“\")\n    else:\n        print(\"ðŸŸ¢ [Acquire] MIT IRs already staged or unavailable\")\n\n    # â”€â”€â”€ Stage 2: Load & normalize every IR to 48 kHz mono â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def load_and_normalize_ir(filepath: Path, target_sr: int = SR) -> Optional[np.ndarray]:\n        '''Load an IR file, force mono / target SR, peak-normalize.'''\n        try:\n            audio, _ = librosa.load(str(filepath), sr=target_sr, mono=True)\n            if len(audio) < 64:\n                return None\n            peak = np.max(np.abs(audio))\n            if peak > 1e-6:\n                audio = audio / peak\n            return audio.astype(np.float32)\n        except Exception:\n            return None\n\n    print(\"\\nðŸŸ¢ [Acquire] Loading all impulse responses...\")\n    all_irs: Dict[str, dict] = {}\n\n    # Bad pool â€” user's unlabeled IRs\n    for ext in ('*.irs', '*.wav'):\n        for f in sorted(PATHS['irs'].rglob(ext)):\n            ir = load_and_normalize_ir(f)\n            if ir is not None:\n                all_irs[f'user_{f.stem}'] = {'audio': ir, 'source': 'user'}\n\n    # Target pool â€” MIT labeled IRs\n    if MIT_IR_DIR.exists():\n        for f in sorted(MIT_IR_DIR.rglob('*.wav')):\n            ir = load_and_normalize_ir(f)\n            if ir is not None:\n                all_irs[f'mit_{f.stem}'] = {'audio': ir, 'source': 'mit'}\n\n    print(f\"ðŸŸ¢ [Acquire] Loaded {len(all_irs)} IRs \"\n          f\"(user/bad: {sum(1 for v in all_irs.values() if v['source']=='user')}, \"\n          f\"MIT/target: {sum(1 for v in all_irs.values() if v['source']=='mit')})\")\n\n    # â”€â”€â”€ Stage 3: Classify & build pools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    def compute_ir_features(ir_audio: np.ndarray, sr: int = SR) -> dict:\n        '''Compute RT60 estimate, spectral centroid, and clarity (C50).'''\n        energy = ir_audio ** 2\n        cumsum = np.cumsum(energy[::-1])[::-1]\n\n        # RT60 â€” time for energy to decay 60 dB\n        rt60 = len(ir_audio) / sr\n        if cumsum[0] > 1e-10:\n            decay_db = 10 * np.log10(cumsum / cumsum[0] + 1e-12)\n            idx = np.where(decay_db < -60)[0]\n            if len(idx) > 0:\n                rt60 = idx[0] / sr\n\n        # Spectral centroid\n        S = np.abs(np.fft.rfft(ir_audio))\n        freqs = np.fft.rfftfreq(len(ir_audio), 1 / sr)\n        centroid = float(np.sum(freqs * S) / (np.sum(S) + 1e-10))\n\n        # Clarity C50 â€” early-to-late energy ratio at 50 ms\n        split = int(0.05 * sr)\n        early = np.sum(ir_audio[:split] ** 2) + 1e-12\n        late  = np.sum(ir_audio[split:] ** 2) + 1e-12\n        c50   = float(10 * np.log10(early / late))\n\n        return {\n            'rt60': round(rt60, 4),\n            'centroid': round(centroid, 1),\n            'c50': round(c50, 2),\n            'length_sec': round(len(ir_audio) / sr, 4),\n        }\n\n    ir_catalogue = {}\n    bad_pool:    List[str] = []\n    target_pool: List[str] = []\n\n    for ir_id, ir_data in all_irs.items():\n        feats = compute_ir_features(ir_data['audio'])\n        ir_data['features'] = feats\n        ir_catalogue[ir_id] = {'source': ir_data['source'], **feats}\n\n        # MIT IRs â†’ target pool (labeled ground-truth rooms)\n        # User IRs â†’ bad pool (unlabeled/degraded)\n        if ir_data['source'] == 'mit':\n            target_pool.append(ir_id)\n        else:\n            bad_pool.append(ir_id)\n\n    # Safety: ensure target pool is adequate\n    if len(target_pool) < 20:\n        mit_ids = sorted(\n            [k for k, v in all_irs.items() if v['source'] == 'mit'],\n            key=lambda k: all_irs[k]['features']['c50'], reverse=True\n        )\n        target_pool = mit_ids if mit_ids else list(all_irs.keys())[:50]\n        bad_pool = [k for k in all_irs if k not in target_pool]\n\n    print(f\"ðŸŸ¢ [Classify] Bad pool:    {len(bad_pool)} IRs\")\n    print(f\"ðŸŸ¢ [Classify] Target pool: {len(target_pool)} IRs\")\n\n    # ðŸ’¾ Save catalogue\n    with open(CATALOGUE_PATH, 'w') as f:\n        json.dump(ir_catalogue, f, indent=2)\n    print(\"ðŸŸ¢ [Classify] ir_catalogue.json saved âœ“\")","metadata":{"execution":{"iopub.status.busy":"2026-02-24T05:39:07.712079Z","iopub.execute_input":"2026-02-24T05:39:07.712385Z","iopub.status.idle":"2026-02-24T05:39:13.758472Z","shell.execute_reply.started":"2026-02-24T05:39:07.712353Z","shell.execute_reply":"2026-02-24T05:39:13.757139Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ðŸŸ¢ [Acquire] Downloading MIT IRs...\nðŸŸ¢ [Acquire] MIT IRs downloaded and extracted âœ“\n\nðŸŸ¢ [Acquire] Loading all impulse responses...\nðŸŸ¢ [Acquire] Loaded 659 IRs (user/bad: 389, MIT/target: 270)\nðŸŸ¢ [Classify] Bad pool:    389 IRs\nðŸŸ¢ [Classify] Target pool: 270 IRs\nðŸŸ¢ [Classify] ir_catalogue.json saved âœ“\n","output_type":"stream"}],"execution_count":15},{"id":"8fc6437b","cell_type":"code","source":"import torch\nfrom transformers import ClapModel, ClapProcessor\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 3 â€” CLAP Target Embedding Cache\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCLAP_MODEL_ID = \"laion/larger_clap_music_and_speech\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ðŸŸ¢ [CLAP] Device: {device}\")\n\n# â”€â”€â”€ Model loading chain: local â†’ previous run â†’ download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprev_clap = PREV_RUN_PATH / 'clap_model'\n\nif (CLAP_DIR / 'config.json').exists():\n    print(\"ðŸŸ¢ [CLAP] Loading frozen model from current working dir...\")\n    clap_processor = ClapProcessor.from_pretrained(CLAP_DIR)\n    clap_model = ClapModel.from_pretrained(CLAP_DIR).to(device).eval()\n\nelif prev_clap.exists() and (prev_clap / 'config.json').exists():\n    print(\"ðŸŸ¢ [CLAP] Copying frozen model from previous run...\")\n    shutil.copytree(prev_clap, CLAP_DIR, dirs_exist_ok=True)\n    clap_processor = ClapProcessor.from_pretrained(CLAP_DIR)\n    clap_model = ClapModel.from_pretrained(CLAP_DIR).to(device).eval()\n\nelse:\n    print(\"ðŸŸ¢ [CLAP] Downloading model from Hugging Face...\")\n    clap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)\n    clap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(device).eval()\n    clap_model.save_pretrained(CLAP_DIR)\n    clap_processor.save_pretrained(CLAP_DIR)\n    print(f\"ðŸŸ¢ [CLAP] Frozen model saved to {CLAP_DIR}\")\n\nCLAP_DIM = clap_model.config.projection_dim\nprint(f\"ðŸŸ¢ [CLAP] Loaded â€” embedding dim = {CLAP_DIM}\")\n\n\ndef get_clap_audio_embedding(audio: np.ndarray, sr: int = SR) -> np.ndarray:\n    \"\"\"\n    Encode audio through CLAP's audio tower. Returns (CLAP_DIM,) float32.\n\n    âš  Critical: extract .pooler_output from the wrapper before calling .cpu().\n    \"\"\"\n    inputs = clap_processor(\n        audio=audio, sampling_rate=sr, return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = clap_model.get_audio_features(**inputs)\n        emb = outputs.pooler_output\n    return emb.cpu().numpy().flatten().astype(np.float32)\n\n\n# â”€â”€â”€ Pre-compute CLAP embeddings for every target-pool IR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Convolve each IR with 3s of white noise so CLAP has a rich scene.\nCLAP_CACHE_PATH = OUTPUT / 'clap_cache.npz'\nprev_cache = PREV_RUN_PATH / 'clap_cache.npz'\n\nif CLAP_CACHE_PATH.exists():\n    print(\"ðŸŸ¢ [CLAP] Found existing clap_cache.npz â€” skipping embedding compute.\")\n    clap_cache = dict(np.load(CLAP_CACHE_PATH))\n    print(f\"ðŸŸ¢ [CLAP] Loaded {len(clap_cache)} embeddings from cache âœ“  (dim={CLAP_DIM})\")\n\nelif prev_cache.exists():\n    print(\"ðŸŸ¢ [CLAP] Copying clap_cache.npz from previous run...\")\n    shutil.copy2(prev_cache, CLAP_CACHE_PATH)\n    clap_cache = dict(np.load(CLAP_CACHE_PATH))\n    print(f\"ðŸŸ¢ [CLAP] Loaded {len(clap_cache)} embeddings from previous run âœ“\")\n\nelse:\n    print(f\"\\nðŸŸ¢ [CLAP] Pre-computing embeddings for {len(target_pool)} target IRs...\")\n\n    # Need IR audio loaded â€” if catalogue was loaded from disk, lazy-load now\n    if _catalogue_loaded_from_disk:\n        print(\"   (Lazy-loading IR audio for embedding computation...)\")\n        def _lazy_load_ir(ir_id: str) -> Optional[np.ndarray]:\n            if ir_id.startswith('mit_'):\n                for f in MIT_IR_DIR.rglob('*.wav'):\n                    if f.stem == ir_id[4:]:\n                        audio, _ = librosa.load(str(f), sr=SR, mono=True)\n                        peak = np.max(np.abs(audio))\n                        return (audio / peak).astype(np.float32) if peak > 1e-6 else audio\n            elif ir_id.startswith('user_'):\n                for ext in ('*.irs', '*.wav'):\n                    for f in PATHS['irs'].rglob(ext):\n                        if f.stem == ir_id[5:]:\n                            audio, _ = librosa.load(str(f), sr=SR, mono=True)\n                            peak = np.max(np.abs(audio))\n                            return (audio / peak).astype(np.float32) if peak > 1e-6 else audio\n            return None\n\n        all_irs = {}\n        for ir_id in target_pool + bad_pool:\n            a = _lazy_load_ir(ir_id)\n            if a is not None:\n                all_irs[ir_id] = {'audio': a, 'source': ir_catalogue[ir_id]['source']}\n\n    ref_noise = np.random.randn(SR * 3).astype(np.float32) * 0.1\n\n    clap_cache: Dict[str, np.ndarray] = {}\n    for i, ir_id in enumerate(target_pool):\n        if ir_id not in all_irs:\n            continue\n        ir_audio = all_irs[ir_id]['audio']\n        scene = fftconvolve(ref_noise, ir_audio, mode='full')[:SR * 3]\n        scene = scene / (np.max(np.abs(scene)) + 1e-8)\n        clap_cache[ir_id] = get_clap_audio_embedding(scene)\n        if (i + 1) % 50 == 0:\n            print(f\"  {i+1}/{len(target_pool)}\")\n\n    np.savez(CLAP_CACHE_PATH, **clap_cache)\n    print(f\"ðŸŸ¢ [CLAP] Cached {len(clap_cache)} embeddings âœ“  (dim={CLAP_DIM})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T05:39:46.866334Z","iopub.execute_input":"2026-02-24T05:39:46.866654Z","iopub.status.idle":"2026-02-24T05:42:39.639760Z","shell.execute_reply.started":"2026-02-24T05:39:46.866629Z","shell.execute_reply":"2026-02-24T05:42:39.638628Z"}},"outputs":[{"name":"stdout","text":"ðŸŸ¢ [CLAP] Device: cpu\nðŸŸ¢ [CLAP] Loading frozen model from current working dir...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/555 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11707899883d4816af9d222d2c8be934"}},"metadata":{}},{"name":"stdout","text":"ðŸŸ¢ [CLAP] Loaded â€” embedding dim = 512\n\nðŸŸ¢ [CLAP] Pre-computing embeddings for 270 target IRs...\n  50/270\n  100/270\n  150/270\n  200/270\n  250/270\nðŸŸ¢ [CLAP] Cached 270 embeddings âœ“  (dim=512)\n","output_type":"stream"}],"execution_count":17},{"id":"1542e8be","cell_type":"code","source":"import pyloudnorm as pyln\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PHASE 4 â€” Vocal Sterilization & \"Dead\" Audio Guarantee\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nmeter = pyln.Meter(SR)\nSTATE_FILE = STERILIZED_DIR / 'sterilize_state.json'\n\n# â”€â”€â”€ Check if a previous run already completed sterilization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprev_sterilized = PREV_RUN_PATH / 'sterilized_batches'\nprev_state_file = prev_sterilized / 'sterilize_state.json' if prev_sterilized.exists() else None\n\n_skip_sterilization = False\nif prev_state_file and prev_state_file.exists():\n    with open(prev_state_file) as f:\n        prev_state = json.load(f)\n    if prev_state.get('completed', False):\n        print(\"ðŸŸ¢ [Sterilize] Previous run completed sterilization. Copying batches...\")\n        shutil.copytree(prev_sterilized, STERILIZED_DIR, dirs_exist_ok=True)\n        _skip_sterilization = True\n        print(\"ðŸŸ¢ [Sterilize] Sterilized batches copied from previous run âœ“\")\n\nif STATE_FILE.exists() and not _skip_sterilization:\n    with open(STATE_FILE) as f:\n        _st = json.load(f)\n    if _st.get('completed', False):\n        print(\"ðŸŸ¢ [Sterilize] Already completed in this working dir. Skipping.\")\n        _skip_sterilization = True\n\nif not _skip_sterilization:\n    def discover_audio_files() -> List[Tuple[Path, str]]:\n        \"\"\"Collect all vocal files from all datasets.\"\"\"\n        \n        files = []\n        # LJSpeech\n        for f in sorted(PATHS['ljspeech'].rglob('*.wav')):\n            files.append((f, 'ljspeech'))\n        # VCTK â€” subfolder per speaker\n        for f in sorted(PATHS['vctk'].rglob('*.wav')):\n            files.append((f, f'vctk_{f.parent.name}'))\n        # Language Identifier â€” english clips\n        for ext in ('*.wav', '*.mp3', '*.ogg'):\n            for f in sorted(PATHS['langid_en'].rglob(ext)):\n                files.append((f, 'langid_en'))\n        return files\n\n    def sterilize_and_segment(filepath: Path, tag: str) -> List[dict]:\n        \"\"\"Load â†’ noise-reduce â†’ trim â†’ LUFS normalize â†’ segment into 5s windows.\"\"\"\n        try:\n            audio, _ = librosa.load(str(filepath), sr=SR, mono=True)\n        except Exception:\n            return []\n\n        if len(audio) < SR * 1.0:\n            return []\n\n        # Spectral noise reduction â€” strip residual room tone / hiss\n        audio = nr.reduce_noise(y=audio, sr=SR, stationary=True, prop_decrease=0.85)\n\n        # Trim absolute silence\n        audio, _ = librosa.effects.trim(audio, top_db=40)\n        if len(audio) < SR * 1.5:\n            return []\n\n        # Normalize loudness to -23 LUFS\n        try:\n            loudness = meter.integrated_loudness(audio)\n            if loudness > -70:\n                audio = pyln.normalize.loudness(audio, loudness, -23.0)\n        except Exception:\n            pass\n\n        # Segment into rigid CLIP_SAMPLES (5.0s) chunks\n        segments = []\n        for start in range(0, len(audio) - SR, CLIP_SAMPLES):\n            chunk = audio[start : start + CLIP_SAMPLES]\n            if len(chunk) < CLIP_SAMPLES:\n                chunk = np.pad(chunk, (0, CLIP_SAMPLES - len(chunk)))\n\n            rms = np.sqrt(np.mean(chunk ** 2))\n            if rms < 1e-4:\n                continue\n\n            segments.append({\n                'audio':   chunk.astype(np.float32),\n                'file':    filepath.name,\n                'dataset': tag,\n            })\n        return segments\n\n    # â”€â”€ Discover & deterministic shuffle â”€â”€\n    print(\"ðŸŸ¢ [Sterilize] Discovering audio files...\")\n    all_audio_files = discover_audio_files()\n\n    # Sort alphabetically â†’ shuffle with fixed seed for reproducibility\n    all_audio_files.sort(key=lambda x: str(x[0]))\n    random.Random(42).shuffle(all_audio_files)\n    print(f\"ðŸŸ¢ [Sterilize] Found {len(all_audio_files)} source files\")\n\n    # â”€â”€ Resume from sterilize checkpoint â”€â”€\n    cursor_start = 0\n    if STATE_FILE.exists():\n        with open(STATE_FILE) as f:\n            cursor_start = json.load(f).get('cursor', 0)\n\n    STERILIZE_CHUNK = 500\n    vocal_segments: List[dict] = []\n\n    print(f\"ðŸŸ¢ [Sterilize] Processing from file index {cursor_start}...\")\n    for i in range(cursor_start, len(all_audio_files)):\n        fpath, tag = all_audio_files[i]\n\n        # Budget check\n        if get_output_size_gb() > MAX_OUTPUT_GB:\n            print(f\"\\nâš  Output size limit reached. Saving sterilization progress.\")\n            break\n\n        if i % 200 == 0 and i > cursor_start:\n            print(f\"ðŸŸ¢ Processed {i}/{len(all_audio_files)} files â†’ \"\n                  f\"{len(vocal_segments)} segments in RAM buffer\")\n\n        segs = sterilize_and_segment(fpath, tag)\n        vocal_segments.extend(segs)\n\n        # ðŸ’¾ Granular disk flushing every STERILIZE_CHUNK files\n        if (i + 1) % STERILIZE_CHUNK == 0 or (i + 1) == len(all_audio_files):\n            batch_index = (i + 1) // STERILIZE_CHUNK\n            batch_path = STERILIZED_DIR / f\"sterilized_batch_{batch_index:04d}.pkl\"\n\n            with open(batch_path, 'wb') as f:\n                pickle.dump(vocal_segments, f)\n\n            completed = (i + 1) >= len(all_audio_files)\n            with open(STATE_FILE, 'w') as f:\n                json.dump({'cursor': i + 1, 'completed': completed}, f)\n\n            print(f\"ðŸ’¾ Saved {len(vocal_segments)} segments to {batch_path.name}. RAM cleared.\")\n            vocal_segments.clear()\n\n# â”€â”€ Aggregate: read all .pkl batches to compile the segment count â”€â”€\nprint(\"\\nðŸŸ¢ [Sterilize] Compiling dataset breakdown from disk...\")\nds_counts = defaultdict(int)\ntotal_segments = 0\n\nfor batch_file in sorted(STERILIZED_DIR.glob('*.pkl')):\n    with open(batch_file, 'rb') as f:\n        batch_data = pickle.load(f)\n        total_segments += len(batch_data)\n        for s in batch_data:\n            ds_counts[s['dataset'].split('_')[0]] += 1\n\nprint(f\"ðŸŸ¢ [Sterilize] Total sterile segments: {total_segments}\")\nfor ds, cnt in sorted(ds_counts.items()):\n    print(f\"  {ds}: {cnt}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T05:43:08.804038Z","iopub.execute_input":"2026-02-24T05:43:08.804356Z"}},"outputs":[{"name":"stdout","text":"ðŸŸ¢ [Sterilize] Discovering audio files...\nðŸŸ¢ [Sterilize] Found 133835 source files\nðŸŸ¢ [Sterilize] Processing from file index 0...\nðŸŸ¢ Processed 200/133835 files â†’ 230 segments in RAM buffer\nðŸŸ¢ Processed 400/133835 files â†’ 462 segments in RAM buffer\nðŸ’¾ Saved 579 segments to sterilized_batch_0001.pkl. RAM cleared.\nðŸŸ¢ Processed 600/133835 files â†’ 111 segments in RAM buffer\nðŸŸ¢ Processed 800/133835 files â†’ 340 segments in RAM buffer\nðŸ’¾ Saved 561 segments to sterilized_batch_0002.pkl. RAM cleared.\nðŸŸ¢ Processed 1000/133835 files â†’ 0 segments in RAM buffer\nðŸŸ¢ Processed 1200/133835 files â†’ 219 segments in RAM buffer\nðŸŸ¢ Processed 1400/133835 files â†’ 431 segments in RAM buffer\nðŸ’¾ Saved 541 segments to sterilized_batch_0003.pkl. RAM cleared.\nðŸŸ¢ Processed 1600/133835 files â†’ 108 segments in RAM buffer\nðŸŸ¢ Processed 1800/133835 files â†’ 320 segments in RAM buffer\nðŸ’¾ Saved 543 segments to sterilized_batch_0004.pkl. RAM cleared.\nðŸŸ¢ Processed 2000/133835 files â†’ 0 segments in RAM buffer\nðŸŸ¢ Processed 2200/133835 files â†’ 228 segments in RAM buffer\nðŸŸ¢ Processed 2400/133835 files â†’ 446 segments in RAM buffer\nðŸ’¾ Saved 567 segments to sterilized_batch_0005.pkl. RAM cleared.\nðŸŸ¢ Processed 2600/133835 files â†’ 116 segments in RAM buffer\nðŸŸ¢ Processed 2800/133835 files â†’ 332 segments in RAM buffer\nðŸ’¾ Saved 567 segments to sterilized_batch_0006.pkl. RAM cleared.\nðŸŸ¢ Processed 3000/133835 files â†’ 0 segments in RAM buffer\nðŸŸ¢ Processed 3200/133835 files â†’ 211 segments in RAM buffer\nðŸŸ¢ Processed 3400/133835 files â†’ 439 segments in RAM buffer\nðŸ’¾ Saved 546 segments to sterilized_batch_0007.pkl. RAM cleared.\nðŸŸ¢ Processed 3600/133835 files â†’ 114 segments in RAM buffer\n","output_type":"stream"}],"execution_count":null},{"id":"51828f09","cell_type":"markdown","source":"---\n> âš ï¸ **Phase Skip**: Phases 2â€“4 above can be skipped entirely if a previous run\n> completed them successfully. The notebook detects existing `ir_catalogue.json`,\n> `clap_cache.npz`, and `sterilize_state.json` to bypass redundant computation.\n> Previous run files are located at `/kaggle/input/notebooks/itorousa/genesis-data-run#`.\n---\n","metadata":{}},{"id":"8148d746","cell_type":"markdown","source":"## ðŸ”— Checkpoint Chaining (20 GB Limit)\n\nIf the output hit the size limit before processing all vocals:\n\n1. **Save this notebook's output** as a Kaggle dataset (e.g. `genesis-data-run1`)\n2. **Create a new notebook** (or re-run this one) and attach:\n   - All the same input datasets (IRs, LJSpeech, VCTK, Language Identifier)\n   - The previous output as input (update `PREV_RUN_PATH` in Cell 3)\n3. **Run all cells** â€” the checkpoint system automatically skips completed work\n\nEach run produces ~19 GB of training triples. Chain as many times as needed.\n\n### Using the Data in Training\n\n```python\n# In the training notebook, load all batches from all runs:\nimport numpy as np\nfrom pathlib import Path\n\nrun_dirs = [\n    Path('/kaggle/input/genesis-data-run1/batches'),\n    Path('/kaggle/input/genesis-data-run2/batches'),\n    # ... add more runs\n]\n\nfor run_dir in run_dirs:\n    for batch_file in sorted(run_dir.glob('batch_*.npz')):\n        data = np.load(batch_file)\n        source_audio = data['source_audio']   # (N, 240000) int16\n        target_audio = data['target_audio']   # (N, 240000) int16\n        target_clap  = data['target_clap']    # (N, CLAP_DIM) float32\n        # Convert int16 back to float32: audio = source_audio.astype(np.float32) / 32767\n        # Compute STFT on-the-fly during training for memory efficiency\n```\n","metadata":{}}]}