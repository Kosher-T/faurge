# 02_dataset_generation_pipeline: Synthetic Degradation & Labeling

## Overview
This document defines the data extraction and synthetic degradation pipeline required to train Fabian's Physical Regression Head. Because Fabian operates as a state-aware controller, he must learn to map a 1024-dimensional fused state tensor to absolute physical targets, while simultaneously calculating the input's purity to flag downstream agents.

This pipeline will run in the cloud (Kaggle) to generate the static dataset.

## Part 1: The Degradation Suite
To simulate real-world hardware and acoustic failures, pristine audio is passed through a randomized combination of the following DSP degradation functions:

1.  **Periodic Clocking Pops:** Injects a hard DC-offset click or "boop" at exactly 1-second intervals to simulate USB buffer underruns or unshielded cable interference.
2.  **Broadband Noise:** Injects randomized white, pink, or brown noise to simulate high noise floors and cheap preamps.
3.  **Muddy EQ (Masking):** Applies aggressive low-mid frequency boosts (200Hz - 500Hz) while rolling off the high-end to simulate proximity effect and cheap dynamic microphones.
4.  **Digital Clipping:** Applies hard clipping / saturation to simulate input gain overdrive.
5.  **Comb Filtering (Room Acoustics):** Applies short, static micro-delays (1ms - 10ms) to simulate phase cancellation from untreated walls.
6.  **Harsh Sibilance:** Applies high-Q resonant EQ boosts in the 5kHz - 8kHz range to mimic cheap condenser diaphragms.

## Part 2: The Extraction Pipeline (Step-by-Step)

For every entry in the LAION / AudioCaps datasets, the pipeline executes the following loop:

### Step A: The Pristine Anchor
1.  **Ingest:** Load the pristine audio file and its semantic text description.
2.  **Extract Targets:** Measure the absolute physical metrics of the pristine audio. These are the mathematical goals Ursula will eventually chase.
    * $T_{LTAS}$ (Pristine Frequency Curve - 64D array)
    * $T_{LUFS}$ (Pristine Loudness - Scalar)
    * $T_{Dyn}$ (Pristine Crest Factor/ZCR - 2D array)
3.  **Encode Text:** Pass the text description through the frozen CLAP text encoder to generate $e_{text}$ (512D vector).

### Step B: The Degradation & Current State
1.  **Destroy:** Run a copy of the pristine audio through a randomized subset of the Degradation Suite (e.g., add Muddy EQ + Periodic Pops).
2.  **Extract Purity:** Measure the $T_{Purity}$ (SNR / THD) of this *degraded* audio file. This teaches Fabian's routing head how to recognize a broken signal.
3.  **Encode Audio:** Pass the degraded audio through the frozen CLAP audio encoder to generate $e_{audio\_degraded}$ (512D vector).

### Step C: State Fusion
1.  **Concatenate:** Mathematically combine the semantic intent with the degraded reality: 
    `Input_Tensor = numpy.concatenate([e_text, e_audio_degraded])` $\rightarrow$ 1024D array.

## Part 3: The Input-Output (I/O) Mapping

The final serialized dataset (saved as Parquet or JSONL) will contain exactly two data blocks per row, perfectly formatted for supervised neural network training:

**Model Input (X):**
* `fused_state`: The 1024D tensor representing `[Desired_Text_Latent, Current_Degraded_Audio_Latent]`.

**Model Targets / Ground Truth (Y):**
* `target_ltas`: The 64D array of the *pristine* audio.
* `target_lufs`: The scalar loudness of the *pristine* audio.
* `target_dyn`: The 2D dynamics array of the *pristine* audio.
* `input_purity`: The scalar Purity metric of the *degraded* audio.

By training on this mapping, Fabian learns to evaluate a broken audio signal, compare it against the user's semantic intent, predict the exact physical parameters required to fix it, and accurately score its level of distortion.