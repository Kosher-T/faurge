{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14927063,"datasetId":9551535,"databundleVersionId":15794187},{"sourceType":"datasetVersion","sourceId":101413,"datasetId":53291,"databundleVersionId":103953},{"sourceType":"datasetVersion","sourceId":4588404,"datasetId":2675000,"databundleVersionId":4649793},{"sourceType":"kernelVersion","sourceId":299970533}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9595e3ad","cell_type":"code","source":"!pip install -q noisereduce pyloudnorm soundfile librosa transformers torch\n\nimport os, json, hashlib, time, random, warnings, shutil, pickle\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple\nfrom collections import defaultdict\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom scipy.signal import fftconvolve\nimport noisereduce as nr\nimport torch\nfrom transformers import ClapModel, ClapProcessor\nimport pyloudnorm as pyln\n\nwarnings.filterwarnings('ignore')\nrandom.seed(42)\nnp.random.seed(42)\n\nprint(\"üü¢ [Setup] Libraries imported and deterministic seeds set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:10:31.008727Z","iopub.execute_input":"2026-02-26T05:10:31.009084Z","iopub.status.idle":"2026-02-26T05:10:34.244667Z","shell.execute_reply.started":"2026-02-26T05:10:31.009060Z","shell.execute_reply":"2026-02-26T05:10:34.243551Z"}},"outputs":[{"name":"stdout","text":"üü¢ [Setup] Libraries imported and deterministic seeds set.\n","output_type":"stream"}],"execution_count":11},{"id":"f06df653-775d-405b-82e9-996732efb8af","cell_type":"markdown","source":"### Wipe Kaggle's Output Storage","metadata":{}},{"id":"2184223c-ebcf-4d96-a569-e0a08aa1ea13","cell_type":"code","source":"import os\nimport shutil\n\nworking_dir = '/kaggle/working'\n\nprint(\"üü° Wiping Kaggle working directory...\")\n\nfor item in os.listdir(working_dir):\n    item_path = os.path.join(working_dir, item)\n    try:\n        if os.path.isfile(item_path) or os.path.islink(item_path):\n            os.unlink(item_path)\n        elif os.path.isdir(item_path):\n            shutil.rmtree(item_path)\n    except Exception as e:\n        print(f\"üî¥ Failed to delete {item_path}. Reason: {e}\")\n\nprint(\"üü¢ Kaggle working directory is completely clean.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:11:03.832419Z","iopub.execute_input":"2026-02-26T05:11:03.832710Z","iopub.status.idle":"2026-02-26T05:11:03.840665Z","shell.execute_reply.started":"2026-02-26T05:11:03.832688Z","shell.execute_reply":"2026-02-26T05:11:03.839996Z"}},"outputs":[{"name":"stdout","text":"üü° Wiping Kaggle working directory...\nüü¢ Kaggle working directory is completely clean.\n","output_type":"stream"}],"execution_count":12},{"id":"3a4d9a93-0729-4381-92e6-21f1101d510b","cell_type":"markdown","source":"### Remove a Specific Folder/File","metadata":{}},{"id":"79622b28-b88f-47e0-a74a-d5e635002071","cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\ndef delete_target(target_path):\n    target = Path(target_path)\n    \n    if not target.exists():\n        print(f\"üü° Target does not exist: {target}\")\n        return\n        \n    try:\n        if target.is_file() or target.is_symlink():\n            target.unlink()\n            print(f\"üü¢ Successfully deleted file: {target}\")\n        elif target.is_dir():\n            shutil.rmtree(target)\n            print(f\"üü¢ Successfully deleted directory: {target}\")\n    except Exception as e:\n        print(f\"üî¥ Failed to delete {target}. Reason: {e}\")\n\n# Just drop your path here\ndelete_target('/kaggle/working/sterilized_batches')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:12:34.036297Z","iopub.execute_input":"2026-02-26T05:12:34.036569Z","iopub.status.idle":"2026-02-26T05:12:34.425762Z","shell.execute_reply.started":"2026-02-26T05:12:34.036550Z","shell.execute_reply":"2026-02-26T05:12:34.424714Z"}},"outputs":[{"name":"stdout","text":"üü¢ Successfully deleted directory: /kaggle/working/sterilized_batches\n","output_type":"stream"}],"execution_count":19},{"id":"34b748e3","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 0 ‚Äî Environment & Path Configuration\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n# KAGGLE INPUT PATHS\nPATHS = {\n    'bad_irs':   Path('/kaggle/input/datasets/itorousa/impulse-responses'),\n    'mit_irs':   Path('/kaggle/input/datasets/kynthesis/mit-reverb-dataset/MIT_Reverb_Dataset/MIT_Reverb_Dataset'),\n    'ljspeech':  Path('/kaggle/input/datasets/dromosys/ljspeech/'),\n    'vctk':      Path('/kaggle/input/datasets/kynthesis/vctk-corpus/VCTK-Corpus/wav48'),\n#   'langid_en': Path('/kaggle/input/datasets/shrivatssudhir/language-identifier/english/clips')\n}\n\n# ‚ö†Ô∏è UPDATE THIS PATH WHEN CHAINING RUNS ‚ö†Ô∏è\nPREV_RUN_PATH = Path('/kaggle/input/notebooks/itorousa/genesis-data-run1')\n\n# OUTPUT PATHS\nOUTPUT          = Path('/kaggle/working')\nBATCH_DIR       = OUTPUT / 'batches'\nCLAP_DIR        = OUTPUT / 'clap_model'\nSTERILIZED_DIR  = OUTPUT / 'sterilized_batches'\nMIT_IR_DIR      = OUTPUT / 'irs' / 'mit_irs'\n\nfor d in [BATCH_DIR, CLAP_DIR, STERILIZED_DIR, MIT_IR_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# AUDIO PARAMETERS & BUDGET\nSR            = 48_000\nCLIP_SEC      = 5.0\nCLIP_SAMPLES  = int(SR * CLIP_SEC)\nTRIPLES_PER_BATCH  = 500\nMAX_OUTPUT_GB      = 19.0 \n\nprint(f\"üü¢ [Config] Output budget: {MAX_OUTPUT_GB} GB | SR: {SR} | Batch Size: {TRIPLES_PER_BATCH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:11:19.202625Z","iopub.execute_input":"2026-02-26T05:11:19.202926Z","iopub.status.idle":"2026-02-26T05:11:19.211348Z","shell.execute_reply.started":"2026-02-26T05:11:19.202903Z","shell.execute_reply":"2026-02-26T05:11:19.210060Z"}},"outputs":[{"name":"stdout","text":"üü¢ [Config] Output budget: 19.0 GB | SR: 48000 | Batch Size: 500\n","output_type":"stream"}],"execution_count":14},{"id":"ab6336cd","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 1 ‚Äî Aggressive Checkpoint Initialization\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nCHECKPOINT_PATH = OUTPUT / 'checkpoint.json'\n\ndef get_output_size_gb() -> float:\n    '''Calculate the exact size of /kaggle/working in GB.'''\n    total = sum(f.stat().st_size for f in OUTPUT.rglob('*') if f.is_file())\n    return total / (1024 ** 3)\n\ndef load_checkpoint() -> dict:\n    # 1) Check previous run first (chaining)\n    prev_ckpt = PREV_RUN_PATH / 'checkpoint.json'\n    if prev_ckpt.exists():\n        with open(prev_ckpt) as f:\n            ckpt = json.load(f)\n        ckpt['run_number'] += 1\n        print(f\"üü¢ [Checkpoint] ‚ôª Resuming from previous run: {ckpt['triples_completed']} triples done\")\n        return ckpt\n\n    # 2) Check current working dir (kernel restart mid-session)\n    if CHECKPOINT_PATH.exists():\n        with open(CHECKPOINT_PATH) as f:\n            return json.load(f)\n\n    # 3) Fresh start\n    return {\n        'batch_id': 0,\n        'triples_completed': 0,\n        'vocal_cursor': 0,\n        'run_number': 1,\n    }\n\ndef save_checkpoint(ckpt: dict):\n    with open(CHECKPOINT_PATH, 'w') as f:\n        json.dump(ckpt, f, indent=2)\n\nckpt = load_checkpoint()\nsave_checkpoint(ckpt) # Initialize immediately\n\nprint(f\"üü¢ [Checkpoint] Run #{ckpt['run_number']} | Starting at vocal cursor {ckpt['vocal_cursor']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:11:23.472404Z","iopub.execute_input":"2026-02-26T05:11:23.472677Z","iopub.status.idle":"2026-02-26T05:11:23.489073Z","shell.execute_reply.started":"2026-02-26T05:11:23.472660Z","shell.execute_reply":"2026-02-26T05:11:23.487706Z"}},"outputs":[{"name":"stdout","text":"üü¢ [Checkpoint] ‚ôª Resuming from previous run: 0 triples done\nüü¢ [Checkpoint] Run #2 | Starting at vocal cursor 0\n","output_type":"stream"}],"execution_count":15},{"id":"85eb9eb0","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 2 ‚Äî Impulse Response Acquisition & Pooling\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nCATALOGUE_PATH = OUTPUT / 'ir_catalogue.json'\nprev_catalogue = PREV_RUN_PATH / 'ir_catalogue.json'\n\nbad_pool: List[str] = []\ntarget_pool: List[str] = []\nir_catalogue: Dict = {}\n\n# ‚îÄ‚îÄ‚îÄ Fast path: Copy from previous run if exists ‚îÄ‚îÄ‚îÄ\nif prev_catalogue.exists() and not CATALOGUE_PATH.exists():\n    shutil.copy2(prev_catalogue, CATALOGUE_PATH)\n    print(\"üü¢ [IR Phase] Copied catalogue from previous run.\")\n\nif CATALOGUE_PATH.exists():\n    print(\"üü¢ [IR Phase] Found existing catalogue. Bypassing extraction.\")\n    with open(CATALOGUE_PATH, 'r') as f:\n        ir_catalogue = json.load(f)\n        \n    for ir_id, feats in ir_catalogue.items():\n        if feats['source'] == 'mit':\n            target_pool.append(ir_id)\n        else:\n            bad_pool.append(ir_id)\n            \n    print(f\"üü¢ [IR Phase] Re-hydrated pools -> Target (MIT): {len(target_pool)} | Bad (Custom): {len(bad_pool)}\")\n\nelse:\n    print(\"üü¢ [IR Phase] Processing IRs from scratch...\")\n    \n    # Copy MIT IRs to working directory for easier access later\n    if PATHS['mit_irs'].exists():\n        shutil.copytree(PATHS['mit_irs'], MIT_IR_DIR, dirs_exist_ok=True)\n        \n    def process_ir(filepath: Path, source_tag: str):\n        try:\n            audio, _ = librosa.load(str(filepath), sr=SR, mono=True)\n            if len(audio) < 64: return\n            peak = np.max(np.abs(audio))\n            if peak > 1e-6: audio = audio / peak\n            \n            ir_id = f\"{source_tag}_{filepath.stem}\"\n            ir_catalogue[ir_id] = {'source': source_tag, 'path': str(filepath)}\n            \n            if source_tag == 'mit': target_pool.append(ir_id)\n            else: bad_pool.append(ir_id)\n        except Exception:\n            pass\n\n    # Process MIT (Target)\n    for f in MIT_IR_DIR.rglob('*.wav'):\n        process_ir(f, 'mit')\n        \n    # Process Custom (Bad)\n    for ext in ('*.irs', '*.wav'):\n        for f in PATHS['bad_irs'].rglob(ext):\n            process_ir(f, 'bad')\n\n    with open(CATALOGUE_PATH, 'w') as f:\n        json.dump(ir_catalogue, f, indent=2)\n        \n    print(f\"üü¢ [IR Phase] Catalogue built -> Target: {len(target_pool)} | Bad: {len(bad_pool)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:11:28.262427Z","iopub.execute_input":"2026-02-26T05:11:28.262733Z","iopub.status.idle":"2026-02-26T05:11:28.277021Z","shell.execute_reply.started":"2026-02-26T05:11:28.262712Z","shell.execute_reply":"2026-02-26T05:11:28.276031Z"}},"outputs":[{"name":"stdout","text":"üü¢ [IR Phase] Copied catalogue from previous run.\nüü¢ [IR Phase] Found existing catalogue. Bypassing extraction.\nüü¢ [IR Phase] Re-hydrated pools -> Target (MIT): 270 | Bad (Custom): 389\n","output_type":"stream"}],"execution_count":16},{"id":"8fc6437b","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 3 ‚Äî CLAP Target Embedding Cache\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nCLAP_CACHE_PATH = OUTPUT / 'clap_cache.npz'\nprev_cache = PREV_RUN_PATH / 'clap_cache.npz'\nclap_cache_data = {}\n\n# ‚îÄ‚îÄ‚îÄ Fast path: Load from cache ‚îÄ‚îÄ‚îÄ\nif prev_cache.exists() and not CLAP_CACHE_PATH.exists():\n    shutil.copy2(prev_cache, CLAP_CACHE_PATH)\n    print(\"üü¢ [CLAP Phase] Copied embedding cache from previous run.\")\n\nif CLAP_CACHE_PATH.exists():\n    print(\"üü¢ [CLAP Phase] Found existing clap_cache.npz. Bypassing model loading.\")\n    clap_cache_data = dict(np.load(CLAP_CACHE_PATH))\n    print(f\"üü¢ [CLAP Phase] Loaded {len(clap_cache_data)} embeddings.\")\nelse:\n    print(\"üü¢ [CLAP Phase] Loading model to compute embeddings...\")\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    CLAP_MODEL_ID = \"laion/larger_clap_music_and_speech\"\n    \n    # Handle frozen model transfer to avoid re-downloading\n    prev_clap_model = PREV_RUN_PATH / 'clap_model'\n    if prev_clap_model.exists() and not CLAP_DIR.exists():\n        shutil.copytree(prev_clap_model, CLAP_DIR, dirs_exist_ok=True)\n        \n    if (CLAP_DIR / 'config.json').exists():\n        clap_processor = ClapProcessor.from_pretrained(CLAP_DIR)\n        clap_model = ClapModel.from_pretrained(CLAP_DIR).to(device).eval()\n    else:\n        clap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)\n        clap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(device).eval()\n        clap_model.save_pretrained(CLAP_DIR)\n        clap_processor.save_pretrained(CLAP_DIR)\n\n    ref_noise = np.random.randn(SR * 3).astype(np.float32) * 0.1\n    \n    for i, ir_id in enumerate(target_pool):\n        if ir_id not in ir_catalogue: continue\n        \n        ir_audio, _ = librosa.load(ir_catalogue[ir_id]['path'], sr=SR, mono=True)\n        scene = fftconvolve(ref_noise, ir_audio, mode='full')[:SR * 3]\n        scene = scene / (np.max(np.abs(scene)) + 1e-8)\n        \n        inputs = clap_processor(audio=scene, sampling_rate=SR, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = clap_model.get_audio_features(**inputs)\n            # Critical: Must extract pooler_output\n            emb = outputs.pooler_output.cpu().numpy().flatten().astype(np.float32)\n            \n        clap_cache_data[ir_id] = emb\n        \n    np.savez(CLAP_CACHE_PATH, **clap_cache_data)\n    print(f\"üü¢ [CLAP Phase] Computed and cached {len(clap_cache_data)} embeddings.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:11:42.597705Z","iopub.execute_input":"2026-02-26T05:11:42.598012Z","iopub.status.idle":"2026-02-26T05:11:42.662245Z","shell.execute_reply.started":"2026-02-26T05:11:42.597988Z","shell.execute_reply":"2026-02-26T05:11:42.661160Z"}},"outputs":[{"name":"stdout","text":"üü¢ [CLAP Phase] Copied embedding cache from previous run.\nüü¢ [CLAP Phase] Found existing clap_cache.npz. Bypassing model loading.\nüü¢ [CLAP Phase] Loaded 270 embeddings.\n","output_type":"stream"}],"execution_count":17},{"id":"1542e8be","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 4 ‚Äî Vocal Sterilization & \"Dead\" Audio Guarantee\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSTATE_FILE = STERILIZED_DIR / 'sterilize_state.json'\nprev_sterilized = PREV_RUN_PATH / 'sterilized_batches'\n\n# ‚îÄ‚îÄ‚îÄ Fast path: Transfer previous chunks ‚îÄ‚îÄ‚îÄ\nif prev_sterilized.exists() and (prev_sterilized / 'sterilize_state.json').exists():\n    if not STATE_FILE.exists():\n        shutil.copytree(prev_sterilized, STERILIZED_DIR, dirs_exist_ok=True)\n        print(\"üü¢ [Sterilize Phase] Copied sterilized batches from previous run.\")\n\n_st = {}\nif STATE_FILE.exists():\n    with open(STATE_FILE) as f:\n        _st = json.load(f)\n\nif _st.get('completed', False):\n    print(\"üü¢ [Sterilize Phase] Sterilization previously completed. Bypassing extraction.\")\nelse:\n    print(\"üü¢ [Sterilize Phase] Discovering source audio...\")\n    meter = pyln.Meter(SR)\n    all_files = []\n    \n    for p in PATHS.values():\n        if p.name in ['impulse-responses', 'MIT_Reverb_Dataset']: continue\n        for ext in ('*.wav', '*.mp3', '*.ogg'):\n            all_files.extend([(f, p.name) for f in p.rglob(ext)])\n            \n    all_files.sort(key=lambda x: str(x[0]))\n    random.Random(42).shuffle(all_files)\n    print(f\"üü¢ [Sterilize Phase] Found {len(all_files)} raw vocal files.\")\n    \n    cursor = _st.get('cursor', 0)\n    print(f\"üü¢ [Sterilize Phase] Resuming extraction at file index: {cursor}\")\n    \n    vocal_segments = []\n    STERILIZE_CHUNK = 500\n    \n    for i in range(cursor, len(all_files)):\n        if get_output_size_gb() > MAX_OUTPUT_GB:\n            print(f\"‚ö†Ô∏è Output limit reached. Pausing sterilization.\")\n            break\n            \n        fpath, tag = all_files[i]\n        try:\n            audio, _ = librosa.load(str(fpath), sr=SR, mono=True)\n            if len(audio) < SR * 1.5: continue\n            \n            audio = nr.reduce_noise(y=audio, sr=SR, stationary=True, prop_decrease=0.85)\n            audio, _ = librosa.effects.trim(audio, top_db=40)\n            if len(audio) < SR * 1.5: continue\n            \n            loudness = meter.integrated_loudness(audio)\n            if loudness > -70: audio = pyln.normalize.loudness(audio, loudness, -23.0)\n            \n            for start in range(0, len(audio) - SR, CLIP_SAMPLES):\n                chunk = audio[start : start + CLIP_SAMPLES]\n                if len(chunk) < CLIP_SAMPLES:\n                    chunk = np.pad(chunk, (0, CLIP_SAMPLES - len(chunk)))\n                \n                if np.sqrt(np.mean(chunk ** 2)) < 1e-4: continue\n                vocal_segments.append({'audio': chunk.astype(np.float32), 'file': fpath.name, 'dataset': tag})\n                \n        except Exception: pass\n        \n        # Flush to disk to protect RAM\n        if (i + 1) % STERILIZE_CHUNK == 0 or (i + 1) == len(all_files):\n            batch_idx = (i + 1) // STERILIZE_CHUNK\n            batch_path = STERILIZED_DIR / f\"sterilized_batch_{batch_idx:04d}.pkl\"\n            with open(batch_path, 'wb') as f:\n                pickle.dump(vocal_segments, f)\n                \n            completed = (i + 1) >= len(all_files)\n            with open(STATE_FILE, 'w') as f:\n                json.dump({'cursor': i + 1, 'completed': completed}, f)\n                \n            print(f\"  üíæ Saved {len(vocal_segments)} segments to {batch_path.name}. RAM cleared.\")\n            vocal_segments.clear()\n\n# Verification check\ntotal_segs = sum(len(pickle.load(open(f, 'rb'))) for f in STERILIZED_DIR.glob('*.pkl'))\nprint(f\"üü¢ [Sterilize Phase] Total sterilized 5.0s segments on disk: {total_segs}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:11:51.018408Z","iopub.execute_input":"2026-02-26T05:11:51.018692Z","iopub.status.idle":"2026-02-26T05:12:10.207689Z","shell.execute_reply.started":"2026-02-26T05:11:51.018671Z","shell.execute_reply":"2026-02-26T05:12:10.205615Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2762604801.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprev_sterilized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev_sterilized\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'sterilize_state.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSTATE_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_sterilized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTERILIZED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üü¢ [Sterilize Phase] Copied sterilized batches from previous run.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    601\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;31m# Will raise a SpecialFileError for unsupported file types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                 \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;31m# catch the Error from the recursive copytree so that we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# continue with other files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"id":"51828f09","cell_type":"markdown","source":"---\n> ‚ö†Ô∏è **Phase Skip**: Phases 2‚Äì4 above can be skipped entirely if a previous run\n> completed them successfully. The notebook detects existing `ir_catalogue.json`,\n> `clap_cache.npz`, and `sterilize_state.json` to bypass redundant computation.\n> Previous run files are located at `/kaggle/input/notebooks/itorousa/genesis-data-run#`.\n---\n","metadata":{}},{"id":"ffa2c518","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 5 ‚Äî The Messy DSP Toolkit\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# Random, destructive audio effects applied ONLY to the source input\n# to simulate terrible recording environments.\n\ndef add_noise(audio: np.ndarray, noise_type: str, snr_db: float) -> np.ndarray:\n    n = len(audio)\n    if noise_type == 'white': noise = np.random.randn(n)\n    elif noise_type == 'pink':\n        freqs = np.fft.rfftfreq(n, 1 / SR); freqs[0] = 1\n        noise = np.fft.irfft(1.0 / np.sqrt(freqs) * np.exp(2j * np.pi * np.random.rand(len(freqs))))[:n]\n    elif noise_type == 'brown':\n        noise = np.cumsum(np.random.randn(n)); noise -= np.mean(noise)\n    elif noise_type == 'hvac':\n        from scipy.signal import butter, sosfilt\n        sos = butter(4, [100, 1000], btype='band', fs=SR, output='sos')\n        noise = sosfilt(sos, np.random.randn(n))\n    else: noise = np.random.randn(n)\n\n    sig_power = np.mean(audio ** 2) + 1e-12\n    noise_power = np.mean(noise ** 2) + 1e-12\n    noise = noise * np.sqrt((sig_power / (10 ** (snr_db / 10))) / noise_power)\n    return audio + noise.astype(np.float32)\n\ndef apply_eq(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    for lo, hi in [(80, 300), (300, 3000), (3000, 12000)]:\n        gain_db = random.uniform(-6, 6)\n        if abs(gain_db) >= 1:\n            try:\n                sos = butter(2, [lo, hi], btype='band', fs=SR, output='sos')\n                audio += sosfilt(sos, audio) * ((10 ** (gain_db / 20)) - 1)\n            except Exception: pass\n    return audio.astype(np.float32)\n\ndef apply_highpass(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    sos = butter(4, random.uniform(60, 300), btype='high', fs=SR, output='sos')\n    return sosfilt(sos, audio).astype(np.float32)\n\ndef apply_lowpass(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    sos = butter(4, random.uniform(3000, 16000), btype='low', fs=SR, output='sos')\n    return sosfilt(sos, audio).astype(np.float32)\n\ndef apply_bitcrush(audio: np.ndarray) -> np.ndarray:\n    levels = 2 ** random.randint(8, 16)\n    return (np.round(audio * levels) / levels).astype(np.float32)\n\ndef apply_hard_clip(audio: np.ndarray) -> np.ndarray:\n    threshold = random.uniform(0.3, 0.9)\n    return np.clip(audio, -threshold, threshold).astype(np.float32)\n\nDEGRADATIONS = {\n    'noise':    lambda a: add_noise(a, random.choice(['white', 'pink', 'brown', 'hvac']), random.uniform(5, 40)),\n    'eq':       apply_eq,\n    'highpass': apply_highpass,\n    'lowpass':  apply_lowpass,\n    'gain':     lambda a: a * (10 ** (random.uniform(-6, 6) / 20)),\n    'bitcrush': apply_bitcrush,\n    'clip':     apply_hard_clip,\n}\n\ndef apply_random_degradations(audio: np.ndarray, min_augs=3, max_augs=6) -> Tuple[np.ndarray, List[str]]:\n    chosen = random.sample(list(DEGRADATIONS.keys()), random.randint(min_augs, max_augs))\n    for name in chosen: audio = DEGRADATIONS[name](audio)\n    return np.clip(audio, -1.0, 1.0).astype(np.float32), chosen\n\nprint(\"üü¢ [Degradation] Toolkit loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:49:00.973202Z","iopub.execute_input":"2026-02-26T05:49:00.973742Z","iopub.status.idle":"2026-02-26T05:49:00.995259Z","shell.execute_reply.started":"2026-02-26T05:49:00.973701Z","shell.execute_reply":"2026-02-26T05:49:00.993491Z"}},"outputs":[{"name":"stdout","text":"üü¢ [Degradation] Toolkit loaded.\n","output_type":"stream"}],"execution_count":26},{"id":"545106be-36dd-4f89-83bc-5cf93b98a7b3","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 6 ‚Äî The Streaming Training Data Engine\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\ndef convolve_trim(vocal: np.ndarray, ir: np.ndarray) -> np.ndarray:\n    wet = fftconvolve(vocal, ir, mode='full')[:CLIP_SAMPLES]\n    peak = np.max(np.abs(wet))\n    return (wet / peak).astype(np.float32) if peak > 1e-6 else wet.astype(np.float32)\n\ndef audio_to_int16(audio: np.ndarray) -> np.ndarray:\n    return (np.clip(audio, -1, 1) * 32767).astype(np.int16)\n\n# ‚îÄ‚îÄ Dynamic IR Audio Loading (Path-Drift Proof) ‚îÄ‚îÄ\nloaded_irs = {}\nprint(\"üü¢ [Engine] Mapping available IR audio files across all directories...\")\n\n# 1. Build a lookup table of ALL .wav and .irs files on the machine\naudio_lookup = {}\nfor search_dir in [MIT_IR_DIR, PATHS['bad_irs'], PREV_RUN_PATH / 'irs']:\n    if search_dir.exists():\n        for f in search_dir.rglob('*'):\n            if f.suffix.lower() in ['.wav', '.irs'] and f.is_file():\n                audio_lookup[f.name] = f\n                \n# 2. Load the IRs using the lookup table\nif not CATALOGUE_PATH.exists():\n    print(\"üî¥ [Engine] FATAL: ir_catalogue.json is missing!\")\nelse:\n    with open(CATALOGUE_PATH, 'r') as f: ir_cat = json.load(f)\n    for ir_id, data in ir_cat.items():\n        original_name = Path(data['path']).name\n        if original_name in audio_lookup:\n            try:\n                a, _ = librosa.load(str(audio_lookup[original_name]), sr=SR, mono=True)\n                peak = np.max(np.abs(a))\n                loaded_irs[ir_id] = (a / peak).astype(np.float32) if peak > 1e-6 else a.astype(np.float32)\n            except Exception: pass\n\nprint(f\"üü¢ [Engine] Successfully loaded {len(loaded_irs)} IRs into RAM.\")\nif len(loaded_irs) == 0:\n    print(\"üî¥ [Engine] FATAL: Could not load any IR audio! Generation loop will skip all segments.\")\n\n# ‚îÄ‚îÄ Robust PKL Discovery ‚îÄ‚îÄ\npkl_files = []\nif (PREV_RUN_PATH / 'sterilized_batches').exists():\n    pkl_files.extend(list((PREV_RUN_PATH / 'sterilized_batches').glob('*.pkl')))\npkl_files.extend(list(STERILIZED_DIR.glob('*.pkl')))\n\nunique_pkls = {p.name: p for p in pkl_files}\npkl_files = sorted(unique_pkls.values(), key=lambda x: x.name)\n\nprint(f\"üü¢ [Engine] Found {len(pkl_files)} sterilized batch files.\")\nprint(f\"üü¢ [Engine] Pool Sizes -> Target: {len(target_pool)} | Bad: {len(bad_pool)}\")\n\n# ‚îÄ‚îÄ Setup Variables ‚îÄ‚îÄ\nclap_cache_data = dict(np.load(CLAP_CACHE_PATH)) if CLAP_CACHE_PATH.exists() else {}\nbatch_id = ckpt.get('batch_id', 0)\ntotal_completed = ckpt.get('triples_completed', 0)\n\nscenarios = ['1', '1a', '1b', '1c', '2', '3', '3a', '4', '5']\nweights =   [ 0.07, 0.04, 0.03, 0.06, 0.20, 0.07, 0.13, 0.20, 0.20] \n\nbatch_sources, batch_targets, batch_claps, batch_meta = [], [], [], []\n\nprint(f\"\\nüü¢ [Engine] Starting streaming triple generation. Checkpoint: {total_completed} completed.\")\nt_start = time.time()\nlimit_reached = False\n\nfor pkl_file in pkl_files:\n    if limit_reached: break\n    print(f\"\\n  ‚Üí Processing {pkl_file.name}...\")\n    \n    with open(pkl_file, 'rb') as f: chunk_segments = pickle.load(f)\n    random.shuffle(chunk_segments)\n    \n    skipped_ir = 0\n    skipped_qa = 0\n    \n    for seg in chunk_segments:\n        if get_output_size_gb() > MAX_OUTPUT_GB:\n            print(f\"\\n‚ö†Ô∏è Output limit reached ({MAX_OUTPUT_GB} GB). Stopping generation.\")\n            limit_reached = True\n            break\n            \n        V = seg['audio']\n        scen = random.choices(scenarios, weights=weights, k=1)[0]\n        bad_ir = random.choice(bad_pool) if bad_pool else None\n        tgt_ir = random.choice(target_pool) if target_pool else None\n        tgt_ir2 = random.choice(target_pool) if target_pool else None\n        \n        # Guardrail: If IR audio is missing from RAM, skip and tally\n        if not bad_ir or not tgt_ir or bad_ir not in loaded_irs or tgt_ir not in loaded_irs: \n            skipped_ir += 1\n            continue\n\n        source_audio = V.copy()\n        target_audio = V.copy()\n        used_target_ir = tgt_ir\n        \n        # Execute the 20k Scenario Matrix Mathematics\n        if scen == '1': \n            target_audio = convolve_trim(V, loaded_irs[tgt_ir])\n            source_audio = add_noise(target_audio, 'white', random.uniform(5, 20))\n        elif scen == '1a': \n            source_audio = convolve_trim(V, loaded_irs[tgt_ir])\n            source_audio = add_noise(source_audio, 'white', random.uniform(5, 20))\n        elif scen == '1b': \n            target_audio = convolve_trim(V, loaded_irs[tgt_ir])\n            source_audio = add_noise(V, 'white', random.uniform(5, 20))\n        elif scen == '1c': \n            target_audio = convolve_trim(V, loaded_irs[tgt_ir])\n            source_audio = convolve_trim(V, loaded_irs[tgt_ir2])\n            source_audio = add_noise(source_audio, 'white', random.uniform(5, 20))\n        elif scen == '2': \n            target_audio = convolve_trim(V, loaded_irs[tgt_ir])\n            source_audio = apply_lowpass(apply_highpass(target_audio))\n        elif scen == '3': \n            source_audio = convolve_trim(V, loaded_irs[bad_ir])\n        elif scen == '3a': \n            target_audio = convolve_trim(V, loaded_irs[tgt_ir])\n            source_audio = convolve_trim(V, loaded_irs[bad_ir])\n        elif scen == '4': \n            target_audio = convolve_trim(V, loaded_irs[tgt_ir])\n        elif scen == '5': \n            source_audio, _ = apply_random_degradations(V)\n\n        # QA Checks: Skip completely silent or mathematically corrupted tensors\n        if np.sqrt(np.mean(source_audio**2)) < 1e-4 or np.sqrt(np.mean(target_audio**2)) < 1e-4: \n            skipped_qa += 1\n            continue\n        if np.any(np.isnan(source_audio)) or np.any(np.isnan(target_audio)): \n            skipped_qa += 1\n            continue\n\n        # CLAP lookup\n        tgt_clap = clap_cache_data.get(used_target_ir) if scen not in ['1a', '3', '5'] else np.zeros(512, dtype=np.float32)\n        if tgt_clap is None: tgt_clap = np.zeros(512, dtype=np.float32)\n\n        batch_sources.append(audio_to_int16(source_audio))\n        batch_targets.append(audio_to_int16(target_audio))\n        batch_claps.append(tgt_clap)\n        batch_meta.append({'scenario': scen, 'vocal': seg['file'], 'bad_ir': bad_ir, 'tgt_ir': tgt_ir, 'tgt_ir2': tgt_ir2 if scen == '1c' else None})\n\n        # Save batch when full\n        if len(batch_sources) >= TRIPLES_PER_BATCH:\n            np.savez(BATCH_DIR / f'batch_{batch_id:04d}.npz', source_audio=np.stack(batch_sources), target_audio=np.stack(batch_targets), target_clap=np.stack(batch_claps))\n            with open(BATCH_DIR / f'batch_{batch_id:04d}_meta.json', 'w') as f: json.dump(batch_meta, f)\n            \n            total_completed += len(batch_sources)\n            batch_id += 1\n            ckpt.update({'batch_id': batch_id, 'triples_completed': total_completed})\n            save_checkpoint(ckpt)\n            \n            rate = len(batch_sources) / (time.time() - t_start)\n            print(f\"    üíæ Batch {batch_id-1:04d} saved | {total_completed:,} total | {get_output_size_gb():.1f} GB\")\n            \n            batch_sources.clear(); batch_targets.clear(); batch_claps.clear(); batch_meta.clear()\n            t_start = time.time()\n            \n    # Print metrics for the chunk\n    buffered = len(batch_sources)\n    print(f\"    ‚Ü≥ Chunk Complete | Buffered: {buffered} | Skipped (Missing IR): {skipped_ir} | Skipped (QA): {skipped_qa}\")\n    del chunk_segments # Protect RAM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:49:57.215748Z","iopub.execute_input":"2026-02-26T05:49:57.216944Z","iopub.status.idle":"2026-02-26T05:57:09.808577Z","shell.execute_reply.started":"2026-02-26T05:49:57.216915Z","shell.execute_reply":"2026-02-26T05:57:09.807488Z"}},"outputs":[{"name":"stdout","text":"üü¢ [Engine] Mapping available IR audio files across all directories...\nüü¢ [Engine] Successfully loaded 659 IRs into RAM.\nüü¢ [Engine] Found 34 sterilized batch files.\nüü¢ [Engine] Pool Sizes -> Target: 270 | Bad: 389\n\nüü¢ [Engine] Starting streaming triple generation. Checkpoint: 0 completed.\n\n  ‚Üí Processing sterilized_batch_0001.pkl...\n    üíæ Batch 0000 saved | 500 total | 0.4 GB\n    ‚Ü≥ Chunk Complete | Buffered: 64 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0003.pkl...\n    üíæ Batch 0001 saved | 1,000 total | 0.9 GB\n    üíæ Batch 0002 saved | 1,500 total | 1.3 GB\n    ‚Ü≥ Chunk Complete | Buffered: 167 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0004.pkl...\n    üíæ Batch 0003 saved | 2,000 total | 1.8 GB\n    ‚Ü≥ Chunk Complete | Buffered: 202 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0005.pkl...\n    üíæ Batch 0004 saved | 2,500 total | 2.2 GB\n    ‚Ü≥ Chunk Complete | Buffered: 233 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0006.pkl...\n    üíæ Batch 0005 saved | 3,000 total | 2.7 GB\n    ‚Ü≥ Chunk Complete | Buffered: 267 | Skipped (Missing IR): 0 | Skipped (QA): 1\n\n  ‚Üí Processing sterilized_batch_0007.pkl...\n    üíæ Batch 0006 saved | 3,500 total | 3.1 GB\n    ‚Ü≥ Chunk Complete | Buffered: 317 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0008.pkl...\n    üíæ Batch 0007 saved | 4,000 total | 3.6 GB\n    ‚Ü≥ Chunk Complete | Buffered: 362 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0010.pkl...\n    üíæ Batch 0008 saved | 4,500 total | 4.0 GB\n    üíæ Batch 0009 saved | 5,000 total | 4.5 GB\n    ‚Ü≥ Chunk Complete | Buffered: 430 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0011.pkl...\n    üíæ Batch 0010 saved | 5,500 total | 4.9 GB\n    ‚Ü≥ Chunk Complete | Buffered: 462 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0012.pkl...\n    üíæ Batch 0011 saved | 6,000 total | 5.4 GB\n    üíæ Batch 0012 saved | 6,500 total | 5.8 GB\n    ‚Ü≥ Chunk Complete | Buffered: 18 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0013.pkl...\n    üíæ Batch 0013 saved | 7,000 total | 6.3 GB\n    ‚Ü≥ Chunk Complete | Buffered: 67 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0014.pkl...\n    üíæ Batch 0014 saved | 7,500 total | 6.7 GB\n    ‚Ü≥ Chunk Complete | Buffered: 109 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0015.pkl...\n    üíæ Batch 0015 saved | 8,000 total | 7.2 GB\n    ‚Ü≥ Chunk Complete | Buffered: 150 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0016.pkl...\n    üíæ Batch 0016 saved | 8,500 total | 7.6 GB\n    ‚Ü≥ Chunk Complete | Buffered: 196 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0017.pkl...\n    üíæ Batch 0017 saved | 9,000 total | 8.1 GB\n    ‚Ü≥ Chunk Complete | Buffered: 240 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0018.pkl...\n    üíæ Batch 0018 saved | 9,500 total | 8.5 GB\n    ‚Ü≥ Chunk Complete | Buffered: 257 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0019.pkl...\n    üíæ Batch 0019 saved | 10,000 total | 9.0 GB\n    ‚Ü≥ Chunk Complete | Buffered: 308 | Skipped (Missing IR): 0 | Skipped (QA): 1\n\n  ‚Üí Processing sterilized_batch_0020.pkl...\n    üíæ Batch 0020 saved | 10,500 total | 9.4 GB\n    ‚Ü≥ Chunk Complete | Buffered: 350 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0021.pkl...\n    üíæ Batch 0021 saved | 11,000 total | 9.9 GB\n    ‚Ü≥ Chunk Complete | Buffered: 385 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0022.pkl...\n    üíæ Batch 0022 saved | 11,500 total | 10.3 GB\n    ‚Ü≥ Chunk Complete | Buffered: 425 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0023.pkl...\n    üíæ Batch 0023 saved | 12,000 total | 10.8 GB\n    ‚Ü≥ Chunk Complete | Buffered: 476 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0024.pkl...\n    üíæ Batch 0024 saved | 12,500 total | 11.2 GB\n    üíæ Batch 0025 saved | 13,000 total | 11.7 GB\n    ‚Ü≥ Chunk Complete | Buffered: 27 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0025.pkl...\n    üíæ Batch 0026 saved | 13,500 total | 12.1 GB\n    ‚Ü≥ Chunk Complete | Buffered: 78 | Skipped (Missing IR): 0 | Skipped (QA): 1\n\n  ‚Üí Processing sterilized_batch_0026.pkl...\n    üíæ Batch 0027 saved | 14,000 total | 12.5 GB\n    ‚Ü≥ Chunk Complete | Buffered: 123 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0027.pkl...\n    üíæ Batch 0028 saved | 14,500 total | 13.0 GB\n    ‚Ü≥ Chunk Complete | Buffered: 148 | Skipped (Missing IR): 0 | Skipped (QA): 1\n\n  ‚Üí Processing sterilized_batch_0028.pkl...\n    üíæ Batch 0029 saved | 15,000 total | 13.4 GB\n    ‚Ü≥ Chunk Complete | Buffered: 170 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0029.pkl...\n    üíæ Batch 0030 saved | 15,500 total | 13.9 GB\n    ‚Ü≥ Chunk Complete | Buffered: 194 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0030.pkl...\n    üíæ Batch 0031 saved | 16,000 total | 14.3 GB\n    ‚Ü≥ Chunk Complete | Buffered: 243 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0031.pkl...\n    üíæ Batch 0032 saved | 16,500 total | 14.8 GB\n    ‚Ü≥ Chunk Complete | Buffered: 300 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0033.pkl...\n    üíæ Batch 0033 saved | 17,000 total | 15.2 GB\n    üíæ Batch 0034 saved | 17,500 total | 15.7 GB\n    ‚Ü≥ Chunk Complete | Buffered: 396 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0034.pkl...\n    üíæ Batch 0035 saved | 18,000 total | 16.1 GB\n    ‚Ü≥ Chunk Complete | Buffered: 454 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0035.pkl...\n    üíæ Batch 0036 saved | 18,500 total | 16.6 GB\n    üíæ Batch 0037 saved | 19,000 total | 17.0 GB\n    ‚Ü≥ Chunk Complete | Buffered: 15 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0037.pkl...\n    üíæ Batch 0038 saved | 19,500 total | 17.5 GB\n    üíæ Batch 0039 saved | 20,000 total | 17.9 GB\n    ‚Ü≥ Chunk Complete | Buffered: 97 | Skipped (Missing IR): 0 | Skipped (QA): 0\n\n  ‚Üí Processing sterilized_batch_0038.pkl...\n    üíæ Batch 0040 saved | 20,500 total | 18.4 GB\n    ‚Ü≥ Chunk Complete | Buffered: 123 | Skipped (Missing IR): 0 | Skipped (QA): 0\n","output_type":"stream"}],"execution_count":28},{"id":"7d0c46af","cell_type":"markdown","source":"---\n> ‚ö†Ô∏è **Phase Skip**: Phases 5‚Äì6 above can be skipped if a previous run already\n> generated sufficient training batches. Attach the previous output as input and\n> the checkpoint system will resume from where it left off.\n---\n","metadata":{}},{"id":"5a65a23a","cell_type":"code","source":"# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# PHASE 7 ‚Äî Manifest & Pipeline Conclusion\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n# ‚îÄ‚îÄ Flush remaining triples ‚îÄ‚îÄ\nif batch_sources:\n    np.savez(BATCH_DIR / f'batch_{batch_id:04d}.npz', source_audio=np.stack(batch_sources), target_audio=np.stack(batch_targets), target_clap=np.stack(batch_claps))\n    with open(BATCH_DIR / f'batch_{batch_id:04d}_meta.json', 'w') as f: json.dump(batch_meta, f)\n    total_completed += len(batch_sources)\n    batch_id += 1\n    ckpt.update({'batch_id': batch_id, 'triples_completed': total_completed})\n    save_checkpoint(ckpt)\n\n# ‚îÄ‚îÄ Compute Checksums ‚îÄ‚îÄ\ndef sha256_file(path: Path) -> str:\n    h = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''): h.update(chunk)\n    return h.hexdigest()\n\nprint(\"üü¢ [Manifest] Computing SHA-256 checksums...\")\nmanifest = {\n    'run_number': ckpt['run_number'],\n    'triples_total': ckpt['triples_completed'],\n    'batches': ckpt['batch_id'],\n    'sample_rate': SR, 'clip_samples': CLIP_SAMPLES,\n    'output_size_gb': round(get_output_size_gb(), 3),\n    'batch_checksums': {f.name: sha256_file(f) for f in sorted(BATCH_DIR.glob('batch_*.npz'))}\n}\n\nwith open(OUTPUT / 'manifest.json', 'w') as f: json.dump(manifest, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"  GENESIS DATA CURATION ‚Äî RUN {ckpt['run_number']} COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"  Total triples:    {manifest['triples_total']:,}\")\nprint(f\"  Batches output:   {manifest['batches']}\")\nprint(f\"  Output size:      {manifest['output_size_gb']:.2f} GB\")\nprint(f\"{'='*60}\")\n\nif limit_reached:\n    print(f\"\\n‚ö†Ô∏è  BUDGET LIMIT REACHED. To continue:\")\n    print(f\"    1. Save this notebook's output as a Kaggle dataset.\")\n    print(f\"    2. Update PREV_RUN_PATH in Cell 2 to point to it.\")\n    print(f\"    3. Restart and run all. Checkpoints will automatically resume.\")\nelse:\n    print(f\"\\nüü¢ All sterilized vocals processed. Dataset complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T05:59:09.311575Z","iopub.execute_input":"2026-02-26T05:59:09.312093Z","iopub.status.idle":"2026-02-26T06:00:34.867960Z","shell.execute_reply.started":"2026-02-26T05:59:09.312057Z","shell.execute_reply":"2026-02-26T06:00:34.866332Z"}},"outputs":[{"name":"stdout","text":"üü¢ [Manifest] Computing SHA-256 checksums...\n\n============================================================\n  GENESIS DATA CURATION ‚Äî RUN 2 COMPLETE\n============================================================\n  Total triples:    20,623\n  Batches output:   42\n  Output size:      18.48 GB\n============================================================\n\nüü¢ All sterilized vocals processed. Dataset complete!\n","output_type":"stream"}],"execution_count":29},{"id":"1e1d4b25-212b-489e-be61-16143bdc59b9","cell_type":"code","source":"from pathlib import Path\n\ndef print_tree(dir_path=\"/kaggle/working\"):\n    base = Path(dir_path)\n    \n    if not base.exists():\n        print(f\"üî¥ Directory not found: {dir_path}\")\n        return\n        \n    print(f\"üü¢ {base.resolve()}\")\n    \n    # Recursively grab everything and sort it\n    for path in sorted(base.rglob('*')):\n        depth = len(path.relative_to(base).parts)\n        # Create the branching indentation\n        indent = '‚îÇ   ' * (depth - 1) + '‚îú‚îÄ‚îÄ '\n        print(f\"{indent}{path.name}\")\n\n# Run it\nprint_tree()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T06:01:27.895971Z","iopub.execute_input":"2026-02-26T06:01:27.897576Z","iopub.status.idle":"2026-02-26T06:01:27.917581Z","shell.execute_reply.started":"2026-02-26T06:01:27.897531Z","shell.execute_reply":"2026-02-26T06:01:27.916279Z"}},"outputs":[{"name":"stdout","text":"üü¢ /kaggle/working\n‚îú‚îÄ‚îÄ .virtual_documents\n‚îÇ   ‚îú‚îÄ‚îÄ __notebook_source__.ipynb\n‚îú‚îÄ‚îÄ batches\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0000.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0000_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0001.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0001_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0002.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0002_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0003.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0003_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0004.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0004_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0005.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0005_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0006.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0006_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0007.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0007_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0008.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0008_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0009.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0009_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0010.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0010_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0011.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0011_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0012.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0012_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0013.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0013_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0014.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0014_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0015.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0015_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0016.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0016_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0017.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0017_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0018.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0018_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0019.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0019_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0020.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0020_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0021.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0021_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0022.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0022_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0023.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0023_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0024.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0024_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0025.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0025_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0026.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0026_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0027.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0027_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0028.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0028_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0029.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0029_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0030.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0030_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0031.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0031_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0032.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0032_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0033.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0033_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0034.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0034_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0035.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0035_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0036.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0036_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0037.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0037_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0038.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0038_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0039.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0039_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0040.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0040_meta.json\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0041.npz\n‚îÇ   ‚îú‚îÄ‚îÄ batch_0041_meta.json\n‚îú‚îÄ‚îÄ checkpoint.json\n‚îú‚îÄ‚îÄ clap_cache.npz\n‚îú‚îÄ‚îÄ clap_model\n‚îú‚îÄ‚îÄ ir_catalogue.json\n‚îú‚îÄ‚îÄ irs\n‚îÇ   ‚îú‚îÄ‚îÄ mit_irs\n‚îú‚îÄ‚îÄ manifest.json\n","output_type":"stream"}],"execution_count":30},{"id":"8148d746","cell_type":"markdown","source":"## üîó Checkpoint Chaining (20 GB Limit)\n\nIf the output hit the size limit before processing all vocals:\n\n1. **Save this notebook's output** as a Kaggle dataset (e.g. `genesis-data-run1`)\n2. **Create a new notebook** (or re-run this one) and attach:\n   - All the same input datasets (IRs, LJSpeech, VCTK, Language Identifier)\n   - The previous output as input (update `PREV_RUN_PATH` in Cell 3)\n3. **Run all cells** ‚Äî the checkpoint system automatically skips completed work\n\nEach run produces ~19 GB of training triples. Chain as many times as needed.\n\n### Using the Data in Training\n\n```python\n# In the training notebook, load all batches from all runs:\nimport numpy as np\nfrom pathlib import Path\n\nrun_dirs = [\n    Path('/kaggle/input/genesis-data-run1/batches'),\n    Path('/kaggle/input/genesis-data-run2/batches'),\n    # ... add more runs\n]\n\nfor run_dir in run_dirs:\n    for batch_file in sorted(run_dir.glob('batch_*.npz')):\n        data = np.load(batch_file)\n        source_audio = data['source_audio']   # (N, 240000) int16\n        target_audio = data['target_audio']   # (N, 240000) int16\n        target_clap  = data['target_clap']    # (N, CLAP_DIM) float32\n        # Convert int16 back to float32: audio = source_audio.astype(np.float32) / 32767\n        # Compute STFT on-the-fly during training for memory efficiency\n```\n","metadata":{}}]}