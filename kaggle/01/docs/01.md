# 01_acquire_and_augment.md

## 游릭 Phase 0: Environment & Path Configuration
**Description:** Initialize global parameters, deterministic seeds (for reproducible shuffling), and map the exact Kaggle dataset paths. This ensures the data pipeline is rigid, repeatable, and dynamically adjusts to being chained across multiple Kaggle sessions.

**Input Paths:**
* **Bad IRs (Unlabeled):** `/kaggle/input/datasets/itorousa/impulse-responses`
* **Target IRs (MIT Labeled Origin):** `/kaggle/input/datasets/kynthesis/mit-reverb-dataset/MIT_Reverb_Dataset/MIT_Reverb_Dataset`
* **Target IRs (Working Paths):** First run path: `/kaggle/working/irs/mit_irs`. Subsequent runs path: `/kaggle/input/notebooks/itorousa/genesis-data-run1/irs/mit_irs`
* **Dry Vocals (LJSpeech):** `/kaggle/input/datasets/dromosys/ljspeech/`
* **Dry Vocals (VCTK):** `/kaggle/input/datasets/kynthesis/vctk-corpus/VCTK-Corpus/wav48`
* **Dry Vocals (English LangID):** `/kaggle/input/datasets/shrivatssudhir/language-identifier/english/clips`
* **Previous Run (For chaining):** `/kaggle/input/notebooks/itorousa/genesis-data-run1` (adjustable based on previous dataset output)

**Output Paths:**
* **Base:** `/kaggle/working`
* **Subdirectories to create:** `batches/`, `clap_model/`, `sterilized_batches/`, `irs/`

**Audio & Budget Parameters:**
* Sample Rate: 48,000 Hz
* Clip Length: 5.0 seconds (240,000 samples)
* Model Formats: Waveforms saved as `int16` (to be converted to STFT during training), Embeddings as `float32`.
* Batch Size: 500 triples per file
* Output Budget: 19.0 GB maximum (to prevent Kaggle I/O timeouts)
* Augmentations per clip: 3 to 6

---

## 游릭 Phase 1: Aggressive Checkpoint Initialization & Hardware Protection
**Description:** Ensure the script seamlessly resumes from where it left off with extreme granularity. Because Kaggle sessions time out and user fatigue is real, the script will not rely on finishing in one sitting. Every major loop will save its state.

1.  **State Discovery:** * Look for `checkpoint.json` in the **Previous Run** directory. If found, increment the `run_number` and load the operational stats.
    * If not found in the previous run, check the **Current Working Directory** (useful if a session was paused and restarted without committing).
    * If neither exists, initialize a fresh state: `batch_id: 0`, `triples_completed: 0`, `vocal_cursor: 0`.
2.  **Budget Monitoring:** Provide a helper function to calculate the exact current size of `/kaggle/working` in GB. This function will be called throughout the pipeline to enforce the 19GB budget ceiling and safely halt execution before a crash.

---

## 游릭 Phase 2: Impulse Response Acquisition & Pooling
**Description:** Load, standardize, and physically separate the IRs into the "Messy/Bad" pool and the "Target" pool. Avoid redownloading or extracting if previous runs already did the work.

1.  **Stage MIT IRs:** Copy the MIT IRs from the raw dataset (`paths.md`) to `/kaggle/working/irs/mit_irs`. If `PREV_RUN_PATH/irs/mit_irs` exists, copy it from there instead to save processing time.
2.  **Iterate Bad IRs:** Scan the custom user IR path for `.irs` and `.wav` files (`/kaggle/input/datasets/itorousa/impulse-responses`).
3.  **Iterate Target IRs:** Scan the MIT IR path for `.wav` files (`/kaggle/working/irs/mit_irs`).
4.  **Standardization:** For every file found across both directories:
    * Load at 48 kHz.
    * Force to mono (to satisfy CLAP and conserve GPU memory later).
    * Peak-normalize the waveform.
    * Discard if the array is mathematically degenerate (e.g., shorter than 64 samples).
5.  **Pool Assignment:**
    * Push all custom/unlabeled IRs into `bad_pool`.
    * Push all MIT IRs into `target_pool`.
6.  游 **Save Catalogue:** Map the file metadata to `ir_catalogue.json`. On future runs, immediately check for this file. If it exists, bypass the entire audio-loading block and instantly reconstruct the pools from JSON.

---

## 游릭 Phase 3: CLAP Target Embedding Cache
**Description:** Freeze the Hugging Face CLAP model locally and pre-compute the acoustic embeddings for all "Target" (MIT) IRs. 

1.  **Model Loading/Caching:**
    * Check for the frozen `clap_model` directory in the current working directory.
    * If missing, check the **Previous Run** directory. Copy it over via `shutil.copytree` if found.
    * If completely missing, download `laion/larger_clap_music_and_speech`, save it locally to `/kaggle/working/clap_model`, and set to evaluation mode on the GPU.
2.  **Pre-compute Embeddings:**
    * Check for `clap_cache.npz` in the current or previous run. Copy/load it if it exists to entirely skip the convolution logic.
    * If missing: Iterate over every IR strictly in the `target_pool`.
    * Convolve the target IR with 3 seconds of generated white noise so CLAP has a rich frequency spectrum to read.
    * 丘멆잺 **Critical Extraction Step:** Pass the scene through the processor and call `clap_model.get_audio_features()`. From the resulting object, you *must* explicitly extract the `.pooler_output` attribute before moving the tensor to the CPU. Do not call `.cpu()` on the raw wrapper object.
    * Flatten the extracted tensor to a 512-dim NumPy float32 array, and map it to the IR's ID.
    * 游 Save the dictionary as `clap_cache.npz`.

---

## 游릭 Phase 4: Vocal Sterilization & "Dead" Audio Guarantee
**Description:** Extract pristine dry vocals to serve as the mathematical ground truth. Real-world "dry" vocals still have preamp hum and room tone. We must ensure the audio is completely "dead" before any IR is applied.

1.  **Discovery:** Glob all audio files from LJSpeech, VCTK, and the English LangID paths.
2.  **Deterministic Shuffle:** Sort the file list alphabetically, then shuffle using a fixed seed (42). This guarantees the array order remains identical across Kaggle restarts, allowing granular resume functionality.
3.  **Process (starting exactly from vocal_cursor index):**
    * Load file (48kHz, mono). Ignore if < 1.0 seconds.
    * **Spectral Noise Reduction:** Apply a strict noise reduction pass (e.g., via `noisereduce` with a high proportion decrease). This mathematically strips away residual room tone, hiss, or preamp hum, guaranteeing the audio is "dead" and pristine.
    * Trim absolute silence from the top/tail.
    * Measure LUFS. Normalize the dead audio to -23.0 LUFS.
4.  **Segmentation:** Slice into rigid 240,000-sample (5.0s) chunks. Zero-pad the final chunk if short.
5.  **Budget Check:** If `/kaggle/working` > 19.0 GB, gracefully break the loop for a safe Kaggle commit.
6.  **Granular Disk Flushing (RAM Protection):** * Process files in chunks of 500.
    * Once a chunk is processed, 游 dump the segments to a `.pkl` file in `sterilized_batches/`.
    * 游 Write the current index to `sterilize_state.json` as a hard checkpoint.
    * *Clear the RAM array completely* to prevent Kaggle OOM crashes.
7.  **Aggregation:** Read all `.pkl` files from disk to compile the final list of dead, dry segments.

---

**Extremely Important:** The notebook must be able to skip phases 2-4 if the previous run(s) completed them successfully and I judge the dataset size to be sufficient. It can do this by utilizing files from other notebooks located in /kaggle/input/notebooks/itorousa/genesis-data-run#

---

## 游릭 Phase 5: The Messy DSP Toolkit
**Description:** Define a suite of random, destructive audio effects to apply strictly to the `messy_wet_audio` input to simulate terrible recording environments. Checkpointing remains important for this phase.

1.  **Noise Additions:** Generate colored noise (White, Pink, Brown, HVAC, 60Hz hum) and scale it to a random target SNR (5dB to 40dB).
2.  **Parametric EQ:** Randomly shift gain across Low, Mid, and High frequency bands to simulate cheap smartphone microphones or bad placement.
3.  **Filters:** Random Highpass and Lowpass to simulate severe frequency roll-off.
4.  **Bitcrushing/Clipping:** Quantize the audio resolution and apply harsh mathematical clipping.
5.  **Randomizer:** A wrapper that randomly selects 3 to 6 of these functions and applies them sequentially.

---

## 游릭 Phase 6: The Training Data Engine (Triple Generation)
**Description:** The core loop. Create the `(Messy Wet Input, Target MIT IR) -> Clean Target Audio` triples for Genesis.

1.  Resume iteration exactly from `checkpoint['vocal_cursor']`.
2.  **For each dead dry vocal segment:**
    * 丘멆잺 **Budget Check:** Evaluate folder size. If `/kaggle/working` > 19.0 GB, break the loop and prep for shutdown.
    * **Select Spaces:** Pick one random Bad IR and one random Target MIT IR.
    * **Create Target Audio (Ground Truth):** Convolve the dead Dry Vocal solely with the Target MIT IR. Trim to exactly 5s, peak-normalize.
    * **Create Messy Wet Audio (Input):** * Convolve the dead Dry Vocal with the Bad IR.
        * Mix this bad convolution with the raw dead Dry Vocal at a random ratio (e.g., 30% to 100% wet).
        * Pass the result through the Messy DSP Toolkit. Trim to exactly 5s, peak-normalize.
    * **Fetch Conditioning:** Retrieve the pre-computed CLAP float32 embedding for the Target MIT IR from `clap_cache.npz`.
    * **QA Verification:** Measure RMS of both audio signals. Reject and skip the triple if either is silent or contains NaNs.
    * **Quantize:** Convert `messy_wet_audio` and `target_audio` to `int16` arrays to drastically compress disk space.
    * Append all three components (Input Int16, Target Int16, CLAP Float32) to the current batch array.
3.  **Granular Batch Save Mechanism:**
    * When the batch hits 500 triples, 游 save to `batch_{id}.npz`.
    * 游 Update `checkpoint.json` with the new cursor position.
    * Clear the batch lists from RAM immediately.

---

**Extremely Important:** The notebook must be able to skip phases 5 and 6 if the previous run(s) completed them successfully. It can do this by utilizing files from other notebooks located in /kaggle/input/notebooks/itorousa/genesis-data-run#

## 游릭 Phase 7: Manifest & Pipeline Conclusion
**Description:** Wrap up the run, compute integrity checks, and prepare the dataset for the actual Training Notebook.

1.  Flush any remaining triples (< 500) to a final `batch_{id}.npz` file.
2.  **Checksums:** Iterate through all generated `batch_*.npz` files and compute their SHA-256 hashes.
3.  **Generate manifest.json:** 游 Write out the total triples generated in this session, global dataset dimensions, total output size, and the checksum dictionary.
4.  **Status Report:**
    * 丘멆잺 If halted due to the 19GB limit: Print detailed instructions to save the commit as a dataset, update `PREV_RUN_PATH` in the next notebook, and restart to continue the chain.
    * 游릭 If completed (all vocals processed): Print a success message indicating the curation phase is entirely finished and the output is ready to be fed into Genesis's STFT Dataloader in the training phase.