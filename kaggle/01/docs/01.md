# 01_acquire_and_augment.md

## 游릭 Phase 0: Environment & Path Configuration
**Description:** Initialize global parameters, deterministic seeds (for reproducible shuffling), and map the exact Kaggle dataset paths. This ensures the data pipeline is rigid, repeatable, and dynamically adjusts to being chained across multiple Kaggle sessions.

**Input Paths:**
* **Bad IRs (Unlabeled):** `/kaggle/input/datasets/itorousa/impulse-responses`
* **Target IRs (MIT Labeled Origin):** `/kaggle/input/datasets/kynthesis/mit-reverb-dataset/MIT_Reverb_Dataset/MIT_Reverb_Dataset`
* **Target IRs (Working Paths):** First run path: `/kaggle/working/irs/mit_irs`. Subsequent runs path: `/kaggle/input/notebooks/itorousa/genesis-data-run1/irs/mit_irs`
* **Dry Vocals (LJSpeech):** `/kaggle/input/datasets/dromosys/ljspeech/`
* **Dry Vocals (VCTK):** `/kaggle/input/datasets/kynthesis/vctk-corpus/VCTK-Corpus/wav48`
* **Previous Run (For chaining):** `/kaggle/input/notebooks/itorousa/genesis-data-run1`

**Output Paths:**
* **Base:** `/kaggle/working`
* **Subdirectories to create:** `batches/`, `clap_model/`, `sterilized_batches/`, `irs/`

**Audio & Budget Parameters:**
* Sample Rate: 48,000 Hz
* Clip Length: 5.0 seconds (240,000 samples)
* Model Formats: Waveforms saved as `int16` (to be converted to STFT during training), Embeddings as `float32`.
* Batch Size: 500 triples per file
* Output Budget: 19.0 GB maximum (to prevent Kaggle I/O timeouts)

---

## 游릭 Phase 1: Aggressive Checkpoint Initialization & Hardware Protection
**Description:** Ensure the script seamlessly resumes from where it left off with extreme granularity. Every major loop will save its state.

1.  **State Discovery:** * Look for `checkpoint.json` in the **Previous Run** directory. If found, increment the `run_number` and load the operational stats.
    * If not found, check the **Current Working Directory**.
    * If neither exists, initialize a fresh state: `batch_id: 0`, `triples_completed: 0`, `vocal_cursor: 0`.
2.  **Budget Monitoring:** A helper function will calculate the exact current size of `/kaggle/working` in GB to enforce the 19GB budget ceiling and safely halt execution.

---

## 游릭 Phase 2: Impulse Response Acquisition & Pooling
**Description:** Load, standardize, and physically separate the IRs. 
*Extremely Important: This notebook must dynamically skip this phase if the previous run completed it successfully.*

1.  **Stage MIT IRs:** Copy the MIT IRs to `/kaggle/working/irs/mit_irs`. 
2.  **Standardization:** For every file found across both directories:
    * Load at 48 kHz.
    * Force to mono.
    * Peak-normalize the waveform.
    * Discard if shorter than 64 samples.
3.  **Pool Assignment:**
    * Push all custom/unlabeled IRs into the `bad_pool`.
    * Push all labeled MIT IRs into the `target_pool`.
4.  游 **Save Catalogue:** Map the metadata to `ir_catalogue.json`. 

---

## 游릭 Phase 3: CLAP Target Embedding Cache
**Description:** Freeze the Hugging Face CLAP model locally and pre-compute the acoustic embeddings for all "Target" (MIT) IRs. 

1.  **Model Loading/Caching:** Check for the frozen `clap_model` locally or in the Previous Run. If missing, download `laion/larger_clap_music_and_speech` and set to evaluation mode on the GPU.
2.  **Pre-compute Embeddings:**
    * Convolve the target IR with 3 seconds of generated white noise.
    * 丘멆잺 **Critical Extraction Step:** Pass the scene through the processor and explicitly extract the `.pooler_output` attribute before moving the tensor to the CPU. 
    * Flatten to a 512-dim NumPy float32 array and save the dictionary as `clap_cache.npz`.

---

## 游릭 Phase 4: Vocal Sterilization & "Dead" Audio Guarantee
**Description:** Extract pristine dry vocals to serve as the mathematical ground truth. We must ensure the audio is completely "dead" before any IR is applied.

1.  **Discovery & Shuffle:** Glob all audio files from LJSpeech, VCTK, and the English LangID paths. Sort alphabetically and shuffle using a fixed seed (42) for reproducibility.
2.  **Process:**
    * **Spectral Noise Reduction:** Strip residual room tone/hiss via `noisereduce`.
    * Trim absolute silence.
    * Normalize to -23.0 LUFS.
3.  **Segmentation:** Slice into rigid 240,000-sample (5.0s) chunks. 
4.  **Granular Disk Flushing:** Process files in chunks of 500. Dump segments to `.pkl` files in `sterilized_batches/` and clear RAM completely. Update `sterilize_state.json`.

---

## 游릭 Phase 5: The Messy DSP Toolkit
**Description:** Define a suite of destructive audio effects to apply strictly to the input to simulate terrible recording environments. 

1.  **Noise Additions:** Generate colored noise (White, Pink, Brown, HVAC, 60Hz hum) and hardware pops.
2.  **Parametric EQ:** Randomly shift gain to simulate cheap microphones.
3.  **Filters (Bandwidth Truncation):** Aggressive Highpass and Lowpass filters to simulate "telephone" bandwidth.
4.  **Bitcrushing/Clipping:** Quantize resolution and apply harsh mathematical clipping.

---

## 游릭 Phase 6: The Training Data Engine (The 20k Matrix)
**Description:** The core loop. Create exactly 20,000 triples distributed rigidly across five distinct acoustic repair scenarios.
*Extremely Important: Skip if previous runs generated sufficient batches.*


1.  Resume iteration exactly from `checkpoint['vocal_cursor']`.
2.  **Weighted Scenario Selection:** For each dead dry vocal segment, select one of the following scenarios based on a tracking algorithm to ensure exact percentage distributions:
    * **Scenario 1: Signal Extraction (20%)**
        * Target: `Vocal + Target IR`
        * Input: `Vocal + Bad IR + Heavy Noise` (or variations with Dry/Noise).
    * **Scenario 2: Harmonic Inpainting (20%)**
        * Target: `Vocal + Target IR`
        * Input: `Telephone-Filtered Vocal + Target IR`.
    * **Scenario 3: Acoustic Stripping (20%)**
        * Target: `Vocal + Target IR` (or Dry)
        * Input: `Vocal + Muddy Bad IR`.
    * **Scenario 4: Target Projection (20%)**
        * Target: `Vocal + Target IR`
        * Input: `Dry Vocal`.
    * **Scenario 5: Pure Denoising (20%)**
        * Target: `Dry Vocal`
        * Input: `Dry Vocal + Severe Noise/Crackle`.
3.  **Fetch Conditioning:** Retrieve the pre-computed CLAP float32 embedding for the Target state from `clap_cache.npz`.
4.  **QA & Quantize:** Reject if silent or containing NaNs. Convert input and target to `int16` arrays.
5.  **Granular Batch Save:** Save to `batch_{id}.npz` every 500 triples, update `checkpoint.json`, and clear RAM. Break loop if budget exceeds 19.0 GB.

---

## 游릭 Phase 7: Manifest & Pipeline Conclusion
**Description:** Wrap up the run, compute integrity checks, and prepare the dataset for the actual Training Notebook.

1.  Flush any remaining triples (< 500) to a final `batch_{id}.npz` file.
2.  **Checksums:** Compute SHA-256 hashes for all generated batches.
3.  **Generate manifest.json:** 游 Write out the total triples generated, global dataset dimensions, total output size, and the checksum dictionary.
4.  **Status Report:**
    * 丘멆잺 If halted due to the 19GB limit: Print detailed instructions to save the commit as a dataset, update `PREV_RUN_PATH` in the next notebook, and restart to continue the chain.
    * 游릭 If completed: Print a success message indicating the output is ready for Genesis's STFT Dataloader.