{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14927063,"datasetId":9551535,"databundleVersionId":15794187},{"sourceType":"datasetVersion","sourceId":7085845,"datasetId":4060468,"databundleVersionId":7173592},{"sourceType":"datasetVersion","sourceId":101413,"datasetId":53291,"databundleVersionId":103953},{"sourceType":"datasetVersion","sourceId":4588404,"datasetId":2675000,"databundleVersionId":4649793}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"02d93efa","cell_type":"markdown","source":"# ðŸ§¬ Genesis IR Head â€” Data Curation Pipeline\n\n**Notebook 01**: Acquire â†’ Sterilize â†’ Augment â†’ Target\n\nThis notebook produces training triples for Genesis's IR Synthesis Head:\n\n| Tensor | Format | Description |\n|--------|--------|-------------|\n| `source_wet` | int16 waveform | Sterile vocal + bad-room IR + random degradations |\n| `target_wet` | int16 waveform | Same sterile vocal + studio IR (ground truth) |\n| `target_clap` | float32 vector | CLAP embedding of the target-wet audio |\n\n**Checkpoint chaining**: Output is capped at ~19 GB. A `checkpoint.json` tracks\nprogress so the next run (using this output as input) continues where we left off.\n","metadata":{}},{"id":"1cdc49b0","cell_type":"code","source":"!pip install -q noisereduce pyloudnorm soundfile librosa transformers torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:45:25.691801Z","iopub.execute_input":"2026-02-23T11:45:25.692147Z","iopub.status.idle":"2026-02-23T11:45:29.901000Z","shell.execute_reply.started":"2026-02-23T11:45:25.692113Z","shell.execute_reply":"2026-02-23T11:45:29.899713Z"}},"outputs":[],"execution_count":2},{"id":"86ffd3a4","cell_type":"code","source":"import os, json, hashlib, time, random, warnings, struct\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple\nfrom collections import defaultdict\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom scipy.signal import fftconvolve\nimport noisereduce as nr\n\nwarnings.filterwarnings('ignore')\nrandom.seed(42)\nnp.random.seed(42)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# KAGGLE INPUT PATHS  (datasets attached to this notebook)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPATHS = {\n    'irs':       Path('/kaggle/input/datasets/itorousa/impulse-responses'),\n    'ljspeech':  Path('/kaggle/input/datasets/dromosys/ljspeech/'),\n    'vctk':      Path('/kaggle/input/datasets/kynthesis/vctk-corpus/VCTK-Corpus/wav48'),\n    'langid_en': Path('/kaggle/input/datasets/shrivatssudhir/language-identifier/english/clips'),\n}\n\n# If chaining from a previous run, attach its output as input here:\nPREV_RUN_PATH = Path('/kaggle/input/genesis-data-run1')  # adjust per run\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# OUTPUT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nOUTPUT       = Path('/kaggle/working')\nBATCH_DIR    = OUTPUT / 'batches'\nACQUIRED_DIR = OUTPUT / 'acquired_irs'\nCLAP_DIR     = OUTPUT / 'clap_model'\n\nfor d in [BATCH_DIR, ACQUIRED_DIR, CLAP_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# AUDIO PARAMETERS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSR            = 48000\nCLIP_SEC      = 5.0\nCLIP_SAMPLES  = int(SR * CLIP_SEC)    # 240,000 samples\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# BUDGET & BATCHING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTRIPLES_PER_BATCH  = 500\nMAX_OUTPUT_GB      = 19.0             # safety margin under 20 GB cap\nAUGMENTATIONS_MIN  = 3\nAUGMENTATIONS_MAX  = 6\n\nprint(f\"SR={SR}, CLIP_SEC={CLIP_SEC}, CLIP_SAMPLES={CLIP_SAMPLES}\")\nprint(f\"Output budget: {MAX_OUTPUT_GB} GB, batch size: {TRIPLES_PER_BATCH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:45:29.904181Z","iopub.execute_input":"2026-02-23T11:45:29.904530Z","iopub.status.idle":"2026-02-23T11:45:34.195342Z","shell.execute_reply.started":"2026-02-23T11:45:29.904496Z","shell.execute_reply":"2026-02-23T11:45:34.194478Z"}},"outputs":[{"name":"stdout","text":"SR=48000, CLIP_SEC=5.0, CLIP_SAMPLES=240000\nOutput budget: 19.0 GB, batch size: 500\n","output_type":"stream"}],"execution_count":3},{"id":"322d8a4f","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CHECKPOINT  â€” resume across runs\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nCHECKPOINT_PATH = OUTPUT / 'checkpoint.json'\n\ndef load_checkpoint() -> dict:\n    \"\"\"Load from previous run's output (if chained) or current working dir.\"\"\"\n    # Check previous run first\n    prev_ckpt = PREV_RUN_PATH / 'checkpoint.json'\n    if prev_ckpt.exists():\n        with open(prev_ckpt) as f:\n            ckpt = json.load(f)\n        ckpt['run_number'] += 1\n        print(f\"[Checkpoint] Resuming from previous run: {ckpt['triples_completed']} triples done\")\n        return ckpt\n\n    # Check current working dir\n    if CHECKPOINT_PATH.exists():\n        with open(CHECKPOINT_PATH) as f:\n            return json.load(f)\n\n    # Fresh start\n    return {\n        'batch_id': 0,\n        'triples_completed': 0,\n        'vocal_cursor': 0,        # index into shuffled vocal list\n        'run_number': 1,\n    }\n\ndef save_checkpoint(ckpt: dict):\n    with open(CHECKPOINT_PATH, 'w') as f:\n        json.dump(ckpt, f, indent=2)\n\ndef get_output_size_gb() -> float:\n    total = sum(f.stat().st_size for f in OUTPUT.rglob('*') if f.is_file())\n    return total / (1024 ** 3)\n\nckpt = load_checkpoint()\nprint(f\"[Checkpoint] Run #{ckpt['run_number']}, \"\n      f\"{ckpt['triples_completed']} triples completed so far, \"\n      f\"starting at vocal cursor {ckpt['vocal_cursor']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:45:34.196733Z","iopub.execute_input":"2026-02-23T11:45:34.197141Z","iopub.status.idle":"2026-02-23T11:45:34.206398Z","shell.execute_reply.started":"2026-02-23T11:45:34.197105Z","shell.execute_reply":"2026-02-23T11:45:34.205522Z"}},"outputs":[{"name":"stdout","text":"[Checkpoint] Run #1, 0 triples completed so far, starting at vocal cursor 0\n","output_type":"stream"}],"execution_count":4},{"id":"50c8c54d","cell_type":"code","source":"import urllib.request, zipfile\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 1: ACQUIRE â€” download external IR collections\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef download_if_needed(url: str, dest_dir: Path, name: str):\n    marker = dest_dir / f'.{name}_done'\n    if marker.exists():\n        print(f\"  [Acquire] {name} already downloaded\")\n        return\n    print(f\"  [Acquire] Downloading {name}...\")\n    zip_path = dest_dir / f'{name}.zip'\n    urllib.request.urlretrieve(url, str(zip_path))\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        zf.extractall(dest_dir / name)\n    zip_path.unlink()\n    marker.touch()\n    print(f\"  [Acquire] {name} âœ“\")\n\n# MIT Impulse Response Survey â€” 271 real-world room IRs (CC-BY 4.0)\ndownload_if_needed(\n    'https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip',\n    ACQUIRED_DIR, 'mit_irs'\n)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Load & normalize every IR to 48 kHz mono\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef load_and_normalize_ir(filepath: Path, target_sr: int = SR) -> Optional[np.ndarray]:\n    \"\"\"Load an IR file, force mono / target SR, peak-normalize.\"\"\"\n    try:\n        audio, _ = librosa.load(str(filepath), sr=target_sr, mono=True)\n        if len(audio) < 64:        # degenerate\n            return None\n        peak = np.max(np.abs(audio))\n        if peak > 1e-6:\n            audio = audio / peak\n        return audio.astype(np.float32)\n    except Exception:\n        return None\n\nprint(\"\\n[Acquire] Loading all impulse responses...\")\nall_irs: Dict[str, dict] = {}\n\n# 1) User's IRs  (supports .irs and .wav)\nfor ext in ('*.irs', '*.wav'):\n    for f in sorted(PATHS['irs'].glob(ext)):\n        ir = load_and_normalize_ir(f)\n        if ir is not None:\n            all_irs[f'user_{f.stem}'] = {'audio': ir, 'source': 'user'}\n\n# 2) MIT IRs\nmit_dir = ACQUIRED_DIR / 'mit_irs'\nif mit_dir.exists():\n    for f in sorted(mit_dir.rglob('*.wav')):\n        ir = load_and_normalize_ir(f)\n        if ir is not None:\n            all_irs[f'mit_{f.stem}'] = {'audio': ir, 'source': 'mit'}\n\nprint(f\"[Acquire] Loaded {len(all_irs)} IRs \"\n      f\"(user: {sum(1 for v in all_irs.values() if v['source']=='user')}, \"\n      f\"MIT: {sum(1 for v in all_irs.values() if v['source']=='mit')})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:45:34.207408Z","iopub.execute_input":"2026-02-23T11:45:34.207669Z","iopub.status.idle":"2026-02-23T11:45:53.256982Z","shell.execute_reply.started":"2026-02-23T11:45:34.207645Z","shell.execute_reply":"2026-02-23T11:45:53.256004Z"}},"outputs":[{"name":"stdout","text":"  [Acquire] mit_irs already downloaded\n\n[Acquire] Loading all impulse responses...\n[Acquire] Loaded 659 IRs (user: 389, MIT: 270)\n","output_type":"stream"}],"execution_count":5},{"id":"c30ed307","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Classify IRs into \"bad\" (room/degraded) and \"studio\" (clean target)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef compute_ir_features(ir_audio: np.ndarray, sr: int = SR) -> dict:\n    \"\"\"Compute RT60 estimate, spectral centroid, and clarity (C50).\"\"\"\n    energy = ir_audio ** 2\n    cumsum = np.cumsum(energy[::-1])[::-1]\n\n    # RT60 â€” time for energy to decay 60 dB\n    rt60 = len(ir_audio) / sr\n    if cumsum[0] > 1e-10:\n        decay_db = 10 * np.log10(cumsum / cumsum[0] + 1e-12)\n        idx = np.where(decay_db < -60)[0]\n        if len(idx) > 0:\n            rt60 = idx[0] / sr\n\n    # Spectral centroid\n    S = np.abs(np.fft.rfft(ir_audio))\n    freqs = np.fft.rfftfreq(len(ir_audio), 1 / sr)\n    centroid = float(np.sum(freqs * S) / (np.sum(S) + 1e-10))\n\n    # Clarity C50 â€” early-to-late energy ratio at 50 ms boundary\n    split = int(0.05 * sr)\n    early = np.sum(ir_audio[:split] ** 2) + 1e-12\n    late  = np.sum(ir_audio[split:] ** 2) + 1e-12\n    c50   = float(10 * np.log10(early / late))\n\n    return {\n        'rt60': round(rt60, 4),\n        'centroid': round(centroid, 1),\n        'c50': round(c50, 2),\n        'length_sec': round(len(ir_audio) / sr, 4),\n    }\n\nir_catalogue = {}\nbad_pool:    List[str] = []   # contamination IRs (rooms, halls, degraded)\nstudio_pool: List[str] = []   # target IRs (mastering, clarity, studio)\n\nfor ir_id, ir_data in all_irs.items():\n    feats = compute_ir_features(ir_data['audio'])\n    ir_data['features'] = feats\n    ir_catalogue[ir_id] = {'source': ir_data['source'], **feats}\n\n    # Heuristic:\n    #   MIT IRs â†’ always bad pool (real rooms)\n    #   User IRs with long RT60 or low clarity â†’ bad pool\n    #   User IRs with short RT60 and high clarity â†’ studio pool\n    if ir_data['source'] == 'mit':\n        bad_pool.append(ir_id)\n    elif feats['rt60'] > 0.25 or feats['c50'] < 8:\n        bad_pool.append(ir_id)\n    else:\n        studio_pool.append(ir_id)\n\n# Safety: ensure both pools are adequate\nif len(studio_pool) < 20:\n    # Sort user IRs by clarity, take top third as studio\n    user_ids = sorted(\n        [k for k, v in all_irs.items() if v['source'] == 'user'],\n        key=lambda k: all_irs[k]['features']['c50'], reverse=True\n    )\n    studio_pool = user_ids[:max(50, len(user_ids) // 3)]\n    bad_pool = [k for k in all_irs if k not in studio_pool]\n\nprint(f\"[Classify] Bad pool: {len(bad_pool)} IRs\")\nprint(f\"[Classify] Studio pool: {len(studio_pool)} IRs\")\n\n# Save catalogue\nwith open(OUTPUT / 'ir_catalogue.json', 'w') as f:\n    json.dump(ir_catalogue, f, indent=2)\nprint(\"[Classify] ir_catalogue.json saved âœ“\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:45:53.259886Z","iopub.execute_input":"2026-02-23T11:45:53.260214Z","iopub.status.idle":"2026-02-23T11:45:55.065472Z","shell.execute_reply.started":"2026-02-23T11:45:53.260186Z","shell.execute_reply":"2026-02-23T11:45:55.064555Z"}},"outputs":[{"name":"stdout","text":"[Classify] Bad pool: 320 IRs\n[Classify] Studio pool: 339 IRs\n[Classify] ir_catalogue.json saved âœ“\n","output_type":"stream"}],"execution_count":6},{"id":"b2c4c6db","cell_type":"code","source":"import torch\nfrom transformers import ClapModel, ClapProcessor\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 2: Freeze CLAP model & cache studio-pool embeddings\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCLAP_MODEL_ID = \"laion/larger_clap_music_and_speech\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"[CLAP] Device: {device}\")\n\nprint(\"[CLAP] Loading model...\")\nclap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)\nclap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(device).eval()\nCLAP_DIM = clap_model.config.projection_dim\nprint(f\"[CLAP] Loaded â€” embedding dim = {CLAP_DIM}\")\n\n# Save frozen model for downstream notebooks\nclap_model.save_pretrained(CLAP_DIR)\nclap_processor.save_pretrained(CLAP_DIR)\nprint(f\"[CLAP] Frozen model saved to {CLAP_DIR}\")\n\n\ndef get_clap_audio_embedding(audio: np.ndarray, sr: int = SR) -> np.ndarray:\n    \"\"\"Encode audio through CLAP's audio tower. Returns (CLAP_DIM,) float32.\"\"\"\n    inputs = clap_processor(\n        audio=audio, sampling_rate=sr, return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = clap_model.get_audio_features(**inputs)\n        # Extract the pooled embedding tensor from the output object\n        emb = outputs.pooler_output\n    return emb.cpu().numpy().flatten().astype(np.float32)\n\n\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Pre-compute CLAP embeddings for every studio-pool IR.\n# We convolve each IR with 3 seconds of white noise so CLAP has\n# a richer scene to analyse (raw IRs are too short / sparse).\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(f\"\\n[CLAP] Pre-computing embeddings for {len(studio_pool)} studio IRs...\")\nref_noise = np.random.randn(SR * 3).astype(np.float32) * 0.1\n\nclap_cache: Dict[str, np.ndarray] = {}\nfor i, ir_id in enumerate(studio_pool):\n    ir_audio = all_irs[ir_id]['audio']\n    scene = fftconvolve(ref_noise, ir_audio, mode='full')[:SR * 3]\n    scene = scene / (np.max(np.abs(scene)) + 1e-8)\n    clap_cache[ir_id] = get_clap_audio_embedding(scene)\n    if (i + 1) % 50 == 0:\n        print(f\"  {i+1}/{len(studio_pool)}\")\n\nnp.savez(OUTPUT / 'clap_cache.npz', **clap_cache)\nprint(f\"[CLAP] Cached {len(clap_cache)} embeddings âœ“  (dim={CLAP_DIM})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:45:55.066655Z","iopub.execute_input":"2026-02-23T11:45:55.067010Z","iopub.status.idle":"2026-02-23T11:49:26.367278Z","shell.execute_reply.started":"2026-02-23T11:45:55.066977Z","shell.execute_reply":"2026-02-23T11:49:26.366387Z"}},"outputs":[{"name":"stdout","text":"[CLAP] Device: cpu\n[CLAP] Loading model...\n","output_type":"stream"},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dae95410883498fa806e82d87ea2b01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cbdff204c0f48058dae3ebc399e3388"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d2ee8f7e7314d4585138b0685bd6f52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0905b4738c3c46ce87c9d1fc5f5d0ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24d0b02c7b446b6bb96437d5bd26746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90be0eb1e6241bc8b5383ed02904bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce270dd33ce549dc830453e326094ab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/776M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cdda11d904e4ee88b8e26b207432171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/555 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2352d0191b9c4ea4820cc47a0f853778"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/776M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710068b28c9147fab6b0cc62051b9330"}},"metadata":{}},{"name":"stdout","text":"[CLAP] Loaded â€” embedding dim = 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1fb6d6eb22043a2b0134786d0483b9c"}},"metadata":{}},{"name":"stdout","text":"[CLAP] Frozen model saved to /kaggle/working/clap_model\n\n[CLAP] Pre-computing embeddings for 339 studio IRs...\n  50/339\n  100/339\n  150/339\n  200/339\n  250/339\n  300/339\n[CLAP] Cached 339 embeddings âœ“  (dim=512)\n","output_type":"stream"}],"execution_count":7},{"id":"cd282e26","cell_type":"code","source":"import json\nimport pickle\nimport random\nfrom pathlib import Path\nfrom collections import defaultdict\nimport numpy as np\nimport pyloudnorm as pyln\nimport librosa\nimport noisereduce as nr\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 3: STERILIZE â€” noise reduce, trim, segment, normalize\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nmeter = pyln.Meter(SR)\n\ndef discover_audio_files() -> List[Tuple[Path, str]]:\n    \"\"\"Collect all vocal files from all datasets. Returns (path, dataset_tag).\"\"\"\n    files = []\n    # LJSpeech\n    for f in sorted(PATHS['ljspeech'].rglob('*.wav')):\n        files.append((f, 'ljspeech'))\n    # VCTK â€” subfolder per speaker\n    for f in sorted(PATHS['vctk'].rglob('*.wav')):\n        files.append((f, f'vctk_{f.parent.name}'))\n    # Language Identifier â€” english clips\n    for ext in ('*.wav', '*.mp3', '*.ogg'):\n        for f in sorted(PATHS['langid_en'].rglob(ext)):\n            files.append((f, 'langid_en'))\n    return files\n\n\ndef sterilize_and_segment(filepath: Path, tag: str) -> List[dict]:\n    \"\"\"\n    Load â†’ noise-reduce â†’ trim silence â†’ segment into CLIP_SEC windows.\n    Returns list of dicts with 'audio' (float32) and metadata.\n    \"\"\"\n    try:\n        audio, sr_orig = librosa.load(str(filepath), sr=SR, mono=True)\n    except Exception:\n        return []\n\n    if len(audio) < SR * 1.0:   # skip clips shorter than 1 second\n        return []\n\n    # â”€â”€ Spectral noise reduction (simulates Ursula's output) â”€â”€\n    audio = nr.reduce_noise(y=audio, sr=SR, stationary=True, prop_decrease=0.85)\n\n    # â”€â”€ Trim silence â”€â”€\n    audio, _ = librosa.effects.trim(audio, top_db=40)\n    if len(audio) < SR * 1.5:\n        return []\n\n    # â”€â”€ Normalize loudness to -23 LUFS â”€â”€\n    try:\n        loudness = meter.integrated_loudness(audio)\n        if loudness > -70:  # not silence\n            audio = pyln.normalize.loudness(audio, loudness, -23.0)\n    except Exception:\n        pass\n\n    # â”€â”€ Segment into CLIP_SEC windows â”€â”€\n    segments = []\n    for start in range(0, len(audio) - SR, CLIP_SAMPLES):\n        chunk = audio[start : start + CLIP_SAMPLES]\n        if len(chunk) < CLIP_SAMPLES:\n            chunk = np.pad(chunk, (0, CLIP_SAMPLES - len(chunk)))\n\n        # Skip near-silent segments\n        rms = np.sqrt(np.mean(chunk ** 2))\n        if rms < 1e-4:\n            continue\n\n        segments.append({\n            'audio':   chunk.astype(np.float32),\n            'file':    filepath.name,\n            'dataset': tag,\n        })\n    return segments\n\n\n# â”€â”€ Discover & sterilize â”€â”€\nprint(\"ðŸŸ¢ [Sterilize] Discovering audio files...\")\nall_audio_files = discover_audio_files()\n\n# Sort first to guarantee deterministic order, then seed the shuffle.\n# This ensures Kaggle processes the exact same files in the same order if the kernel restarts.\nall_audio_files.sort(key=lambda x: str(x[0]))\nrandom.Random(42).shuffle(all_audio_files)\n\nprint(f\"ðŸŸ¢ [Sterilize] Found {len(all_audio_files)} source files\")\n\n# â”€â”€ Checkpoint & VRAM/RAM Setup â”€â”€\nSTERILIZE_CHUNK = 500   # files processed at a time before flushing to disk\nOUTPUT_DIR = Path('/kaggle/working/sterilized_batches')\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nSTATE_FILE = OUTPUT_DIR / 'sterilize_state.json'\n\ncursor_start = 0\nif STATE_FILE.exists():\n    with open(STATE_FILE, 'r') as f:\n        cursor_start = json.load(f).get('cursor', 0)\n\nvocal_segments: List[dict] = []\n\nprint(f\"ðŸŸ¢ [Sterilize] Processing from file index {cursor_start}...\")\n\nfor i in range(cursor_start, len(all_audio_files)):\n    fpath, tag = all_audio_files[i]\n    \n    if i % 200 == 0 and i > cursor_start:\n        print(f\"ðŸŸ¢ Processed {i}/{len(all_audio_files)} files â†’ \"\n              f\"{len(vocal_segments)} segments in current RAM buffer\")\n        \n    segs = sterilize_and_segment(fpath, tag)\n    vocal_segments.extend(segs)\n\n    # Save to disk every STERILIZE_CHUNK files, or on the absolute last file\n    if (i + 1) % STERILIZE_CHUNK == 0 or (i + 1) == len(all_audio_files):\n        batch_index = (i + 1) // STERILIZE_CHUNK\n        batch_path = OUTPUT_DIR / f\"sterilized_batch_{batch_index:04d}.pkl\"\n        \n        with open(batch_path, 'wb') as f:\n            pickle.dump(vocal_segments, f)\n            \n        with open(STATE_FILE, 'w') as f:\n            json.dump({'cursor': i + 1}, f)\n            \n        print(f\"ðŸ’¾ Saved {len(vocal_segments)} segments to {batch_path.name}. RAM cleared.\")\n        vocal_segments.clear() # Dump RAM to prevent Kaggle OOM crash\n\n# â”€â”€ Aggregate counts from all saved batches for the final printout â”€â”€\nprint(\"\\nðŸŸ¢ [Sterilize] Process complete. Compiling dataset breakdown from disk...\")\nds_counts = defaultdict(int)\ntotal_segments = 0\n\nfor batch_file in sorted(OUTPUT_DIR.glob('*.pkl')):\n    with open(batch_file, 'rb') as f:\n        batch_data = pickle.load(f)\n        total_segments += len(batch_data)\n        for s in batch_data:\n            ds_counts[s['dataset'].split('_')[0]] += 1\n\nprint(f\"ðŸŸ¢ [Sterilize] Total sterile segments across all batches: {total_segments}\")\nfor ds, cnt in sorted(ds_counts.items()):\n    print(f\"  {ds}: {cnt}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T11:49:26.368502Z","iopub.execute_input":"2026-02-23T11:49:26.369152Z"}},"outputs":[{"name":"stdout","text":"ðŸŸ¢ [Sterilize] Discovering audio files...\nðŸŸ¢ [Sterilize] Found 133835 source files\nðŸŸ¢ [Sterilize] Processing from file index 0...\nðŸŸ¢ Processed 200/133835 files â†’ 230 segments in current RAM buffer\nðŸŸ¢ Processed 400/133835 files â†’ 462 segments in current RAM buffer\nðŸ’¾ Saved 579 segments to sterilized_batch_0001.pkl. RAM cleared.\nðŸŸ¢ Processed 600/133835 files â†’ 111 segments in current RAM buffer\nðŸŸ¢ Processed 800/133835 files â†’ 340 segments in current RAM buffer\nðŸ’¾ Saved 561 segments to sterilized_batch_0002.pkl. RAM cleared.\nðŸŸ¢ Processed 1000/133835 files â†’ 0 segments in current RAM buffer\nðŸŸ¢ Processed 1200/133835 files â†’ 219 segments in current RAM buffer\nðŸŸ¢ Processed 1400/133835 files â†’ 431 segments in current RAM buffer\nðŸ’¾ Saved 541 segments to sterilized_batch_0003.pkl. RAM cleared.\nðŸŸ¢ Processed 1600/133835 files â†’ 108 segments in current RAM buffer\nðŸŸ¢ Processed 1800/133835 files â†’ 320 segments in current RAM buffer\n","output_type":"stream"}],"execution_count":null},{"id":"4bd7e817","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DEGRADATION TOOLKIT â€” applied to source_wet ONLY\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef add_noise(audio: np.ndarray, noise_type: str, snr_db: float) -> np.ndarray:\n    \"\"\"Mix coloured noise at the target SNR.\"\"\"\n    n = len(audio)\n    if noise_type == 'white':\n        noise = np.random.randn(n)\n    elif noise_type == 'pink':\n        # Pink noise via spectral shaping\n        freqs = np.fft.rfftfreq(n, 1 / SR)\n        freqs[0] = 1\n        S = 1.0 / np.sqrt(freqs)\n        noise = np.fft.irfft(S * np.exp(2j * np.pi * np.random.rand(len(S))))[:n]\n    elif noise_type == 'brown':\n        noise = np.cumsum(np.random.randn(n))\n        noise -= np.mean(noise)\n    elif noise_type == 'hvac':\n        # Band-limited broadband 100-1000 Hz\n        noise = np.random.randn(n)\n        from scipy.signal import butter, sosfilt\n        sos = butter(4, [100, 1000], btype='band', fs=SR, output='sos')\n        noise = sosfilt(sos, noise)\n    elif noise_type == 'hum':\n        # 50 Hz fundamental + harmonics\n        t = np.arange(n) / SR\n        base_freq = random.choice([50, 60])\n        noise = np.zeros(n)\n        for h in range(1, 6):\n            amp = 1.0 / h\n            noise += amp * np.sin(2 * np.pi * base_freq * h * t + random.uniform(0, 2*np.pi))\n    else:\n        noise = np.random.randn(n)\n\n    # Scale noise to target SNR\n    sig_power = np.mean(audio ** 2) + 1e-12\n    noise_power = np.mean(noise ** 2) + 1e-12\n    target_noise_power = sig_power / (10 ** (snr_db / 10))\n    noise = noise * np.sqrt(target_noise_power / noise_power)\n    return audio + noise.astype(np.float32)\n\n\ndef apply_eq(audio: np.ndarray) -> np.ndarray:\n    \"\"\"Random 3-band parametric EQ (simulates mic coloration).\"\"\"\n    from scipy.signal import butter, sosfilt\n    bands = [\n        (80, 300),     # low\n        (300, 3000),   # mid\n        (3000, 12000), # high\n    ]\n    for lo, hi in bands:\n        gain_db = random.uniform(-6, 6)\n        if abs(gain_db) < 1:\n            continue\n        try:\n            sos = butter(2, [lo, hi], btype='band', fs=SR, output='sos')\n            band_sig = sosfilt(sos, audio)\n            gain_lin = 10 ** (gain_db / 20)\n            audio = audio + band_sig * (gain_lin - 1)\n        except Exception:\n            pass\n    return audio.astype(np.float32)\n\n\ndef apply_highpass(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    cutoff = random.uniform(60, 300)\n    sos = butter(4, cutoff, btype='high', fs=SR, output='sos')\n    return sosfilt(sos, audio).astype(np.float32)\n\n\ndef apply_lowpass(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    cutoff = random.uniform(3000, 16000)\n    sos = butter(4, cutoff, btype='low', fs=SR, output='sos')\n    return sosfilt(sos, audio).astype(np.float32)\n\n\ndef apply_gain_jitter(audio: np.ndarray) -> np.ndarray:\n    gain_db = random.uniform(-6, 6)\n    return audio * (10 ** (gain_db / 20))\n\n\ndef apply_bitcrush(audio: np.ndarray) -> np.ndarray:\n    bits = random.randint(8, 16)\n    levels = 2 ** bits\n    return (np.round(audio * levels) / levels).astype(np.float32)\n\n\n# Registry of all degradation functions\nDEGRADATIONS = {\n    'noise':     lambda a: add_noise(a, random.choice(['white','pink','brown','hvac','hum']),\n                                     random.uniform(5, 40)),\n    'eq':        apply_eq,\n    'highpass':  apply_highpass,\n    'lowpass':   apply_lowpass,\n    'gain':      apply_gain_jitter,\n    'bitcrush':  apply_bitcrush,\n}\n\ndef apply_random_degradations(audio: np.ndarray) -> Tuple[np.ndarray, List[str]]:\n    \"\"\"Apply a random subset of 3-6 degradations. Returns (degraded, list_of_names).\"\"\"\n    n_augs = random.randint(AUGMENTATIONS_MIN, AUGMENTATIONS_MAX)\n    chosen = random.sample(list(DEGRADATIONS.keys()), min(n_augs, len(DEGRADATIONS)))\n    for name in chosen:\n        audio = DEGRADATIONS[name](audio)\n    # Hard clip to [-1, 1] as final safety\n    audio = np.clip(audio, -1.0, 1.0)\n    return audio.astype(np.float32), chosen\n\nprint(\"[Degradation] Toolkit loaded âœ“\")\nprint(f\"  Available: {list(DEGRADATIONS.keys())}\")\nprint(f\"  Per triple: {AUGMENTATIONS_MIN}-{AUGMENTATIONS_MAX} random degradations\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bfd158e0","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 4: THE TRIPLE ENGINE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# For each sterile vocal V:\n#   1) Convolve with random bad IR (A)  â†’ source_wet = V + A\n#   2) Apply random degradations to source_wet\n#   3) Convolve same V with random studio IR (C)  â†’ target_wet = V + C\n#   4) Get CLAP embedding of target_wet\n#   5) Pack into batch\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef convolve_and_trim(vocal: np.ndarray, ir: np.ndarray) -> np.ndarray:\n    \"\"\"Convolve vocal with IR, trim to original length, peak-normalize.\"\"\"\n    wet = fftconvolve(vocal, ir, mode='full')[:len(vocal)]\n    peak = np.max(np.abs(wet))\n    if peak > 1e-6:\n        wet = wet / peak\n    return wet.astype(np.float32)q\n\n\ndef audio_to_int16(audio: np.ndarray) -> np.ndarray:\n    \"\"\"Convert float32 [-1,1] to int16 for compact storage.\"\"\"\n    return (np.clip(audio, -1, 1) * 32767).astype(np.int16)\n\n\n# â”€â”€ Pre-shuffle vocal segments for reproducibility â”€â”€\nrandom.shuffle(vocal_segments)\n\n# â”€â”€ Resume from checkpoint cursor â”€â”€\nstart_idx = ckpt.get('vocal_cursor', 0)\nbatch_id  = ckpt.get('batch_id', 0)\ntotal     = ckpt.get('triples_completed', 0)\n\n# â”€â”€ Batch accumulators â”€â”€\nbatch_sources:  List[np.ndarray] = []\nbatch_targets:  List[np.ndarray] = []\nbatch_claps:    List[np.ndarray] = []\nbatch_meta:     List[dict]       = []\n\n# â”€â”€ Load CLAP cache â”€â”€\nclap_cache_data = dict(np.load(OUTPUT / 'clap_cache.npz'))\n\nprint(f\"\\n[Engine] Starting triple generation...\")\nprint(f\"  Vocal segments: {len(vocal_segments)}\")\nprint(f\"  Bad pool: {len(bad_pool)}, Studio pool: {len(studio_pool)}\")\nprint(f\"  Resuming from index {start_idx}, batch {batch_id}\\n\")\n\nt_start = time.time()\ntriples_this_run = 0\nskipped = 0\n\nfor seg_idx in range(start_idx, len(vocal_segments)):\n    # â”€â”€ Budget check â”€â”€\n    if get_output_size_gb() > MAX_OUTPUT_GB:\n        print(f\"\\nâš   Output size limit reached ({MAX_OUTPUT_GB} GB). Stopping.\")\n        break\n\n    seg = vocal_segments[seg_idx]\n    V = seg['audio']\n\n    # â”€â”€ Pick random bad IR & studio IR â”€â”€\n    bad_ir_id    = random.choice(bad_pool)\n    studio_ir_id = random.choice(studio_pool)\n\n    ir_A = all_irs[bad_ir_id]['audio']\n    ir_C = all_irs[studio_ir_id]['audio']\n\n    # â”€â”€ Convolve â”€â”€\n    source_wet = convolve_and_trim(V, ir_A)\n    target_wet = convolve_and_trim(V, ir_C)\n\n    # â”€â”€ Random wet/dry mix on source (0.3-1.0) â”€â”€\n    wet_dry = random.uniform(0.3, 1.0)\n    source_wet = wet_dry * source_wet + (1 - wet_dry) * V\n\n    # â”€â”€ Degrade source only â”€â”€\n    source_wet, aug_names = apply_random_degradations(source_wet)\n\n    # â”€â”€ QA: reject bad triples â”€â”€\n    src_rms = np.sqrt(np.mean(source_wet ** 2))\n    tgt_rms = np.sqrt(np.mean(target_wet ** 2))\n    if src_rms < 1e-4 or tgt_rms < 1e-4:\n        skipped += 1\n        continue\n    if np.any(np.isnan(source_wet)) or np.any(np.isnan(target_wet)):\n        skipped += 1\n        continue\n\n    # â”€â”€ CLAP embedding for target â”€â”€\n    target_clap = clap_cache_data.get(studio_ir_id)\n    if target_clap is None:\n        # Fallback: compute live\n        target_clap = get_clap_audio_embedding(target_wet)\n\n    # â”€â”€ Accumulate â”€â”€\n    batch_sources.append(audio_to_int16(source_wet))\n    batch_targets.append(audio_to_int16(target_wet))\n    batch_claps.append(target_clap)\n    batch_meta.append({\n        'vocal_file':   seg['file'],\n        'dataset':      seg['dataset'],\n        'bad_ir':       bad_ir_id,\n        'studio_ir':    studio_ir_id,\n        'wet_dry':      round(wet_dry, 3),\n        'degradations': aug_names,\n    })\n\n    # â”€â”€ Flush batch when full â”€â”€\n    if len(batch_sources) >= TRIPLES_PER_BATCH:\n        batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n        np.savez(\n            batch_path,\n            source_audio  = np.stack(batch_sources),\n            target_audio  = np.stack(batch_targets),\n            target_clap   = np.stack(batch_claps),\n        )\n        # Save metadata separately (JSON, lightweight)\n        meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n        with open(meta_path, 'w') as f:\n            json.dump(batch_meta, f)\n\n        triples_this_run += len(batch_sources)\n        total += len(batch_sources)\n        batch_id += 1\n\n        # Checkpoint\n        ckpt.update({\n            'batch_id': batch_id,\n            'triples_completed': total,\n            'vocal_cursor': seg_idx + 1,\n        })\n        save_checkpoint(ckpt)\n\n        elapsed = time.time() - t_start\n        rate = triples_this_run / elapsed if elapsed > 0 else 0\n        print(f\"  Batch {batch_id-1:4d} saved | \"\n              f\"triples: {total:,} total, {triples_this_run:,} this run | \"\n              f\"{rate:.0f}/sec | \"\n              f\"{get_output_size_gb():.1f} GB used\")\n\n        batch_sources.clear()\n        batch_targets.clear()\n        batch_claps.clear()\n        batch_meta.clear()\n\n# â”€â”€ Flush remaining â”€â”€\nif batch_sources:\n    batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n    np.savez(\n        batch_path,\n        source_audio = np.stack(batch_sources),\n        target_audio = np.stack(batch_targets),\n        target_clap  = np.stack(batch_claps),\n    )\n    meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n    with open(meta_path, 'w') as f:\n        json.dump(batch_meta, f)\n\n    triples_this_run += len(batch_sources)\n    total += len(batch_sources)\n    batch_id += 1\n\n    ckpt.update({\n        'batch_id': batch_id,\n        'triples_completed': total,\n        'vocal_cursor': len(vocal_segments),\n    })\n    save_checkpoint(ckpt)\n\nelapsed = time.time() - t_start\nprint(f\"\\n[Engine] Run complete.\")\nprint(f\"  Triples this run: {triples_this_run:,}\")\nprint(f\"  Triples total:    {total:,}\")\nprint(f\"  Batches written:  {batch_id}\")\nprint(f\"  Skipped (QA):     {skipped}\")\nprint(f\"  Elapsed:          {elapsed/60:.1f} min\")\nprint(f\"  Output size:      {get_output_size_gb():.2f} GB\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dcc252cf","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MANIFEST â€” checksums, statistics, verification\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef sha256_file(path: Path) -> str:\n    h = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''):\n            h.update(chunk)\n    return h.hexdigest()\n\nmanifest = {\n    'run_number':        ckpt['run_number'],\n    'triples_total':     ckpt['triples_completed'],\n    'batches':           ckpt['batch_id'],\n    'sample_rate':       SR,\n    'clip_seconds':      CLIP_SEC,\n    'clip_samples':      CLIP_SAMPLES,\n    'clap_dim':          CLAP_DIM,\n    'bad_pool_size':     len(bad_pool),\n    'studio_pool_size':  len(studio_pool),\n    'output_size_gb':    round(get_output_size_gb(), 3),\n    'batch_checksums':   {},\n}\n\nprint(\"[Manifest] Computing checksums...\")\nfor f in sorted(BATCH_DIR.glob('batch_*.npz')):\n    manifest['batch_checksums'][f.name] = sha256_file(f)\n\nwith open(OUTPUT / 'manifest.json', 'w') as f:\n    json.dump(manifest, f, indent=2)\n\nprint(f\"[Manifest] Saved âœ“\")\nprint(f\"\\n{'='*60}\")\nprint(f\"  GENESIS DATA CURATION â€” RUN {ckpt['run_number']} COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"  Total triples:  {manifest['triples_total']:,}\")\nprint(f\"  Batches:         {manifest['batches']}\")\nprint(f\"  Output size:     {manifest['output_size_gb']:.2f} GB\")\nprint(f\"  CLAP dim:        {manifest['clap_dim']}\")\nprint(f\"{'='*60}\")\n\nif ckpt['vocal_cursor'] < len(vocal_segments):\n    remaining = len(vocal_segments) - ckpt['vocal_cursor']\n    print(f\"\\nâš   {remaining} vocal segments remaining.\")\n    print(f\"  To continue: save this output as a dataset,\")\n    print(f\"  attach it to a new notebook as '{PREV_RUN_PATH.name}',\")\n    print(f\"  and re-run this notebook.\")\nelse:\n    print(f\"\\nâœ“  All vocal segments processed. Dataset complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"70c58968","cell_type":"markdown","source":"## ðŸ”— Checkpoint Chaining (20 GB Limit)\n\nIf the output hit the size limit before processing all vocals:\n\n1. **Save this notebook's output** as a Kaggle dataset (e.g. `genesis-data-run1`)\n2. **Create a new notebook** (or re-run this one) and attach:\n   - All the same input datasets (IRs, LJSpeech, VCTK, Language Identifier)\n   - The previous output as input (update `PREV_RUN_PATH` in Cell 3)\n3. **Run all cells** â€” the checkpoint system automatically skips completed work\n\nEach run produces ~19 GB of training triples. Chain as many times as needed.\n\n### Using the Data in Training\n\n```python\n# In the training notebook, load all batches from all runs:\nimport numpy as np\nfrom pathlib import Path\n\nrun_dirs = [\n    Path('/kaggle/input/genesis-data-run1/batches'),\n    Path('/kaggle/input/genesis-data-run2/batches'),\n    # ... add more runs\n]\n\nfor run_dir in run_dirs:\n    for batch_file in sorted(run_dir.glob('batch_*.npz')):\n        data = np.load(batch_file)\n        source_audio = data['source_audio']   # (N, 240000) int16\n        target_audio = data['target_audio']   # (N, 240000) int16\n        target_clap  = data['target_clap']    # (N, CLAP_DIM) float32\n        # Convert int16 back to float32: audio = source_audio.astype(np.float32) / 32767\n        # Compute STFT on-the-fly during training for memory efficiency\n```\n","metadata":{}}]}