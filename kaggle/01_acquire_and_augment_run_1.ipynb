{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14927063,"datasetId":9551535,"databundleVersionId":15794187},{"sourceType":"datasetVersion","sourceId":7085845,"datasetId":4060468,"databundleVersionId":7173592},{"sourceType":"datasetVersion","sourceId":101413,"datasetId":53291,"databundleVersionId":103953},{"sourceType":"datasetVersion","sourceId":4588404,"datasetId":2675000,"databundleVersionId":4649793}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"02d93efa","cell_type":"markdown","source":"# ğŸ§¬ Genesis IR Head â€” Data Curation Pipeline\n\n**Notebook 01**: Acquire â†’ Sterilize â†’ Augment â†’ Target\n\nThis notebook produces training triples for Genesis's IR Synthesis Head:\n\n| Tensor | Format | Description |\n|--------|--------|-------------|\n| `source_wet` | int16 waveform | Sterile vocal + bad-room IR + random degradations |\n| `target_wet` | int16 waveform | Same sterile vocal + studio IR (ground truth) |\n| `target_clap` | float32 vector | CLAP embedding of the target-wet audio |\n\n**Checkpoint chaining**: Output is capped at ~19 GB. A `checkpoint.json` tracks\nprogress so the next run (using this output as input) continues where we left off.\n","metadata":{}},{"id":"1cdc49b0","cell_type":"code","source":"!pip install -q noisereduce pyloudnorm soundfile librosa transformers torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T20:18:10.478000Z","iopub.execute_input":"2026-02-23T20:18:10.478469Z","iopub.status.idle":"2026-02-23T20:18:16.551880Z","shell.execute_reply.started":"2026-02-23T20:18:10.478437Z","shell.execute_reply":"2026-02-23T20:18:16.550592Z"}},"outputs":[],"execution_count":1},{"id":"86ffd3a4","cell_type":"code","source":"import os, json, hashlib, time, random, warnings, struct\nimport shutil\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple\nfrom collections import defaultdict\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nfrom scipy.signal import fftconvolve\nimport noisereduce as nr\n\nwarnings.filterwarnings('ignore')\nrandom.seed(42)\nnp.random.seed(42)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EXTRACT LANGUAGE IDENTIFIER DATASET\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#LANGID_TAR = Path('/kaggle/input/notebooks/itorousa/language-identifier-dataset-eng-lat-dutch/faurge_dataset_archive.tar')\n#LANGID_EXTRACTED = Path('/kaggle/working/langid_extracted')\n\n#if LANGID_TAR.exists() and not LANGID_EXTRACTED.exists():\n    #print(f\"ğŸŸ¢ Extracting English dataset archive...\")\n    #shutil.unpack_archive(str(LANGID_TAR), str(LANGID_EXTRACTED))\n    #print(\"ğŸŸ¢ Extraction complete.\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# KAGGLE INPUT PATHS  (datasets attached to this notebook)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPATHS = {\n    'irs':       Path('/kaggle/input/datasets/itorousa/impulse-responses'),\n    'ljspeech':  Path('/kaggle/input/datasets/dromosys/ljspeech/'),\n    'vctk':      Path('/kaggle/input/datasets/kynthesis/vctk-corpus/VCTK-Corpus/wav48'),\n    'langid_en': Path('/kaggle/input/datasets/shrivatssudhir/language-identifier/english/clips'),\n}\n\n# If chaining from a previous run, attach its output as input here:\nPREV_RUN_PATH = Path('/kaggle/input/genesis-data-run1')  # adjust per run\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# OUTPUT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nOUTPUT       = Path('/kaggle/working')\nBATCH_DIR    = OUTPUT / 'batches'\nACQUIRED_DIR = OUTPUT / 'acquired_irs'\nCLAP_DIR     = OUTPUT / 'clap_model'\n\nfor d in [BATCH_DIR, ACQUIRED_DIR, CLAP_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# AUDIO PARAMETERS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSR            = 48000\nCLIP_SEC      = 5.0\nCLIP_SAMPLES  = int(SR * CLIP_SEC)    # 240,000 samples\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# BUDGET & BATCHING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTRIPLES_PER_BATCH  = 500\nMAX_OUTPUT_GB      = 19.0             # safety margin under 20 GB cap\nAUGMENTATIONS_MIN  = 3\nAUGMENTATIONS_MAX  = 6\n\nprint(f\"SR={SR}, CLIP_SEC={CLIP_SEC}, CLIP_SAMPLES={CLIP_SAMPLES}\")\nprint(f\"Output budget: {MAX_OUTPUT_GB} GB, batch size: {TRIPLES_PER_BATCH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T20:19:51.542448Z","iopub.execute_input":"2026-02-23T20:19:51.542768Z","iopub.status.idle":"2026-02-23T20:19:51.557643Z","shell.execute_reply.started":"2026-02-23T20:19:51.542746Z","shell.execute_reply":"2026-02-23T20:19:51.556239Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3449409613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBATCH_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACQUIRED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLAP_DIR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \"\"\"\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/kaggle/working/batches'"],"ename":"OSError","evalue":"[Errno 28] No space left on device: '/kaggle/working/batches'","output_type":"error"}],"execution_count":5},{"id":"322d8a4f","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CHECKPOINT  â€” resume across runs\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nCHECKPOINT_PATH = OUTPUT / 'checkpoint.json'\n\ndef load_checkpoint() -> dict:\n    \"\"\"Load from previous run's output (if chained) or current working dir.\"\"\"\n    # Check previous run first\n    prev_ckpt = PREV_RUN_PATH / 'checkpoint.json'\n    if prev_ckpt.exists():\n        with open(prev_ckpt) as f:\n            ckpt = json.load(f)\n        ckpt['run_number'] += 1\n        print(f\"[Checkpoint] Resuming from previous run: {ckpt['triples_completed']} triples done\")\n        return ckpt\n\n    # Check current working dir\n    if CHECKPOINT_PATH.exists():\n        with open(CHECKPOINT_PATH) as f:\n            return json.load(f)\n\n    # Fresh start\n    return {\n        'batch_id': 0,\n        'triples_completed': 0,\n        'vocal_cursor': 0,        # index into shuffled vocal list\n        'run_number': 1,\n    }\n\ndef save_checkpoint(ckpt: dict):\n    with open(CHECKPOINT_PATH, 'w') as f:\n        json.dump(ckpt, f, indent=2)\n\ndef get_output_size_gb() -> float:\n    total = sum(f.stat().st_size for f in OUTPUT.rglob('*') if f.is_file())\n    return total / (1024 ** 3)\n\nckpt = load_checkpoint()\nprint(f\"[Checkpoint] Run #{ckpt['run_number']}, \"\n      f\"{ckpt['triples_completed']} triples completed so far, \"\n      f\"starting at vocal cursor {ckpt['vocal_cursor']}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"50c8c54d","cell_type":"code","source":"import urllib.request, zipfile\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 1: ACQUIRE â€” download external IR collections\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef download_if_needed(url: str, dest_dir: Path, name: str):\n    marker = dest_dir / f'.{name}_done'\n    if marker.exists():\n        print(f\"  [Acquire] {name} already downloaded\")\n        return\n    print(f\"  [Acquire] Downloading {name}...\")\n    zip_path = dest_dir / f'{name}.zip'\n    urllib.request.urlretrieve(url, str(zip_path))\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        zf.extractall(dest_dir / name)\n    zip_path.unlink()\n    marker.touch()\n    print(f\"  [Acquire] {name} âœ“\")\n\n# MIT Impulse Response Survey â€” 271 real-world room IRs (CC-BY 4.0)\ndownload_if_needed(\n    'https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip',\n    ACQUIRED_DIR, 'mit_irs'\n)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Load & normalize every IR to 48 kHz mono\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\ndef load_and_normalize_ir(filepath: Path, target_sr: int = SR) -> Optional[np.ndarray]:\n    \"\"\"Load an IR file, force mono / target SR, peak-normalize.\"\"\"\n    try:\n        audio, _ = librosa.load(str(filepath), sr=target_sr, mono=True)\n        if len(audio) < 64:        # degenerate\n            return None\n        peak = np.max(np.abs(audio))\n        if peak > 1e-6:\n            audio = audio / peak\n        return audio.astype(np.float32)\n    except Exception:\n        return None\n\nprint(\"\\n[Acquire] Loading all impulse responses...\")\nall_irs: Dict[str, dict] = {}\n\n# 1) User's IRs  (supports .irs and .wav)\nfor ext in ('*.irs', '*.wav'):\n    for f in sorted(PATHS['irs'].glob(ext)):\n        ir = load_and_normalize_ir(f)\n        if ir is not None:\n            all_irs[f'user_{f.stem}'] = {'audio': ir, 'source': 'user'}\n\n# 2) MIT IRs\nmit_dir = ACQUIRED_DIR / 'mit_irs'\nif mit_dir.exists():\n    for f in sorted(mit_dir.rglob('*.wav')):\n        ir = load_and_normalize_ir(f)\n        if ir is not None:\n            all_irs[f'mit_{f.stem}'] = {'audio': ir, 'source': 'mit'}\n\nprint(f\"[Acquire] Loaded {len(all_irs)} IRs \"\n      f\"(user: {sum(1 for v in all_irs.values() if v['source']=='user')}, \"\n      f\"MIT: {sum(1 for v in all_irs.values() if v['source']=='mit')})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T20:19:26.687568Z","iopub.execute_input":"2026-02-23T20:19:26.688330Z","iopub.status.idle":"2026-02-23T20:19:26.703761Z","shell.execute_reply.started":"2026-02-23T20:19:26.688298Z","shell.execute_reply":"2026-02-23T20:19:26.702268Z"}},"outputs":[{"name":"stdout","text":"  [Acquire] mit_irs already downloaded\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1155850157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_normalize_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;34m\"\"\"Load an IR file, force mono / target SR, peak-normalize.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SR' is not defined"],"ename":"NameError","evalue":"name 'SR' is not defined","output_type":"error"}],"execution_count":4},{"id":"c30ed307","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Classify IRs into \"bad\" (room/degraded) and \"studio\" (clean target)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nimport json\nimport numpy as np\nfrom typing import List\n\nCATALOGUE_PATH = OUTPUT_DIR / 'ir_catalogue.json'\n\nir_catalogue = {}\nbad_pool:    List[str] = []   # contamination IRs (rooms, halls, degraded)\nstudio_pool: List[str] = []   # target IRs (mastering, clarity, studio)\n\nif CATALOGUE_PATH.exists():\n    print(\"ğŸŸ¢ Found existing ir_catalogue.json. Skipping classification compute.\")\n    with open(CATALOGUE_PATH, 'r') as f:\n        ir_catalogue = json.load(f)\n        \n    # Rebuild the pools from the loaded catalogue\n    for ir_id, feats in ir_catalogue.items():\n        if feats['source'] == 'mit':\n            bad_pool.append(ir_id)\n        elif feats['rt60'] > 0.25 or feats['c50'] < 8:\n            bad_pool.append(ir_id)\n        else:\n            studio_pool.append(ir_id)\n            \n    # Safety: ensure both pools are adequate (using the catalogue data)\n    if len(studio_pool) < 20:\n        user_ids = sorted(\n            [k for k, v in ir_catalogue.items() if v['source'] == 'user'],\n            key=lambda k: ir_catalogue[k]['c50'], reverse=True\n        )\n        studio_pool = user_ids[:max(50, len(user_ids) // 3)]\n        bad_pool = [k for k in ir_catalogue if k not in studio_pool]\n        \n    print(f\"ğŸŸ¢ Loaded {len(ir_catalogue)} IRs from disk.\")\n    print(f\"ğŸŸ¢ Bad pool: {len(bad_pool)} IRs | Studio pool: {len(studio_pool)} IRs\")\n\nelse:\n    print(\"ğŸŸ¢ Generating IR classification and features from scratch...\")\n    \n    def compute_ir_features(ir_audio: np.ndarray, sr: int = SR) -> dict:\n        \"\"\"Compute RT60 estimate, spectral centroid, and clarity (C50).\"\"\"\n        energy = ir_audio ** 2\n        cumsum = np.cumsum(energy[::-1])[::-1]\n\n        # RT60 â€” time for energy to decay 60 dB\n        rt60 = len(ir_audio) / sr\n        if cumsum[0] > 1e-10:\n            decay_db = 10 * np.log10(cumsum / cumsum[0] + 1e-12)\n            idx = np.where(decay_db < -60)[0]\n            if len(idx) > 0:\n                rt60 = idx[0] / sr\n\n        # Spectral centroid\n        S = np.abs(np.fft.rfft(ir_audio))\n        freqs = np.fft.rfftfreq(len(ir_audio), 1 / sr)\n        centroid = float(np.sum(freqs * S) / (np.sum(S) + 1e-10))\n\n        # Clarity C50 â€” early-to-late energy ratio at 50 ms boundary\n        split = int(0.05 * sr)\n        early = np.sum(ir_audio[:split] ** 2) + 1e-12\n        late  = np.sum(ir_audio[split:] ** 2) + 1e-12\n        c50   = float(10 * np.log10(early / late))\n\n        return {\n            'rt60': round(rt60, 4),\n            'centroid': round(centroid, 1),\n            'c50': round(c50, 2),\n            'length_sec': round(len(ir_audio) / sr, 4),\n        }\n\n    for ir_id, ir_data in all_irs.items():\n        feats = compute_ir_features(ir_data['audio'])\n        ir_data['features'] = feats\n        ir_catalogue[ir_id] = {'source': ir_data['source'], **feats}\n\n        # Heuristic:\n        #   MIT IRs â†’ always bad pool (real rooms)\n        #   User IRs with long RT60 or low clarity â†’ bad pool\n        #   User IRs with short RT60 and high clarity â†’ studio pool\n        if ir_data['source'] == 'mit':\n            bad_pool.append(ir_id)\n        elif feats['rt60'] > 0.25 or feats['c50'] < 8:\n            bad_pool.append(ir_id)\n        else:\n            studio_pool.append(ir_id)\n\n    # Safety: ensure both pools are adequate\n    if len(studio_pool) < 20:\n        # Sort user IRs by clarity, take top third as studio\n        user_ids = sorted(\n            [k for k, v in all_irs.items() if v['source'] == 'user'],\n            key=lambda k: all_irs[k]['features']['c50'], reverse=True\n        )\n        studio_pool = user_ids[:max(50, len(user_ids) // 3)]\n        bad_pool = [k for k in all_irs if k not in studio_pool]\n\n    print(f\"ğŸŸ¢ Bad pool: {len(bad_pool)} IRs\")\n    print(f\"ğŸŸ¢ Studio pool: {len(studio_pool)} IRs\")\n\n    # Save catalogue\n    with open(CATALOGUE_PATH, 'w') as f:\n        json.dump(ir_catalogue, f, indent=2)\n    print(\"ğŸŸ¢ ir_catalogue.json saved âœ“\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"b2c4c6db","cell_type":"code","source":"import torch\nimport numpy as np\nfrom typing import Dict\nfrom scipy.signal import fftconvolve\nfrom transformers import ClapModel, ClapProcessor\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 2: Freeze CLAP model & cache studio-pool embeddings\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCLAP_MODEL_ID = \"laion/larger_clap_music_and_speech\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸŸ¢ [CLAP] Device: {device}\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Load or Download CLAP Model\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif CLAP_DIR.exists():\n    print(\"ğŸŸ¢ [CLAP] Found existing frozen CLAP model. Loading from local cache...\")\n    clap_processor = ClapProcessor.from_pretrained(CLAP_DIR)\n    clap_model = ClapModel.from_pretrained(CLAP_DIR).to(device).eval()\nelse:\n    print(\"ğŸŸ¢ [CLAP] Downloading model from Hugging Face...\")\n    clap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)\n    clap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(device).eval()\n    \n    # Save frozen model for downstream notebooks\n    CLAP_DIR.mkdir(parents=True, exist_ok=True)\n    clap_model.save_pretrained(CLAP_DIR)\n    clap_processor.save_pretrained(CLAP_DIR)\n    print(f\"ğŸŸ¢ [CLAP] Frozen model saved to {CLAP_DIR}\")\n\nCLAP_DIM = clap_model.config.projection_dim\nprint(f\"ğŸŸ¢ [CLAP] Loaded â€” embedding dim = {CLAP_DIM}\")\n\ndef get_clap_audio_embedding(audio: np.ndarray, sr: int = SR) -> np.ndarray:\n    \"\"\"Encode audio through CLAP's audio tower. Returns (CLAP_DIM,) float32.\"\"\"\n    inputs = clap_processor(\n        audio=audio, sampling_rate=sr, return_tensors=\"pt\"\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = clap_model.get_audio_features(**inputs)\n        # Extract the pooled embedding tensor from the output object\n        emb = outputs.pooler_output\n    return emb.cpu().numpy().flatten().astype(np.float32)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Pre-compute CLAP embeddings for every studio-pool IR.\n# We convolve each IR with 3 seconds of white noise so CLAP has\n# a richer scene to analyse (raw IRs are too short / sparse).\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nCLAP_CACHE_PATH = OUTPUT / 'clap_cache.npz'\n\nif CLAP_CACHE_PATH.exists():\n    print(\"ğŸŸ¢ [CLAP] Found existing clap_cache.npz. Skipping embedding compute.\")\n    clap_cache = dict(np.load(CLAP_CACHE_PATH))\n    print(f\"ğŸŸ¢ [CLAP] Loaded {len(clap_cache)} embeddings from disk âœ“  (dim={CLAP_DIM})\")\nelse:\n    print(f\"\\nğŸŸ¢ [CLAP] Pre-computing embeddings for {len(studio_pool)} studio IRs...\")\n    ref_noise = np.random.randn(SR * 3).astype(np.float32) * 0.1\n\n    clap_cache: Dict[str, np.ndarray] = {}\n    for i, ir_id in enumerate(studio_pool):\n        ir_audio = all_irs[ir_id]['audio']\n        scene = fftconvolve(ref_noise, ir_audio, mode='full')[:SR * 3]\n        scene = scene / (np.max(np.abs(scene)) + 1e-8)\n        clap_cache[ir_id] = get_clap_audio_embedding(scene)\n        if (i + 1) % 50 == 0:\n            print(f\"  {i+1}/{len(studio_pool)}\")\n\n    np.savez(CLAP_CACHE_PATH, **clap_cache)\n    print(f\"ğŸŸ¢ [CLAP] Cached {len(clap_cache)} embeddings âœ“  (dim={CLAP_DIM})\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"cd282e26","cell_type":"code","source":"import json\nimport pickle\nimport random\nfrom pathlib import Path\nfrom collections import defaultdict\nimport numpy as np\nimport pyloudnorm as pyln\nimport librosa\nimport noisereduce as nr\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 3: STERILIZE â€” noise reduce, trim, segment, normalize\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nmeter = pyln.Meter(SR)\n\ndef discover_audio_files() -> List[Tuple[Path, str]]:\n    \"\"\"Collect all vocal files from all datasets. Returns (path, dataset_tag).\"\"\"\n    files = []\n    # LJSpeech\n    for f in sorted(PATHS['ljspeech'].rglob('*.wav')):\n        files.append((f, 'ljspeech'))\n    # VCTK â€” subfolder per speaker\n    for f in sorted(PATHS['vctk'].rglob('*.wav')):\n        files.append((f, f'vctk_{f.parent.name}'))\n    # Language Identifier â€” english clips\n    for ext in ('*.wav', '*.mp3', '*.ogg'):\n        for f in sorted(PATHS['langid_en'].rglob(ext)):\n            files.append((f, 'langid_en'))\n    return files\n\n\ndef sterilize_and_segment(filepath: Path, tag: str) -> List[dict]:\n    \"\"\"\n    Load â†’ noise-reduce â†’ trim silence â†’ segment into CLIP_SEC windows.\n    Returns list of dicts with 'audio' (float32) and metadata.\n    \"\"\"\n    try:\n        audio, sr_orig = librosa.load(str(filepath), sr=SR, mono=True)\n    except Exception:\n        return []\n\n    if len(audio) < SR * 1.0:   # skip clips shorter than 1 second\n        return []\n\n    # â”€â”€ Spectral noise reduction (simulates Ursula's output) â”€â”€\n    audio = nr.reduce_noise(y=audio, sr=SR, stationary=True, prop_decrease=0.85)\n\n    # â”€â”€ Trim silence â”€â”€\n    audio, _ = librosa.effects.trim(audio, top_db=40)\n    if len(audio) < SR * 1.5:\n        return []\n\n    # â”€â”€ Normalize loudness to -23 LUFS â”€â”€\n    try:\n        loudness = meter.integrated_loudness(audio)\n        if loudness > -70:  # not silence\n            audio = pyln.normalize.loudness(audio, loudness, -23.0)\n    except Exception:\n        pass\n\n    # â”€â”€ Segment into CLIP_SEC windows â”€â”€\n    segments = []\n    for start in range(0, len(audio) - SR, CLIP_SAMPLES):\n        chunk = audio[start : start + CLIP_SAMPLES]\n        if len(chunk) < CLIP_SAMPLES:\n            chunk = np.pad(chunk, (0, CLIP_SAMPLES - len(chunk)))\n\n        # Skip near-silent segments\n        rms = np.sqrt(np.mean(chunk ** 2))\n        if rms < 1e-4:\n            continue\n\n        segments.append({\n            'audio':   chunk.astype(np.float32),\n            'file':    filepath.name,\n            'dataset': tag,\n        })\n    return segments\n\n\n# â”€â”€ Discover & sterilize â”€â”€\nprint(\"ğŸŸ¢ [Sterilize] Discovering audio files...\")\nall_audio_files = discover_audio_files()\n\n# Sort first to guarantee deterministic order, then seed the shuffle.\n# This ensures Kaggle processes the exact same files in the same order if the kernel restarts.\nall_audio_files.sort(key=lambda x: str(x[0]))\nrandom.Random(42).shuffle(all_audio_files)\n\nprint(f\"ğŸŸ¢ [Sterilize] Found {len(all_audio_files)} source files\")\n\n# â”€â”€ Checkpoint & VRAM/RAM Setup â”€â”€\nSTERILIZE_CHUNK = 500   # files processed at a time before flushing to disk\nOUTPUT_DIR = Path('/kaggle/working/sterilized_batches')\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nSTATE_FILE = OUTPUT_DIR / 'sterilize_state.json'\n\ncursor_start = 0\nif STATE_FILE.exists():\n    with open(STATE_FILE, 'r') as f:\n        cursor_start = json.load(f).get('cursor', 0)\n\nvocal_segments: List[dict] = []\n\nprint(f\"ğŸŸ¢ [Sterilize] Processing from file index {cursor_start}...\")\n\nfor i in range(cursor_start, len(all_audio_files)):\n    fpath, tag = all_audio_files[i]\n    \n    if i % 200 == 0 and i > cursor_start:\n        print(f\"ğŸŸ¢ Processed {i}/{len(all_audio_files)} files â†’ \"\n              f\"{len(vocal_segments)} segments in current RAM buffer\")\n        \n    segs = sterilize_and_segment(fpath, tag)\n    vocal_segments.extend(segs)\n\n    # Save to disk every STERILIZE_CHUNK files, or on the absolute last file\n    if (i + 1) % STERILIZE_CHUNK == 0 or (i + 1) == len(all_audio_files):\n        batch_index = (i + 1) // STERILIZE_CHUNK\n        batch_path = OUTPUT_DIR / f\"sterilized_batch_{batch_index:04d}.pkl\"\n        \n        with open(batch_path, 'wb') as f:\n            pickle.dump(vocal_segments, f)\n            \n        with open(STATE_FILE, 'w') as f:\n            json.dump({'cursor': i + 1}, f)\n            \n        print(f\"ğŸ’¾ Saved {len(vocal_segments)} segments to {batch_path.name}. RAM cleared.\")\n        vocal_segments.clear() # Dump RAM to prevent Kaggle OOM crash\n\n# â”€â”€ Aggregate counts from all saved batches for the final printout â”€â”€\nprint(\"\\nğŸŸ¢ [Sterilize] Process complete. Compiling dataset breakdown from disk...\")\nds_counts = defaultdict(int)\ntotal_segments = 0\n\nfor batch_file in sorted(OUTPUT_DIR.glob('*.pkl')):\n    with open(batch_file, 'rb') as f:\n        batch_data = pickle.load(f)\n        total_segments += len(batch_data)\n        for s in batch_data:\n            ds_counts[s['dataset'].split('_')[0]] += 1\n\nprint(f\"ğŸŸ¢ [Sterilize] Total sterile segments across all batches: {total_segments}\")\nfor ds, cnt in sorted(ds_counts.items()):\n    print(f\"  {ds}: {cnt}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T20:19:17.366938Z","iopub.execute_input":"2026-02-23T20:19:17.367274Z","iopub.status.idle":"2026-02-23T20:19:17.391407Z","shell.execute_reply.started":"2026-02-23T20:19:17.367248Z","shell.execute_reply":"2026-02-23T20:19:17.390044Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2925582512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmeter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdiscover_audio_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'SR' is not defined"],"ename":"NameError","evalue":"name 'SR' is not defined","output_type":"error"}],"execution_count":3},{"id":"4bd7e817","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DEGRADATION TOOLKIT â€” applied to source_wet ONLY\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef add_noise(audio: np.ndarray, noise_type: str, snr_db: float) -> np.ndarray:\n    \"\"\"Mix coloured noise at the target SNR.\"\"\"\n    n = len(audio)\n    if noise_type == 'white':\n        noise = np.random.randn(n)\n    elif noise_type == 'pink':\n        # Pink noise via spectral shaping\n        freqs = np.fft.rfftfreq(n, 1 / SR)\n        freqs[0] = 1\n        S = 1.0 / np.sqrt(freqs)\n        noise = np.fft.irfft(S * np.exp(2j * np.pi * np.random.rand(len(S))))[:n]\n    elif noise_type == 'brown':\n        noise = np.cumsum(np.random.randn(n))\n        noise -= np.mean(noise)\n    elif noise_type == 'hvac':\n        # Band-limited broadband 100-1000 Hz\n        noise = np.random.randn(n)\n        from scipy.signal import butter, sosfilt\n        sos = butter(4, [100, 1000], btype='band', fs=SR, output='sos')\n        noise = sosfilt(sos, noise)\n    elif noise_type == 'hum':\n        # 50 Hz fundamental + harmonics\n        t = np.arange(n) / SR\n        base_freq = random.choice([50, 60])\n        noise = np.zeros(n)\n        for h in range(1, 6):\n            amp = 1.0 / h\n            noise += amp * np.sin(2 * np.pi * base_freq * h * t + random.uniform(0, 2*np.pi))\n    else:\n        noise = np.random.randn(n)\n\n    # Scale noise to target SNR\n    sig_power = np.mean(audio ** 2) + 1e-12\n    noise_power = np.mean(noise ** 2) + 1e-12\n    target_noise_power = sig_power / (10 ** (snr_db / 10))\n    noise = noise * np.sqrt(target_noise_power / noise_power)\n    return audio + noise.astype(np.float32)\n\n\ndef apply_eq(audio: np.ndarray) -> np.ndarray:\n    \"\"\"Random 3-band parametric EQ (simulates mic coloration).\"\"\"\n    from scipy.signal import butter, sosfilt\n    bands = [\n        (80, 300),     # low\n        (300, 3000),   # mid\n        (3000, 12000), # high\n    ]\n    for lo, hi in bands:\n        gain_db = random.uniform(-6, 6)\n        if abs(gain_db) < 1:\n            continue\n        try:\n            sos = butter(2, [lo, hi], btype='band', fs=SR, output='sos')\n            band_sig = sosfilt(sos, audio)\n            gain_lin = 10 ** (gain_db / 20)\n            audio = audio + band_sig * (gain_lin - 1)\n        except Exception:\n            pass\n    return audio.astype(np.float32)\n\n\ndef apply_highpass(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    cutoff = random.uniform(60, 300)\n    sos = butter(4, cutoff, btype='high', fs=SR, output='sos')\n    return sosfilt(sos, audio).astype(np.float32)\n\n\ndef apply_lowpass(audio: np.ndarray) -> np.ndarray:\n    from scipy.signal import butter, sosfilt\n    cutoff = random.uniform(3000, 16000)\n    sos = butter(4, cutoff, btype='low', fs=SR, output='sos')\n    return sosfilt(sos, audio).astype(np.float32)\n\n\ndef apply_gain_jitter(audio: np.ndarray) -> np.ndarray:\n    gain_db = random.uniform(-6, 6)\n    return audio * (10 ** (gain_db / 20))\n\n\ndef apply_bitcrush(audio: np.ndarray) -> np.ndarray:\n    bits = random.randint(8, 16)\n    levels = 2 ** bits\n    return (np.round(audio * levels) / levels).astype(np.float32)\n\n\n# Registry of all degradation functions\nDEGRADATIONS = {\n    'noise':     lambda a: add_noise(a, random.choice(['white','pink','brown','hvac','hum']),\n                                     random.uniform(5, 40)),\n    'eq':        apply_eq,\n    'highpass':  apply_highpass,\n    'lowpass':   apply_lowpass,\n    'gain':      apply_gain_jitter,\n    'bitcrush':  apply_bitcrush,\n}\n\ndef apply_random_degradations(audio: np.ndarray) -> Tuple[np.ndarray, List[str]]:\n    \"\"\"Apply a random subset of 3-6 degradations. Returns (degraded, list_of_names).\"\"\"\n    n_augs = random.randint(AUGMENTATIONS_MIN, AUGMENTATIONS_MAX)\n    chosen = random.sample(list(DEGRADATIONS.keys()), min(n_augs, len(DEGRADATIONS)))\n    for name in chosen:\n        audio = DEGRADATIONS[name](audio)\n    # Hard clip to [-1, 1] as final safety\n    audio = np.clip(audio, -1.0, 1.0)\n    return audio.astype(np.float32), chosen\n\nprint(\"[Degradation] Toolkit loaded âœ“\")\nprint(f\"  Available: {list(DEGRADATIONS.keys())}\")\nprint(f\"  Per triple: {AUGMENTATIONS_MIN}-{AUGMENTATIONS_MAX} random degradations\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bfd158e0","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STAGE 4: THE TRIPLE ENGINE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# For each sterile vocal V:\n#   1) Convolve with random bad IR (A)  â†’ source_wet = V + A\n#   2) Apply random degradations to source_wet\n#   3) Convolve same V with random studio IR (C)  â†’ target_wet = V + C\n#   4) Get CLAP embedding of target_wet\n#   5) Pack into batch\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef convolve_and_trim(vocal: np.ndarray, ir: np.ndarray) -> np.ndarray:\n    \"\"\"Convolve vocal with IR, trim to original length, peak-normalize.\"\"\"\n    wet = fftconvolve(vocal, ir, mode='full')[:len(vocal)]\n    peak = np.max(np.abs(wet))\n    if peak > 1e-6:\n        wet = wet / peak\n    return wet.astype(np.float32)q\n\n\ndef audio_to_int16(audio: np.ndarray) -> np.ndarray:\n    \"\"\"Convert float32 [-1,1] to int16 for compact storage.\"\"\"\n    return (np.clip(audio, -1, 1) * 32767).astype(np.int16)\n\n\n# â”€â”€ Pre-shuffle vocal segments for reproducibility â”€â”€\nrandom.shuffle(vocal_segments)\n\n# â”€â”€ Resume from checkpoint cursor â”€â”€\nstart_idx = ckpt.get('vocal_cursor', 0)\nbatch_id  = ckpt.get('batch_id', 0)\ntotal     = ckpt.get('triples_completed', 0)\n\n# â”€â”€ Batch accumulators â”€â”€\nbatch_sources:  List[np.ndarray] = []\nbatch_targets:  List[np.ndarray] = []\nbatch_claps:    List[np.ndarray] = []\nbatch_meta:     List[dict]       = []\n\n# â”€â”€ Load CLAP cache â”€â”€\nclap_cache_data = dict(np.load(OUTPUT / 'clap_cache.npz'))\n\nprint(f\"\\n[Engine] Starting triple generation...\")\nprint(f\"  Vocal segments: {len(vocal_segments)}\")\nprint(f\"  Bad pool: {len(bad_pool)}, Studio pool: {len(studio_pool)}\")\nprint(f\"  Resuming from index {start_idx}, batch {batch_id}\\n\")\n\nt_start = time.time()\ntriples_this_run = 0\nskipped = 0\n\nfor seg_idx in range(start_idx, len(vocal_segments)):\n    # â”€â”€ Budget check â”€â”€\n    if get_output_size_gb() > MAX_OUTPUT_GB:\n        print(f\"\\nâš   Output size limit reached ({MAX_OUTPUT_GB} GB). Stopping.\")\n        break\n\n    seg = vocal_segments[seg_idx]\n    V = seg['audio']\n\n    # â”€â”€ Pick random bad IR & studio IR â”€â”€\n    bad_ir_id    = random.choice(bad_pool)\n    studio_ir_id = random.choice(studio_pool)\n\n    ir_A = all_irs[bad_ir_id]['audio']\n    ir_C = all_irs[studio_ir_id]['audio']\n\n    # â”€â”€ Convolve â”€â”€\n    source_wet = convolve_and_trim(V, ir_A)\n    target_wet = convolve_and_trim(V, ir_C)\n\n    # â”€â”€ Random wet/dry mix on source (0.3-1.0) â”€â”€\n    wet_dry = random.uniform(0.3, 1.0)\n    source_wet = wet_dry * source_wet + (1 - wet_dry) * V\n\n    # â”€â”€ Degrade source only â”€â”€\n    source_wet, aug_names = apply_random_degradations(source_wet)\n\n    # â”€â”€ QA: reject bad triples â”€â”€\n    src_rms = np.sqrt(np.mean(source_wet ** 2))\n    tgt_rms = np.sqrt(np.mean(target_wet ** 2))\n    if src_rms < 1e-4 or tgt_rms < 1e-4:\n        skipped += 1\n        continue\n    if np.any(np.isnan(source_wet)) or np.any(np.isnan(target_wet)):\n        skipped += 1\n        continue\n\n    # â”€â”€ CLAP embedding for target â”€â”€\n    target_clap = clap_cache_data.get(studio_ir_id)\n    if target_clap is None:\n        # Fallback: compute live\n        target_clap = get_clap_audio_embedding(target_wet)\n\n    # â”€â”€ Accumulate â”€â”€\n    batch_sources.append(audio_to_int16(source_wet))\n    batch_targets.append(audio_to_int16(target_wet))\n    batch_claps.append(target_clap)\n    batch_meta.append({\n        'vocal_file':   seg['file'],\n        'dataset':      seg['dataset'],\n        'bad_ir':       bad_ir_id,\n        'studio_ir':    studio_ir_id,\n        'wet_dry':      round(wet_dry, 3),\n        'degradations': aug_names,\n    })\n\n    # â”€â”€ Flush batch when full â”€â”€\n    if len(batch_sources) >= TRIPLES_PER_BATCH:\n        batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n        np.savez(\n            batch_path,\n            source_audio  = np.stack(batch_sources),\n            target_audio  = np.stack(batch_targets),\n            target_clap   = np.stack(batch_claps),\n        )\n        # Save metadata separately (JSON, lightweight)\n        meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n        with open(meta_path, 'w') as f:\n            json.dump(batch_meta, f)\n\n        triples_this_run += len(batch_sources)\n        total += len(batch_sources)\n        batch_id += 1\n\n        # Checkpoint\n        ckpt.update({\n            'batch_id': batch_id,\n            'triples_completed': total,\n            'vocal_cursor': seg_idx + 1,\n        })\n        save_checkpoint(ckpt)\n\n        elapsed = time.time() - t_start\n        rate = triples_this_run / elapsed if elapsed > 0 else 0\n        print(f\"  Batch {batch_id-1:4d} saved | \"\n              f\"triples: {total:,} total, {triples_this_run:,} this run | \"\n              f\"{rate:.0f}/sec | \"\n              f\"{get_output_size_gb():.1f} GB used\")\n\n        batch_sources.clear()\n        batch_targets.clear()\n        batch_claps.clear()\n        batch_meta.clear()\n\n# â”€â”€ Flush remaining â”€â”€\nif batch_sources:\n    batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n    np.savez(\n        batch_path,\n        source_audio = np.stack(batch_sources),\n        target_audio = np.stack(batch_targets),\n        target_clap  = np.stack(batch_claps),\n    )\n    meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n    with open(meta_path, 'w') as f:\n        json.dump(batch_meta, f)\n\n    triples_this_run += len(batch_sources)\n    total += len(batch_sources)\n    batch_id += 1\n\n    ckpt.update({\n        'batch_id': batch_id,\n        'triples_completed': total,\n        'vocal_cursor': len(vocal_segments),\n    })\n    save_checkpoint(ckpt)\n\nelapsed = time.time() - t_start\nprint(f\"\\n[Engine] Run complete.\")\nprint(f\"  Triples this run: {triples_this_run:,}\")\nprint(f\"  Triples total:    {total:,}\")\nprint(f\"  Batches written:  {batch_id}\")\nprint(f\"  Skipped (QA):     {skipped}\")\nprint(f\"  Elapsed:          {elapsed/60:.1f} min\")\nprint(f\"  Output size:      {get_output_size_gb():.2f} GB\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dcc252cf","cell_type":"code","source":"# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MANIFEST â€” checksums, statistics, verification\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef sha256_file(path: Path) -> str:\n    h = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''):\n            h.update(chunk)\n    return h.hexdigest()\n\nmanifest = {\n    'run_number':        ckpt['run_number'],\n    'triples_total':     ckpt['triples_completed'],\n    'batches':           ckpt['batch_id'],\n    'sample_rate':       SR,\n    'clip_seconds':      CLIP_SEC,\n    'clip_samples':      CLIP_SAMPLES,\n    'clap_dim':          CLAP_DIM,\n    'bad_pool_size':     len(bad_pool),\n    'studio_pool_size':  len(studio_pool),\n    'output_size_gb':    round(get_output_size_gb(), 3),\n    'batch_checksums':   {},\n}\n\nprint(\"[Manifest] Computing checksums...\")\nfor f in sorted(BATCH_DIR.glob('batch_*.npz')):\n    manifest['batch_checksums'][f.name] = sha256_file(f)\n\nwith open(OUTPUT / 'manifest.json', 'w') as f:\n    json.dump(manifest, f, indent=2)\n\nprint(f\"[Manifest] Saved âœ“\")\nprint(f\"\\n{'='*60}\")\nprint(f\"  GENESIS DATA CURATION â€” RUN {ckpt['run_number']} COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"  Total triples:  {manifest['triples_total']:,}\")\nprint(f\"  Batches:         {manifest['batches']}\")\nprint(f\"  Output size:     {manifest['output_size_gb']:.2f} GB\")\nprint(f\"  CLAP dim:        {manifest['clap_dim']}\")\nprint(f\"{'='*60}\")\n\nif ckpt['vocal_cursor'] < len(vocal_segments):\n    remaining = len(vocal_segments) - ckpt['vocal_cursor']\n    print(f\"\\nâš   {remaining} vocal segments remaining.\")\n    print(f\"  To continue: save this output as a dataset,\")\n    print(f\"  attach it to a new notebook as '{PREV_RUN_PATH.name}',\")\n    print(f\"  and re-run this notebook.\")\nelse:\n    print(f\"\\nâœ“  All vocal segments processed. Dataset complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"70c58968","cell_type":"markdown","source":"## ğŸ”— Checkpoint Chaining (20 GB Limit)\n\nIf the output hit the size limit before processing all vocals:\n\n1. **Save this notebook's output** as a Kaggle dataset (e.g. `genesis-data-run1`)\n2. **Create a new notebook** (or re-run this one) and attach:\n   - All the same input datasets (IRs, LJSpeech, VCTK, Language Identifier)\n   - The previous output as input (update `PREV_RUN_PATH` in Cell 3)\n3. **Run all cells** â€” the checkpoint system automatically skips completed work\n\nEach run produces ~19 GB of training triples. Chain as many times as needed.\n\n### Using the Data in Training\n\n```python\n# In the training notebook, load all batches from all runs:\nimport numpy as np\nfrom pathlib import Path\n\nrun_dirs = [\n    Path('/kaggle/input/genesis-data-run1/batches'),\n    Path('/kaggle/input/genesis-data-run2/batches'),\n    # ... add more runs\n]\n\nfor run_dir in run_dirs:\n    for batch_file in sorted(run_dir.glob('batch_*.npz')):\n        data = np.load(batch_file)\n        source_audio = data['source_audio']   # (N, 240000) int16\n        target_audio = data['target_audio']   # (N, 240000) int16\n        target_clap  = data['target_clap']    # (N, CLAP_DIM) float32\n        # Convert int16 back to float32: audio = source_audio.astype(np.float32) / 32767\n        # Compute STFT on-the-fly during training for memory efficiency\n```\n","metadata":{}}]}