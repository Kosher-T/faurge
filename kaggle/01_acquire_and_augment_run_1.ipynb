{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d93efa",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Genesis IR Head â€” Data Curation Pipeline\n",
    "\n",
    "**Notebook 01**: Acquire â†’ Sterilize â†’ Augment â†’ Target\n",
    "\n",
    "This notebook produces training triples for Genesis's IR Synthesis Head:\n",
    "\n",
    "| Tensor | Format | Description |\n",
    "|--------|--------|-------------|\n",
    "| `source_wet` | int16 waveform | Sterile vocal + bad-room IR + random degradations |\n",
    "| `target_wet` | int16 waveform | Same sterile vocal + studio IR (ground truth) |\n",
    "| `target_clap` | float32 vector | CLAP embedding of the target-wet audio |\n",
    "\n",
    "**Checkpoint chaining**: Output is capped at ~19 GB. A `checkpoint.json` tracks\n",
    "progress so the next run (using this output as input) continues where we left off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q noisereduce pyloudnorm soundfile librosa transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffd3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib, time, random, warnings, struct\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.signal import fftconvolve\n",
    "import noisereduce as nr\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# KAGGLE INPUT PATHS  (datasets attached to this notebook)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PATHS = {\n",
    "    'irs':       Path('/kaggle/input/impulse-responses'),\n",
    "    'ljspeech':  Path('/kaggle/input/ljspeech'),\n",
    "    'vctk':      Path('/kaggle/input/vctk-corpus/VCTK-Corpus/wav48'),\n",
    "    'langid_en': Path('/kaggle/input/language-identifier/english/clips'),\n",
    "}\n",
    "\n",
    "# If chaining from a previous run, attach its output as input here:\n",
    "PREV_RUN_PATH = Path('/kaggle/input/genesis-data-run1')  # adjust per run\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OUTPUT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "OUTPUT       = Path('/kaggle/working')\n",
    "BATCH_DIR    = OUTPUT / 'batches'\n",
    "ACQUIRED_DIR = OUTPUT / 'acquired_irs'\n",
    "CLAP_DIR     = OUTPUT / 'clap_model'\n",
    "\n",
    "for d in [BATCH_DIR, ACQUIRED_DIR, CLAP_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# AUDIO PARAMETERS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SR            = 48000\n",
    "CLIP_SEC      = 5.0\n",
    "CLIP_SAMPLES  = int(SR * CLIP_SEC)    # 240,000 samples\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# BUDGET & BATCHING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "TRIPLES_PER_BATCH  = 500\n",
    "MAX_OUTPUT_GB      = 19.0             # safety margin under 20 GB cap\n",
    "AUGMENTATIONS_MIN  = 3\n",
    "AUGMENTATIONS_MAX  = 6\n",
    "\n",
    "print(f\"SR={SR}, CLIP_SEC={CLIP_SEC}, CLIP_SAMPLES={CLIP_SAMPLES}\")\n",
    "print(f\"Output budget: {MAX_OUTPUT_GB} GB, batch size: {TRIPLES_PER_BATCH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CHECKPOINT  â€” resume across runs\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "CHECKPOINT_PATH = OUTPUT / 'checkpoint.json'\n",
    "\n",
    "def load_checkpoint() -> dict:\n",
    "    \"\"\"Load from previous run's output (if chained) or current working dir.\"\"\"\n",
    "    # Check previous run first\n",
    "    prev_ckpt = PREV_RUN_PATH / 'checkpoint.json'\n",
    "    if prev_ckpt.exists():\n",
    "        with open(prev_ckpt) as f:\n",
    "            ckpt = json.load(f)\n",
    "        ckpt['run_number'] += 1\n",
    "        print(f\"[Checkpoint] Resuming from previous run: {ckpt['triples_completed']} triples done\")\n",
    "        return ckpt\n",
    "\n",
    "    # Check current working dir\n",
    "    if CHECKPOINT_PATH.exists():\n",
    "        with open(CHECKPOINT_PATH) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Fresh start\n",
    "    return {\n",
    "        'batch_id': 0,\n",
    "        'triples_completed': 0,\n",
    "        'vocal_cursor': 0,        # index into shuffled vocal list\n",
    "        'run_number': 1,\n",
    "    }\n",
    "\n",
    "def save_checkpoint(ckpt: dict):\n",
    "    with open(CHECKPOINT_PATH, 'w') as f:\n",
    "        json.dump(ckpt, f, indent=2)\n",
    "\n",
    "def get_output_size_gb() -> float:\n",
    "    total = sum(f.stat().st_size for f in OUTPUT.rglob('*') if f.is_file())\n",
    "    return total / (1024 ** 3)\n",
    "\n",
    "ckpt = load_checkpoint()\n",
    "print(f\"[Checkpoint] Run #{ckpt['run_number']}, \"\n",
    "      f\"{ckpt['triples_completed']} triples completed so far, \"\n",
    "      f\"starting at vocal cursor {ckpt['vocal_cursor']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c8c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, zipfile\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# STAGE 1: ACQUIRE â€” download external IR collections\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def download_if_needed(url: str, dest_dir: Path, name: str):\n",
    "    marker = dest_dir / f'.{name}_done'\n",
    "    if marker.exists():\n",
    "        print(f\"  [Acquire] {name} already downloaded\")\n",
    "        return\n",
    "    print(f\"  [Acquire] Downloading {name}...\")\n",
    "    zip_path = dest_dir / f'{name}.zip'\n",
    "    urllib.request.urlretrieve(url, str(zip_path))\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(dest_dir / name)\n",
    "    zip_path.unlink()\n",
    "    marker.touch()\n",
    "    print(f\"  [Acquire] {name} âœ“\")\n",
    "\n",
    "# MIT Impulse Response Survey â€” 271 real-world room IRs (CC-BY 4.0)\n",
    "download_if_needed(\n",
    "    'https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip',\n",
    "    ACQUIRED_DIR, 'mit_irs'\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Load & normalize every IR to 48 kHz mono\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def load_and_normalize_ir(filepath: Path, target_sr: int = SR) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load an IR file, force mono / target SR, peak-normalize.\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(str(filepath), sr=target_sr, mono=True)\n",
    "        if len(audio) < 64:        # degenerate\n",
    "            return None\n",
    "        peak = np.max(np.abs(audio))\n",
    "        if peak > 1e-6:\n",
    "            audio = audio / peak\n",
    "        return audio.astype(np.float32)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"\\n[Acquire] Loading all impulse responses...\")\n",
    "all_irs: Dict[str, dict] = {}\n",
    "\n",
    "# 1) User's IRs  (supports .irs and .wav)\n",
    "for ext in ('*.irs', '*.wav'):\n",
    "    for f in sorted(PATHS['irs'].glob(ext)):\n",
    "        ir = load_and_normalize_ir(f)\n",
    "        if ir is not None:\n",
    "            all_irs[f'user_{f.stem}'] = {'audio': ir, 'source': 'user'}\n",
    "\n",
    "# 2) MIT IRs\n",
    "mit_dir = ACQUIRED_DIR / 'mit_irs'\n",
    "if mit_dir.exists():\n",
    "    for f in sorted(mit_dir.rglob('*.wav')):\n",
    "        ir = load_and_normalize_ir(f)\n",
    "        if ir is not None:\n",
    "            all_irs[f'mit_{f.stem}'] = {'audio': ir, 'source': 'mit'}\n",
    "\n",
    "print(f\"[Acquire] Loaded {len(all_irs)} IRs \"\n",
    "      f\"(user: {sum(1 for v in all_irs.values() if v['source']=='user')}, \"\n",
    "      f\"MIT: {sum(1 for v in all_irs.values() if v['source']=='mit')})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Classify IRs into \"bad\" (room/degraded) and \"studio\" (clean target)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def compute_ir_features(ir_audio: np.ndarray, sr: int = SR) -> dict:\n",
    "    \"\"\"Compute RT60 estimate, spectral centroid, and clarity (C50).\"\"\"\n",
    "    energy = ir_audio ** 2\n",
    "    cumsum = np.cumsum(energy[::-1])[::-1]\n",
    "\n",
    "    # RT60 â€” time for energy to decay 60 dB\n",
    "    rt60 = len(ir_audio) / sr\n",
    "    if cumsum[0] > 1e-10:\n",
    "        decay_db = 10 * np.log10(cumsum / cumsum[0] + 1e-12)\n",
    "        idx = np.where(decay_db < -60)[0]\n",
    "        if len(idx) > 0:\n",
    "            rt60 = idx[0] / sr\n",
    "\n",
    "    # Spectral centroid\n",
    "    S = np.abs(np.fft.rfft(ir_audio))\n",
    "    freqs = np.fft.rfftfreq(len(ir_audio), 1 / sr)\n",
    "    centroid = float(np.sum(freqs * S) / (np.sum(S) + 1e-10))\n",
    "\n",
    "    # Clarity C50 â€” early-to-late energy ratio at 50 ms boundary\n",
    "    split = int(0.05 * sr)\n",
    "    early = np.sum(ir_audio[:split] ** 2) + 1e-12\n",
    "    late  = np.sum(ir_audio[split:] ** 2) + 1e-12\n",
    "    c50   = float(10 * np.log10(early / late))\n",
    "\n",
    "    return {\n",
    "        'rt60': round(rt60, 4),\n",
    "        'centroid': round(centroid, 1),\n",
    "        'c50': round(c50, 2),\n",
    "        'length_sec': round(len(ir_audio) / sr, 4),\n",
    "    }\n",
    "\n",
    "ir_catalogue = {}\n",
    "bad_pool:    List[str] = []   # contamination IRs (rooms, halls, degraded)\n",
    "studio_pool: List[str] = []   # target IRs (mastering, clarity, studio)\n",
    "\n",
    "for ir_id, ir_data in all_irs.items():\n",
    "    feats = compute_ir_features(ir_data['audio'])\n",
    "    ir_data['features'] = feats\n",
    "    ir_catalogue[ir_id] = {'source': ir_data['source'], **feats}\n",
    "\n",
    "    # Heuristic:\n",
    "    #   MIT IRs â†’ always bad pool (real rooms)\n",
    "    #   User IRs with long RT60 or low clarity â†’ bad pool\n",
    "    #   User IRs with short RT60 and high clarity â†’ studio pool\n",
    "    if ir_data['source'] == 'mit':\n",
    "        bad_pool.append(ir_id)\n",
    "    elif feats['rt60'] > 0.25 or feats['c50'] < 8:\n",
    "        bad_pool.append(ir_id)\n",
    "    else:\n",
    "        studio_pool.append(ir_id)\n",
    "\n",
    "# Safety: ensure both pools are adequate\n",
    "if len(studio_pool) < 20:\n",
    "    # Sort user IRs by clarity, take top third as studio\n",
    "    user_ids = sorted(\n",
    "        [k for k, v in all_irs.items() if v['source'] == 'user'],\n",
    "        key=lambda k: all_irs[k]['features']['c50'], reverse=True\n",
    "    )\n",
    "    studio_pool = user_ids[:max(50, len(user_ids) // 3)]\n",
    "    bad_pool = [k for k in all_irs if k not in studio_pool]\n",
    "\n",
    "print(f\"[Classify] Bad pool: {len(bad_pool)} IRs\")\n",
    "print(f\"[Classify] Studio pool: {len(studio_pool)} IRs\")\n",
    "\n",
    "# Save catalogue\n",
    "with open(OUTPUT / 'ir_catalogue.json', 'w') as f:\n",
    "    json.dump(ir_catalogue, f, indent=2)\n",
    "print(\"[Classify] ir_catalogue.json saved âœ“\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# STAGE 2: Freeze CLAP model & cache studio-pool embeddings\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CLAP_MODEL_ID = \"laion/larger_clap_music_and_speech\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[CLAP] Device: {device}\")\n",
    "\n",
    "print(\"[CLAP] Loading model...\")\n",
    "clap_processor = ClapProcessor.from_pretrained(CLAP_MODEL_ID)\n",
    "clap_model = ClapModel.from_pretrained(CLAP_MODEL_ID).to(device).eval()\n",
    "CLAP_DIM = clap_model.config.projection_dim\n",
    "print(f\"[CLAP] Loaded â€” embedding dim = {CLAP_DIM}\")\n",
    "\n",
    "# Save frozen model for downstream notebooks\n",
    "clap_model.save_pretrained(CLAP_DIR)\n",
    "clap_processor.save_pretrained(CLAP_DIR)\n",
    "print(f\"[CLAP] Frozen model saved to {CLAP_DIR}\")\n",
    "\n",
    "\n",
    "def get_clap_audio_embedding(audio: np.ndarray, sr: int = SR) -> np.ndarray:\n",
    "    \"\"\"Encode audio through CLAP's audio tower. Returns (CLAP_DIM,) float32.\"\"\"\n",
    "    inputs = clap_processor(\n",
    "        audios=audio, sampling_rate=sr, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        emb = clap_model.get_audio_features(**inputs)\n",
    "    return emb.cpu().numpy().flatten().astype(np.float32)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Pre-compute CLAP embeddings for every studio-pool IR.\n",
    "# We convolve each IR with 3 seconds of white noise so CLAP has\n",
    "# a richer scene to analyse (raw IRs are too short / sparse).\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\n[CLAP] Pre-computing embeddings for {len(studio_pool)} studio IRs...\")\n",
    "ref_noise = np.random.randn(SR * 3).astype(np.float32) * 0.1\n",
    "\n",
    "clap_cache: Dict[str, np.ndarray] = {}\n",
    "for i, ir_id in enumerate(studio_pool):\n",
    "    ir_audio = all_irs[ir_id]['audio']\n",
    "    scene = fftconvolve(ref_noise, ir_audio, mode='full')[:SR * 3]\n",
    "    scene = scene / (np.max(np.abs(scene)) + 1e-8)\n",
    "    clap_cache[ir_id] = get_clap_audio_embedding(scene)\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  {i+1}/{len(studio_pool)}\")\n",
    "\n",
    "np.savez(OUTPUT / 'clap_cache.npz', **clap_cache)\n",
    "print(f\"[CLAP] Cached {len(clap_cache)} embeddings âœ“  (dim={CLAP_DIM})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd282e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyloudnorm as pyln\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# STAGE 3: STERILIZE â€” noise reduce, trim, segment, normalize\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "meter = pyln.Meter(SR)\n",
    "\n",
    "def discover_audio_files() -> List[Tuple[Path, str]]:\n",
    "    \"\"\"Collect all vocal files from all datasets. Returns (path, dataset_tag).\"\"\"\n",
    "    files = []\n",
    "    # LJSpeech\n",
    "    for f in sorted(PATHS['ljspeech'].rglob('*.wav')):\n",
    "        files.append((f, 'ljspeech'))\n",
    "    # VCTK â€” subfolder per speaker\n",
    "    for f in sorted(PATHS['vctk'].rglob('*.wav')):\n",
    "        files.append((f, f'vctk_{f.parent.name}'))\n",
    "    # Language Identifier â€” english clips\n",
    "    for ext in ('*.wav', '*.mp3', '*.ogg'):\n",
    "        for f in sorted(PATHS['langid_en'].rglob(ext)):\n",
    "            files.append((f, 'langid_en'))\n",
    "    return files\n",
    "\n",
    "\n",
    "def sterilize_and_segment(filepath: Path, tag: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Load â†’ noise-reduce â†’ trim silence â†’ segment into CLIP_SEC windows.\n",
    "    Returns list of dicts with 'audio' (float32) and metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr_orig = librosa.load(str(filepath), sr=SR, mono=True)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    if len(audio) < SR * 1.0:   # skip clips shorter than 1 second\n",
    "        return []\n",
    "\n",
    "    # â”€â”€ Spectral noise reduction (simulates Ursula's output) â”€â”€\n",
    "    audio = nr.reduce_noise(y=audio, sr=SR, stationary=True, prop_decrease=0.85)\n",
    "\n",
    "    # â”€â”€ Trim silence â”€â”€\n",
    "    audio, _ = librosa.effects.trim(audio, top_db=40)\n",
    "    if len(audio) < SR * 1.5:\n",
    "        return []\n",
    "\n",
    "    # â”€â”€ Normalize loudness to -23 LUFS â”€â”€\n",
    "    try:\n",
    "        loudness = meter.integrated_loudness(audio)\n",
    "        if loudness > -70:  # not silence\n",
    "            audio = pyln.normalize.loudness(audio, loudness, -23.0)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # â”€â”€ Segment into CLIP_SEC windows â”€â”€\n",
    "    segments = []\n",
    "    for start in range(0, len(audio) - SR, CLIP_SAMPLES):\n",
    "        chunk = audio[start : start + CLIP_SAMPLES]\n",
    "        if len(chunk) < CLIP_SAMPLES:\n",
    "            chunk = np.pad(chunk, (0, CLIP_SAMPLES - len(chunk)))\n",
    "\n",
    "        # Skip near-silent segments\n",
    "        rms = np.sqrt(np.mean(chunk ** 2))\n",
    "        if rms < 1e-4:\n",
    "            continue\n",
    "\n",
    "        segments.append({\n",
    "            'audio':   chunk.astype(np.float32),\n",
    "            'file':    filepath.name,\n",
    "            'dataset': tag,\n",
    "        })\n",
    "    return segments\n",
    "\n",
    "\n",
    "# â”€â”€ Discover & sterilize â”€â”€\n",
    "print(\"[Sterilize] Discovering audio files...\")\n",
    "all_audio_files = discover_audio_files()\n",
    "print(f\"[Sterilize] Found {len(all_audio_files)} source files\")\n",
    "\n",
    "# Shuffle deterministically so runs are balanced across datasets\n",
    "random.shuffle(all_audio_files)\n",
    "\n",
    "# Process in batches to manage memory\n",
    "STERILIZE_CHUNK = 500   # files processed at a time\n",
    "vocal_segments: List[dict] = []\n",
    "cursor_start = ckpt.get('vocal_cursor', 0) // 5  # approximate file index\n",
    "\n",
    "print(f\"[Sterilize] Processing from file ~{cursor_start}...\")\n",
    "for i, (fpath, tag) in enumerate(all_audio_files):\n",
    "    if i % 200 == 0 and i > 0:\n",
    "        print(f\"  Processed {i}/{len(all_audio_files)} files â†’ \"\n",
    "              f\"{len(vocal_segments)} segments so far\")\n",
    "    segs = sterilize_and_segment(fpath, tag)\n",
    "    vocal_segments.extend(segs)\n",
    "\n",
    "print(f\"\\n[Sterilize] Total sterile segments: {len(vocal_segments)}\")\n",
    "print(f\"[Sterilize] Dataset breakdown:\")\n",
    "ds_counts = defaultdict(int)\n",
    "for s in vocal_segments:\n",
    "    ds_counts[s['dataset'].split('_')[0]] += 1\n",
    "for ds, cnt in sorted(ds_counts.items()):\n",
    "    print(f\"  {ds}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DEGRADATION TOOLKIT â€” applied to source_wet ONLY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def add_noise(audio: np.ndarray, noise_type: str, snr_db: float) -> np.ndarray:\n",
    "    \"\"\"Mix coloured noise at the target SNR.\"\"\"\n",
    "    n = len(audio)\n",
    "    if noise_type == 'white':\n",
    "        noise = np.random.randn(n)\n",
    "    elif noise_type == 'pink':\n",
    "        # Pink noise via spectral shaping\n",
    "        freqs = np.fft.rfftfreq(n, 1 / SR)\n",
    "        freqs[0] = 1\n",
    "        S = 1.0 / np.sqrt(freqs)\n",
    "        noise = np.fft.irfft(S * np.exp(2j * np.pi * np.random.rand(len(S))))[:n]\n",
    "    elif noise_type == 'brown':\n",
    "        noise = np.cumsum(np.random.randn(n))\n",
    "        noise -= np.mean(noise)\n",
    "    elif noise_type == 'hvac':\n",
    "        # Band-limited broadband 100-1000 Hz\n",
    "        noise = np.random.randn(n)\n",
    "        from scipy.signal import butter, sosfilt\n",
    "        sos = butter(4, [100, 1000], btype='band', fs=SR, output='sos')\n",
    "        noise = sosfilt(sos, noise)\n",
    "    elif noise_type == 'hum':\n",
    "        # 50 Hz fundamental + harmonics\n",
    "        t = np.arange(n) / SR\n",
    "        base_freq = random.choice([50, 60])\n",
    "        noise = np.zeros(n)\n",
    "        for h in range(1, 6):\n",
    "            amp = 1.0 / h\n",
    "            noise += amp * np.sin(2 * np.pi * base_freq * h * t + random.uniform(0, 2*np.pi))\n",
    "    else:\n",
    "        noise = np.random.randn(n)\n",
    "\n",
    "    # Scale noise to target SNR\n",
    "    sig_power = np.mean(audio ** 2) + 1e-12\n",
    "    noise_power = np.mean(noise ** 2) + 1e-12\n",
    "    target_noise_power = sig_power / (10 ** (snr_db / 10))\n",
    "    noise = noise * np.sqrt(target_noise_power / noise_power)\n",
    "    return audio + noise.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_eq(audio: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Random 3-band parametric EQ (simulates mic coloration).\"\"\"\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    bands = [\n",
    "        (80, 300),     # low\n",
    "        (300, 3000),   # mid\n",
    "        (3000, 12000), # high\n",
    "    ]\n",
    "    for lo, hi in bands:\n",
    "        gain_db = random.uniform(-6, 6)\n",
    "        if abs(gain_db) < 1:\n",
    "            continue\n",
    "        try:\n",
    "            sos = butter(2, [lo, hi], btype='band', fs=SR, output='sos')\n",
    "            band_sig = sosfilt(sos, audio)\n",
    "            gain_lin = 10 ** (gain_db / 20)\n",
    "            audio = audio + band_sig * (gain_lin - 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_highpass(audio: np.ndarray) -> np.ndarray:\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    cutoff = random.uniform(60, 300)\n",
    "    sos = butter(4, cutoff, btype='high', fs=SR, output='sos')\n",
    "    return sosfilt(sos, audio).astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_lowpass(audio: np.ndarray) -> np.ndarray:\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    cutoff = random.uniform(3000, 16000)\n",
    "    sos = butter(4, cutoff, btype='low', fs=SR, output='sos')\n",
    "    return sosfilt(sos, audio).astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_gain_jitter(audio: np.ndarray) -> np.ndarray:\n",
    "    gain_db = random.uniform(-6, 6)\n",
    "    return audio * (10 ** (gain_db / 20))\n",
    "\n",
    "\n",
    "def apply_bitcrush(audio: np.ndarray) -> np.ndarray:\n",
    "    bits = random.randint(8, 16)\n",
    "    levels = 2 ** bits\n",
    "    return (np.round(audio * levels) / levels).astype(np.float32)\n",
    "\n",
    "\n",
    "# Registry of all degradation functions\n",
    "DEGRADATIONS = {\n",
    "    'noise':     lambda a: add_noise(a, random.choice(['white','pink','brown','hvac','hum']),\n",
    "                                     random.uniform(5, 40)),\n",
    "    'eq':        apply_eq,\n",
    "    'highpass':  apply_highpass,\n",
    "    'lowpass':   apply_lowpass,\n",
    "    'gain':      apply_gain_jitter,\n",
    "    'bitcrush':  apply_bitcrush,\n",
    "}\n",
    "\n",
    "def apply_random_degradations(audio: np.ndarray) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Apply a random subset of 3-6 degradations. Returns (degraded, list_of_names).\"\"\"\n",
    "    n_augs = random.randint(AUGMENTATIONS_MIN, AUGMENTATIONS_MAX)\n",
    "    chosen = random.sample(list(DEGRADATIONS.keys()), min(n_augs, len(DEGRADATIONS)))\n",
    "    for name in chosen:\n",
    "        audio = DEGRADATIONS[name](audio)\n",
    "    # Hard clip to [-1, 1] as final safety\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return audio.astype(np.float32), chosen\n",
    "\n",
    "print(\"[Degradation] Toolkit loaded âœ“\")\n",
    "print(f\"  Available: {list(DEGRADATIONS.keys())}\")\n",
    "print(f\"  Per triple: {AUGMENTATIONS_MIN}-{AUGMENTATIONS_MAX} random degradations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd158e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# STAGE 4: THE TRIPLE ENGINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# For each sterile vocal V:\n",
    "#   1) Convolve with random bad IR (A)  â†’ source_wet = V + A\n",
    "#   2) Apply random degradations to source_wet\n",
    "#   3) Convolve same V with random studio IR (C)  â†’ target_wet = V + C\n",
    "#   4) Get CLAP embedding of target_wet\n",
    "#   5) Pack into batch\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def convolve_and_trim(vocal: np.ndarray, ir: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convolve vocal with IR, trim to original length, peak-normalize.\"\"\"\n",
    "    wet = fftconvolve(vocal, ir, mode='full')[:len(vocal)]\n",
    "    peak = np.max(np.abs(wet))\n",
    "    if peak > 1e-6:\n",
    "        wet = wet / peak\n",
    "    return wet.astype(np.float32)\n",
    "\n",
    "\n",
    "def audio_to_int16(audio: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert float32 [-1,1] to int16 for compact storage.\"\"\"\n",
    "    return (np.clip(audio, -1, 1) * 32767).astype(np.int16)\n",
    "\n",
    "\n",
    "# â”€â”€ Pre-shuffle vocal segments for reproducibility â”€â”€\n",
    "random.shuffle(vocal_segments)\n",
    "\n",
    "# â”€â”€ Resume from checkpoint cursor â”€â”€\n",
    "start_idx = ckpt.get('vocal_cursor', 0)\n",
    "batch_id  = ckpt.get('batch_id', 0)\n",
    "total     = ckpt.get('triples_completed', 0)\n",
    "\n",
    "# â”€â”€ Batch accumulators â”€â”€\n",
    "batch_sources:  List[np.ndarray] = []\n",
    "batch_targets:  List[np.ndarray] = []\n",
    "batch_claps:    List[np.ndarray] = []\n",
    "batch_meta:     List[dict]       = []\n",
    "\n",
    "# â”€â”€ Load CLAP cache â”€â”€\n",
    "clap_cache_data = dict(np.load(OUTPUT / 'clap_cache.npz'))\n",
    "\n",
    "print(f\"\\n[Engine] Starting triple generation...\")\n",
    "print(f\"  Vocal segments: {len(vocal_segments)}\")\n",
    "print(f\"  Bad pool: {len(bad_pool)}, Studio pool: {len(studio_pool)}\")\n",
    "print(f\"  Resuming from index {start_idx}, batch {batch_id}\\n\")\n",
    "\n",
    "t_start = time.time()\n",
    "triples_this_run = 0\n",
    "skipped = 0\n",
    "\n",
    "for seg_idx in range(start_idx, len(vocal_segments)):\n",
    "    # â”€â”€ Budget check â”€â”€\n",
    "    if get_output_size_gb() > MAX_OUTPUT_GB:\n",
    "        print(f\"\\nâš   Output size limit reached ({MAX_OUTPUT_GB} GB). Stopping.\")\n",
    "        break\n",
    "\n",
    "    seg = vocal_segments[seg_idx]\n",
    "    V = seg['audio']\n",
    "\n",
    "    # â”€â”€ Pick random bad IR & studio IR â”€â”€\n",
    "    bad_ir_id    = random.choice(bad_pool)\n",
    "    studio_ir_id = random.choice(studio_pool)\n",
    "\n",
    "    ir_A = all_irs[bad_ir_id]['audio']\n",
    "    ir_C = all_irs[studio_ir_id]['audio']\n",
    "\n",
    "    # â”€â”€ Convolve â”€â”€\n",
    "    source_wet = convolve_and_trim(V, ir_A)\n",
    "    target_wet = convolve_and_trim(V, ir_C)\n",
    "\n",
    "    # â”€â”€ Random wet/dry mix on source (0.3-1.0) â”€â”€\n",
    "    wet_dry = random.uniform(0.3, 1.0)\n",
    "    source_wet = wet_dry * source_wet + (1 - wet_dry) * V\n",
    "\n",
    "    # â”€â”€ Degrade source only â”€â”€\n",
    "    source_wet, aug_names = apply_random_degradations(source_wet)\n",
    "\n",
    "    # â”€â”€ QA: reject bad triples â”€â”€\n",
    "    src_rms = np.sqrt(np.mean(source_wet ** 2))\n",
    "    tgt_rms = np.sqrt(np.mean(target_wet ** 2))\n",
    "    if src_rms < 1e-4 or tgt_rms < 1e-4:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    if np.any(np.isnan(source_wet)) or np.any(np.isnan(target_wet)):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # â”€â”€ CLAP embedding for target â”€â”€\n",
    "    target_clap = clap_cache_data.get(studio_ir_id)\n",
    "    if target_clap is None:\n",
    "        # Fallback: compute live\n",
    "        target_clap = get_clap_audio_embedding(target_wet)\n",
    "\n",
    "    # â”€â”€ Accumulate â”€â”€\n",
    "    batch_sources.append(audio_to_int16(source_wet))\n",
    "    batch_targets.append(audio_to_int16(target_wet))\n",
    "    batch_claps.append(target_clap)\n",
    "    batch_meta.append({\n",
    "        'vocal_file':   seg['file'],\n",
    "        'dataset':      seg['dataset'],\n",
    "        'bad_ir':       bad_ir_id,\n",
    "        'studio_ir':    studio_ir_id,\n",
    "        'wet_dry':      round(wet_dry, 3),\n",
    "        'degradations': aug_names,\n",
    "    })\n",
    "\n",
    "    # â”€â”€ Flush batch when full â”€â”€\n",
    "    if len(batch_sources) >= TRIPLES_PER_BATCH:\n",
    "        batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n",
    "        np.savez(\n",
    "            batch_path,\n",
    "            source_audio  = np.stack(batch_sources),\n",
    "            target_audio  = np.stack(batch_targets),\n",
    "            target_clap   = np.stack(batch_claps),\n",
    "        )\n",
    "        # Save metadata separately (JSON, lightweight)\n",
    "        meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n",
    "        with open(meta_path, 'w') as f:\n",
    "            json.dump(batch_meta, f)\n",
    "\n",
    "        triples_this_run += len(batch_sources)\n",
    "        total += len(batch_sources)\n",
    "        batch_id += 1\n",
    "\n",
    "        # Checkpoint\n",
    "        ckpt.update({\n",
    "            'batch_id': batch_id,\n",
    "            'triples_completed': total,\n",
    "            'vocal_cursor': seg_idx + 1,\n",
    "        })\n",
    "        save_checkpoint(ckpt)\n",
    "\n",
    "        elapsed = time.time() - t_start\n",
    "        rate = triples_this_run / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Batch {batch_id-1:4d} saved | \"\n",
    "              f\"triples: {total:,} total, {triples_this_run:,} this run | \"\n",
    "              f\"{rate:.0f}/sec | \"\n",
    "              f\"{get_output_size_gb():.1f} GB used\")\n",
    "\n",
    "        batch_sources.clear()\n",
    "        batch_targets.clear()\n",
    "        batch_claps.clear()\n",
    "        batch_meta.clear()\n",
    "\n",
    "# â”€â”€ Flush remaining â”€â”€\n",
    "if batch_sources:\n",
    "    batch_path = BATCH_DIR / f'batch_{batch_id:04d}.npz'\n",
    "    np.savez(\n",
    "        batch_path,\n",
    "        source_audio = np.stack(batch_sources),\n",
    "        target_audio = np.stack(batch_targets),\n",
    "        target_clap  = np.stack(batch_claps),\n",
    "    )\n",
    "    meta_path = BATCH_DIR / f'batch_{batch_id:04d}_meta.json'\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(batch_meta, f)\n",
    "\n",
    "    triples_this_run += len(batch_sources)\n",
    "    total += len(batch_sources)\n",
    "    batch_id += 1\n",
    "\n",
    "    ckpt.update({\n",
    "        'batch_id': batch_id,\n",
    "        'triples_completed': total,\n",
    "        'vocal_cursor': len(vocal_segments),\n",
    "    })\n",
    "    save_checkpoint(ckpt)\n",
    "\n",
    "elapsed = time.time() - t_start\n",
    "print(f\"\\n[Engine] Run complete.\")\n",
    "print(f\"  Triples this run: {triples_this_run:,}\")\n",
    "print(f\"  Triples total:    {total:,}\")\n",
    "print(f\"  Batches written:  {batch_id}\")\n",
    "print(f\"  Skipped (QA):     {skipped}\")\n",
    "print(f\"  Elapsed:          {elapsed/60:.1f} min\")\n",
    "print(f\"  Output size:      {get_output_size_gb():.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc252cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MANIFEST â€” checksums, statistics, verification\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b''):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "manifest = {\n",
    "    'run_number':        ckpt['run_number'],\n",
    "    'triples_total':     ckpt['triples_completed'],\n",
    "    'batches':           ckpt['batch_id'],\n",
    "    'sample_rate':       SR,\n",
    "    'clip_seconds':      CLIP_SEC,\n",
    "    'clip_samples':      CLIP_SAMPLES,\n",
    "    'clap_dim':          CLAP_DIM,\n",
    "    'bad_pool_size':     len(bad_pool),\n",
    "    'studio_pool_size':  len(studio_pool),\n",
    "    'output_size_gb':    round(get_output_size_gb(), 3),\n",
    "    'batch_checksums':   {},\n",
    "}\n",
    "\n",
    "print(\"[Manifest] Computing checksums...\")\n",
    "for f in sorted(BATCH_DIR.glob('batch_*.npz')):\n",
    "    manifest['batch_checksums'][f.name] = sha256_file(f)\n",
    "\n",
    "with open(OUTPUT / 'manifest.json', 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"[Manifest] Saved âœ“\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  GENESIS DATA CURATION â€” RUN {ckpt['run_number']} COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total triples:  {manifest['triples_total']:,}\")\n",
    "print(f\"  Batches:         {manifest['batches']}\")\n",
    "print(f\"  Output size:     {manifest['output_size_gb']:.2f} GB\")\n",
    "print(f\"  CLAP dim:        {manifest['clap_dim']}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if ckpt['vocal_cursor'] < len(vocal_segments):\n",
    "    remaining = len(vocal_segments) - ckpt['vocal_cursor']\n",
    "    print(f\"\\nâš   {remaining} vocal segments remaining.\")\n",
    "    print(f\"  To continue: save this output as a dataset,\")\n",
    "    print(f\"  attach it to a new notebook as '{PREV_RUN_PATH.name}',\")\n",
    "    print(f\"  and re-run this notebook.\")\n",
    "else:\n",
    "    print(f\"\\nâœ“  All vocal segments processed. Dataset complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c58968",
   "metadata": {},
   "source": [
    "## ðŸ”— Checkpoint Chaining (20 GB Limit)\n",
    "\n",
    "If the output hit the size limit before processing all vocals:\n",
    "\n",
    "1. **Save this notebook's output** as a Kaggle dataset (e.g. `genesis-data-run1`)\n",
    "2. **Create a new notebook** (or re-run this one) and attach:\n",
    "   - All the same input datasets (IRs, LJSpeech, VCTK, Language Identifier)\n",
    "   - The previous output as input (update `PREV_RUN_PATH` in Cell 3)\n",
    "3. **Run all cells** â€” the checkpoint system automatically skips completed work\n",
    "\n",
    "Each run produces ~19 GB of training triples. Chain as many times as needed.\n",
    "\n",
    "### Using the Data in Training\n",
    "\n",
    "```python\n",
    "# In the training notebook, load all batches from all runs:\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "run_dirs = [\n",
    "    Path('/kaggle/input/genesis-data-run1/batches'),\n",
    "    Path('/kaggle/input/genesis-data-run2/batches'),\n",
    "    # ... add more runs\n",
    "]\n",
    "\n",
    "for run_dir in run_dirs:\n",
    "    for batch_file in sorted(run_dir.glob('batch_*.npz')):\n",
    "        data = np.load(batch_file)\n",
    "        source_audio = data['source_audio']   # (N, 240000) int16\n",
    "        target_audio = data['target_audio']   # (N, 240000) int16\n",
    "        target_clap  = data['target_clap']    # (N, CLAP_DIM) float32\n",
    "        # Convert int16 back to float32: audio = source_audio.astype(np.float32) / 32767\n",
    "        # Compute STFT on-the-fly during training for memory efficiency\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
