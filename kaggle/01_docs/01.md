# 01_acquire_and_augment.md

## ðŸŸ¢ Phase 0: Environment & Path Configuration
**Description:** Set up the global parameters, deterministic seeds, and strict directory paths based on the Kaggle environment.

**Input Paths (From paths.md):**
* **Bad IRs (Unlabeled User IRs):** `/kaggle/input/datasets/itorousa/impulse-responses`
* **Good IRs (Labeled MIT IRs, to be downloaded)**, stored in output directory.
* **Dry Vocals (LJSpeech):** `/kaggle/input/datasets/dromosys/ljspeech/`
* **Dry Vocals (VCTK):** `/kaggle/input/datasets/kynthesis/vctk-corpus/VCTK-Corpus/wav48`
* **Dry Vocals (English LangID):** `/kaggle/input/datasets/shrivatssudhir/language-identifier/english/clips`
* **Previous Run (For chaining multiple notebooks, 19 GB storage limit):** `/kaggle/input/genesis-data-run1` (adjustable)

**Output Paths:**
* **Base:** `/kaggle/working`
* **Subdirectories to create:** `batches/`, `clap_model/`, `sterilized_batches/`, `irs`

**Audio & Budget Parameters:**
* Sample Rate: 48,000 Hz
* Clip Length: 5.0 seconds (240,000 samples)
* Batch Size: 500 triples per file
* Output Budget: 19.0 GB maximum (to prevent Kaggle I/O timeouts)
* Augmentations per clip: 3 to 6

---

## ðŸŸ¢ Phase 1: Checkpoint Initialization
**Description:** Ensure the script can seamlessly resume from where it left off, whether interrupted by a crash, user fatigue, or chained into a new notebook due to storage limits.

1.  Look for `checkpoint.json` in the **Previous Run** directory (/kaggle/input/notebooks/itorousa/genesis_data_run#). If found, increment the `run_number` and load the stats.
2.  If not found in the previous run, check the **Current Working Directory**.
3.  If neither exists, initialize a fresh state: `batch_id: 0`, `triples_completed: 0`, `vocal_cursor: 0`, `run_number: 1`.
4.  Provide a helper function to calculate the current size of `/kaggle/working` in GB to monitor the budget.

---

## ðŸŸ¢ Phase 2: Impulse Response Acquisition & Normalization
**Description:** Load, standardize, and pool the acoustic spaces.

1.  **Iterate Bad IRs:** Scan the custom IR path for `.wav` and `.irs` files (/kaggle/input/datasets/itorousa/impulse-responses).
2.  **Iterate Good IRs:** Scan the MIT IR path for `.wav` files (/kaggle/working/irs/mit_irs for first run, then /kaggle/input/notebooks/itorousa/genesis_data_run#/irs/mit_irs for subsequent runs).
3.  **Standardization:** For every file found:
    * Load at 48 kHz.
    * Force to mono.
    * Peak-normalize the waveform (divide by the maximum absolute value).
    * Discard if the array is mathematically degenerate (e.g., shorter than 64 samples).
4.  Store them in a unified dictionary mapped to an ID (e.g., `user_filename` and `mit_filename`) tracking their raw audio and source origin.

---

## ðŸŸ¢ Phase 3: IR Classification & Cataloguing
**Description:** Extract acoustic features and strictly separate IRs into "Bad" (degradation pool) and "Studio" (target pool).

1.  **Cache Check:** Look for `ir_catalogue.json` in the Output directory. If it exists, load it, assign pools, and skip all acoustic math.
2.  **Feature Extraction (If no cache):** Calculate:
    * *RT60*: Estimate the time it takes for energy to decay by 60dB.
    * *Spectral Centroid*: The center of mass of the frequency spectrum.
    * *C50 (Clarity)*: Ratio of early energy (first 50ms) to late energy.
3.  **Pool Assignment:**
    * Assign all custom User IRs to the `bad_pool` (these are unlabeled/degraded).
    * Assign all MIT IRs to the `studio_pool` (these are clean/labeled target spaces).
4.  **Save:** Write the calculated features and assignments to `ir_catalogue.json` to bypass this step on future runs.

---

## ðŸŸ¢ Phase 4: CLAP Model Setup & Target Embedding Cache
**Description:** Freeze the Hugging Face CLAP model locally and pre-compute the acoustic embeddings for all "Good" IRs to save massive amounts of compute during the main loop.

1.  **Model Loading/Caching:**
    * Check for the frozen `clap_model` directory in the current working directory.
    * If missing, check the **Previous Run** directory. If found, copy it over.
    * If completely missing, download from Hugging Face (`laion/larger_clap_music_and_speech`), save it locally to `/kaggle/working/clap_model`, and set it to evaluation mode on the GPU.
2.  **Pre-compute Embeddings:**
    * Check for `clap_cache.npz` in the current or previous run. Copy/load if it exists.
    * If missing: Iterate over every IR in the `studio_pool`.
    * Convolve the IR with 3 seconds of generated white noise (to give CLAP a rich frequency spectrum to analyze instead of a tiny transient click).
    * Normalize the convolved scene, pass it through the CLAP processor, extract the `pooler_output`, flatten to a 512-dim NumPy float32 array, and map it to the IR's ID.
    * ðŸ’¾ Save the dictionary as `clap_cache.npz`.

---

## ðŸŸ¢ Phase 5: Vocal Sterilization & RAM Management
**Description:** Prepare the dry vocals while strictly managing RAM to prevent Kaggle Out-Of-Memory (OOM) crashes.

1.  **Discovery:** Glob all audio files from LJSpeech, VCTK, and the English LangID paths.
2.  **Deterministic Shuffle:** Sort the file list alphabetically, then shuffle using a fixed seed. This guarantees the array order remains identical across notebook restarts.
3.  **Process (from vocal_cursor index):**
    * Load file (48kHz, mono). Ignore if < 1.0 seconds.
    * Apply mild spectral noise reduction (Ursula simulation).
    * Trim absolute silence from the top/tail. Ignore if the remaining audio is < 1.5 seconds.
    * Measure LUFS. If not dead silent, loudness-normalize to -23.0 LUFS.
4.  **Segmentation:** Slice the sterilized audio into rigid 240,000-sample (5.0s) chunks. Zero-pad the final chunk if it's slightly short. Discard near-silent chunks based on RMS.
5.  **Disk Flushing (VRAM Protection):** * Process files in chunks of 500.
    * Once a chunk is sterilized, ðŸ’¾ dump the list of segments to a `.pkl` file in `sterilized_batches/` and write the current index to `sterilize_state.json`.
    * *Clear the RAM array completely* before moving to the next 500 files.
6.  **Aggregation:** Read all `.pkl` files from disk to compile the final list of sterilized segments.

---

## ðŸŸ¢ Phase 6: The Degradation Toolkit
**Description:** Define a suite of random, destructive audio effects to apply strictly to the `source_wet` audio.

1.  **Noise Additions:** Generate colored noise (White, Pink, Brown, HVAC band-limited, 50/60Hz Hum) and scale it to a random target SNR (5 to 40 dB).
2.  **Parametric EQ:** Randomly shift gain (-6 to +6 dB) across Low (80-300Hz), Mid (300-3kHz), and High (3k-12kHz) frequency bands.
3.  **Filters:** Random Highpass (60-300Hz) and Random Lowpass (3k-16kHz).
4.  **Gain Jitter:** Randomly adjust overall gain (-6 to +6 dB).
5.  **Bitcrushing:** Quantize the audio resolution down to a random integer between 8 and 16 bits.
6.  **Randomizer Function:** A wrapper that randomly selects 3 to 6 of these functions, applies them sequentially to the waveform, and mathematically hard-clips the final output to exactly [-1.0, 1.0].

---

## ðŸŸ¢ Phase 7: The Triple Engine (Dataset Generator)
**Description:** The core loop. Combine vocals, IRs, and degradations into final training triples.

1.  Resume iteration from `checkpoint['vocal_cursor']`.
2.  **For each sterilized vocal segment:**
    * âš  **Budget Check:** Evaluate the total size of `/kaggle/working`. If > 19.0 GB, gracefully break the loop to allow for a safe Kaggle commit.
    * **Select Spaces:** Pick one random IR from the `bad_pool` (A) and one from the `studio_pool` (C).
    * **Convolve:** * `source_wet` = Vocal convolved with Bad IR (A). Trim back to 5.0 seconds and peak-normalize.
        * `target_wet` = Vocal convolved with Studio IR (C). Trim and peak-normalize.
    * **Mix & Degrade Source:** * Mix `source_wet` with the dry vocal at a random ratio (30% to 100% wet).
        * Pass `source_wet` through the Degradation Toolkit.
    * **QA Verification:** Measure RMS of both wet signals. If either is practically silent or contains NaNs, reject the triple and skip to the next.
    * **Fetch Target Label:** Retrieve the CLAP embedding for the Studio IR from the RAM cache.
    * **Quantize:** Convert `source_wet` and `target_wet` from float32 arrays to `int16` arrays to drastically reduce final file size.
    * Append all three components (Source Int16, Target Int16, CLAP Float32) and metadata to the current batch.
3.  **Batch Save Mechanism:**
    * When the batch hits 500 triples, ðŸ’¾ save to `batch_{id}.npz`.
    * Save human-readable metadata (file origins, applied degradations) to `batch_{id}_meta.json`.
    * Update `checkpoint.json` with the new cursor position and completed triple count.
    * Clear the batch lists from RAM.

---

## ðŸŸ¢ Phase 8: Manifest & Pipeline Conclusion
**Description:** Wrap up the run, compute integrity checks, and prepare the dataset for the next link in the chain.

1.  Flush any remaining triples (< 500) in the batch accumulator to a final `.npz` file.
2.  **Checksums:** Iterate through all generated `batch_*.npz` files and compute their SHA-256 hashes.
3.  **Generate manifest.json:** Write out the total triples, dataset configurations, dimensions, total output size, and the checksum dictionary.
4.  **Status Report:**
    * If the script halted due to the 19GB limit, print a âš  prompt instructing the user to save the commit as a dataset, update `PREV_RUN_PATH`, and restart.
    * If the script completed every vocal segment, print a ðŸŸ¢ success message indicating the curation phase is entirely finished.