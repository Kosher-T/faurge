# Agent 2: Ursula (The DSP Specialist)

## Part 1: Overview
Ursula is the mechanical mixing engineer of Faurge. She operates strictly within the hard, physical bounds of digital signal processing. She cannot synthesize physical space, which is why she no longer receives abstract CLAP vectors. 

Operating in a Reinforcement Learning (RL) loop, she acts as the bridge between Fabian's calculated physics and the literal audio plugins in the active DSP graph (Parametric EQ, Multiband Compressor, De-esser, Exciter, Limiter). Ursula simply receives concrete metric targets (LTAS for EQ, LUFS/LRA for Compression, ZCR for De-essing), grabs the parameters of the DSP nodes, and adjusts them until the current audio perfectly matches Fabian's requested physics.

---

## Part 2: Engineering Specification

### 1. Core Architecture
Ursula is modeled as a **Soft Actor-Critic (SAC)** Reinforcement Learning agent operating in a continuous action space. 



Her "environment" is a headless PipeWire DSP graph. Her "actions" are floating-point values applied to plugin parameters. Her goal is to maximize her mathematical reward by solving the inverse-DSP problem: finding the exact parameter combination that minimizes the physical distance between the current audio's metrics and the target metrics.

### 2. The Inputs
Ursula requires the current state of the environment and the explicit physical destination.

* **Observation Space ($S_t$):**
  * **Current State:** $M_{current}$ (The literal LTAS array, LUFS value, and Crest Factor of the audio currently passing through her DSP graph).
  * **The Target:** $M_{target}$ (The destination metrics generated by Fabian).
  * **Current Parameter Positions:** $A_{t-1} \in \mathbb{R}^{K}$ (Where $K$ is the total number of controllable DSP nodes).

The final input tensor given to Ursula's policy network at any step $t$ is: 
$$S_t = [M_{current} \oplus M_{target} \oplus A_{t-1}]$$

### 3. Processing Mechanics & The Action Space
When Ursula observes $S_t$, her Actor Network outputs a continuous action vector $A_t \in [-1, 1]^K$.

Because audio plugins have vastly different units (Hz, dB, milliseconds, ratios), her normalized $[-1, 1]$ outputs are mathematically un-normalized into the literal API or OSC constraints of the host environment.
* An output of `-1.0` on EQ Band 1 Gain maps to `-15.0 dB`.
* An output of `0.5` on the Compressor Ratio maps to `4:1`.

Ursula pushes these un-normalized values to the active DSP graph, physically altering the audio stream in real-time.

### 4. Raw Outputs
Ursula outputs a highly structured parameter map.

* **DSP Parameter Map (JSON/Dictionary):** A literal array of float values corresponding directly to the host plugin states. 
  * Example: `{"eq_band_1_hz": 80.0, "eq_band_1_gain": -3.2, "compressor_threshold": -18.5, "limiter_gain": 2.0}`

### 5. Training Data and Methodology
Because Ursula is an RL agent, I won't be training her on a static dataset. I will build a custom OpenAI Gym (Gymnasium) environment so she can learn the physics of EQ and compression through brute-force trial and error.

**The Datasets I'll Use:**
* **VCTK / LibriSpeech:** I will use these massive datasets of pristine vocals as my raw material.

**How I Will Train Her (The Sandbox Loop):**
1. **The Setup:** I will spin up a headless PipeWire instance running a generic DSP host (e.g., Carla, or a custom LV2 wrapper). I'll load a dry vocal from VCTK into the buffer.
2. **The Target:** I'll randomly generate a target LTAS curve and a target LUFS value ($M_{target}$) and hand them to her.
3. **The Action:** Ursula adjusts the parameters on the EQ and Compressor plugins. The audio processes through the nodes, and my script instantly calculates the new LTAS and LUFS of that processed audio ($M_{current}$).
4. **The Reward:** I will calculate her reward ($R_t$) by summing the physical errors. I'll measure the Mean Squared Error between the current LTAS and target LTAS, and the absolute difference between the current LUFS and target LUFS. If the error shrinks, she gets a positive reward. 
5. **The Guardrails:** I'll program a massive penalty ($\lambda$) if her adjustments cause the audio to digitally clip (exceed 0.0 dBFS).

**The Math I'll Use for Her Reward Function:**
$$R_t = - \left( \alpha ||LTAS_{current} - LTAS_{target}||_2^2 + \beta |LUFS_{current} - LUFS_{target}| \right) - \lambda \cdot \text{Penalty}_{clip}$$

*(Where $\alpha$ and $\beta$ are weights I will tune to balance the importance of tone vs. loudness).*

I will let her run this loop millions of times overnight. Eventually, her neural network will map the exact acoustic physics of every DSP parameter, allowing her to physically nail any LTAS or LUFS target Fabian throws at her.